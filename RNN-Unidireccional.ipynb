{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenaalamo/Personal/AA2/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "# pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Descomentar en Windows\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Descomentar en Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del DataLoader (importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast  # FAST tokenizer recomendado\n",
    "\n",
    "# Cargo tokenizar\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "PUNCT_TAGS = {\"Ã˜\": 0, \",\": 1, \".\": 2, \"?\": 3, \"Â¿\": 4}\n",
    "CAP_TAGS = {\"lower\": 0, \"init\": 1, \"mix\": 2, \"upper\": 3}\n",
    "\n",
    "def _get_capitalization_type(word):\n",
    "    if not word or word.islower(): return 0\n",
    "    if word.istitle(): return 1\n",
    "    if word.isupper(): return 3\n",
    "    if any(c.isupper() for c in word[1:]): return 2\n",
    "    return 0\n",
    "\n",
    "def get_cap_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Recibe los labels por palabra y devuelve los labels por token para capitalizacion\n",
    "    Si los subtokens pertenecen a la misma palabra, les pone el mismo label (capitalizacion) \n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for word_idx in token_word_map:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(labels_per_word[word_idx])\n",
    "    return labels\n",
    "\n",
    "def get_punct_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Asigna etiquetas de puntuaciÃ³n a los subtokens, siguiendo las reglas:\n",
    "    - Â¿ va en el primer subtoken de la palabra.\n",
    "    - ., ?, , van en el Ãºltimo subtoken de la palabra.\n",
    "    - Ã˜ no se asigna a ningÃºn subtoken (todos -100).\n",
    "    \"\"\"\n",
    "    labels = [0] * len(token_word_map)\n",
    "    word_to_token_idxs = {}\n",
    "\n",
    "    # Construimos un diccionario: word_idx -> [lista de posiciones de tokens]\n",
    "    for token_idx, word_idx in enumerate(token_word_map):\n",
    "        if word_idx is not None:\n",
    "            word_to_token_idxs.setdefault(word_idx, []).append(token_idx)\n",
    "\n",
    "    for word_idx, token_idxs in word_to_token_idxs.items():\n",
    "        punct_label = labels_per_word[word_idx]\n",
    "        if punct_label == PUNCT_TAGS[\"Â¿\"]:\n",
    "            target_idx = token_idxs[0]  # primer subtoken\n",
    "        elif punct_label in {PUNCT_TAGS[\".\"], PUNCT_TAGS[\",\"], PUNCT_TAGS[\"?\"]}:\n",
    "            target_idx = token_idxs[-1]  # Ãºltimo subtoken\n",
    "        else:\n",
    "            continue  # Ã˜: no se asigna nada\n",
    "\n",
    "        labels[target_idx] = punct_label\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataloader(oraciones_raw, max_length, batch_size, device):\n",
    "    \"\"\"\n",
    "    Crea un DataLoader para entrenar un modelo de restauraciÃ³n de puntuaciÃ³n y capitalizaciÃ³n.\n",
    "\n",
    "    A partir de una lista de oraciones correctamente escritas (con puntuaciÃ³n y mayÃºsculas),\n",
    "    esta funciÃ³n:\n",
    "        - Extrae etiquetas de puntuaciÃ³n y capitalizaciÃ³n por palabra.\n",
    "        - \"Corrompe\" el texto al eliminar la puntuaciÃ³n y poner las palabras en minÃºscula.\n",
    "        - Tokeniza las palabras corruptas usando un tokenizer BERT.\n",
    "        - Alinea las etiquetas con los subtokens del tokenizer.\n",
    "        - Crea tensores para las entradas (input_ids, attention_mask) y etiquetas (puntuaciÃ³n y capitalizaciÃ³n).\n",
    "        - Devuelve un DataLoader para entrenamiento en lotes.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "        oraciones_raw (List[str]): Lista de oraciones correctamente formateadas.\n",
    "        max_length (int): Longitud mÃ¡xima de secuencia para truncar/padear.\n",
    "        batch_size (int): TamaÃ±o del batch.\n",
    "        device (str): Dispositivo donde se cargarÃ¡n los tensores ('cpu' o 'cuda').\n",
    "\n",
    "    Retorna:\n",
    "        DataLoader: DataLoader que entrega batches de (input_ids, attention_mask, punct_labels, cap_labels).\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks = []\n",
    "    punct_labels_list = []\n",
    "    cap_labels_list = []\n",
    "\n",
    "    for sent in oraciones_raw:\n",
    "        # Extraer palabras con puntuaciÃ³n\n",
    "        matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sent)) # Detecta puntuaciones y las splitea\n",
    "        words = []\n",
    "        punct_labels = []\n",
    "        cap_labels = []\n",
    "\n",
    "        for i, m in enumerate(matches): # Recorre cada palabra detectada\n",
    "            word_raw = m.group(0) \n",
    "            clean_word = re.sub(r\"[.,?Â¿]\", \"\", word_raw) # Limpia la palabra \"Hola!\" -> \"Hola\"\n",
    "\n",
    "            # PuntuaciÃ³n\n",
    "            before = sent[m.start() - 1] if m.start() > 0 else \"\" # Signo anterior\n",
    "            after = sent[m.end()] if m.end() < len(sent) else \"\"  # Signo posterior\n",
    "            if before == 'Â¿':\n",
    "                punct = PUNCT_TAGS[\"Â¿\"]\n",
    "            elif after in PUNCT_TAGS:\n",
    "                punct = PUNCT_TAGS[after]\n",
    "            else:\n",
    "                punct = PUNCT_TAGS[\"Ã˜\"]\n",
    "\n",
    "            # CapitalizaciÃ³n\n",
    "            cap = _get_capitalization_type(word_raw)\n",
    "\n",
    "            clean_word = clean_word.lower() # Limpia la palabra Hola -> hola\n",
    "\n",
    "            words.append(clean_word)\n",
    "            punct_labels.append(punct)\n",
    "            cap_labels.append(cap)\n",
    "\n",
    "        # TokenizaciÃ³n con BERT\n",
    "        encoding = tokenizer(words,\n",
    "                             is_split_into_words=True,\n",
    "                             return_tensors='pt',\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             max_length=max_length,\n",
    "                             return_attention_mask=True)\n",
    "\n",
    "        # Extraer datos que nos sirven del encoding\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "        attention_mask = encoding['attention_mask'][0]\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Mapea cada subtoken a su palabra\n",
    "\n",
    "        # Alinear etiquetas a subtokens (hasta ahora las teniamos en palabras)\n",
    "        punct_labels_aligned = get_punct_labels_for_tokens(punct_labels, word_ids)\n",
    "        cap_labels_aligned = get_cap_labels_for_tokens(cap_labels, word_ids)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        punct_tensor = torch.tensor(punct_labels_aligned)\n",
    "        cap_tensor = torch.tensor(cap_labels_aligned)\n",
    "\n",
    "        # Aplicar -100 a posiciones de padding\n",
    "        punct_tensor[attention_mask == 0] = -100\n",
    "        cap_tensor[attention_mask == 0] = -100\n",
    "\n",
    "        # Agregar a listas (por oracion)\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "        punct_labels_list.append(punct_tensor)\n",
    "        cap_labels_list.append(cap_tensor)\n",
    "\n",
    "    # Stackear tensores (por batch)\n",
    "    input_ids = torch.stack(input_ids_list).to(device)\n",
    "    attention_masks = torch.stack(attention_masks).to(device)\n",
    "    punct_labels = torch.stack(punct_labels_list).to(device)\n",
    "    cap_labels = torch.stack(cap_labels_list).to(device)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, punct_labels, cap_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra detectada : 'que'\n",
      "Antes del match   : 'Â¿'\n",
      "DespuÃ©s del match : '?'\n",
      "---\n",
      "Palabra detectada : 'Que'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'raro'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ','\n",
      "---\n",
      "Palabra detectada : 'a'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mi'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'me'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'gusta'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mas'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'tomar'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'CocaCola'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ''\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Linea para entender como el get loader determina las puntuaciones\n",
    "\n",
    "import re\n",
    "\n",
    "# Ejemplo de oraciÃ³n\n",
    "sentence = \"Â¿que? Que raro, a mi me gusta mas tomar CocaCola\"\n",
    "\n",
    "# Encuentra palabras con posible puntuaciÃ³n al final\n",
    "matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sentence))\n",
    "\n",
    "# Mostramos los resultados\n",
    "for match in matches:\n",
    "    word_raw = match.group(0)\n",
    "    start = match.start()\n",
    "    end = match.end()\n",
    "\n",
    "    # Caracter anterior y posterior\n",
    "    before = sentence[start - 1] if start > 0 else \"\"\n",
    "    after = sentence[end] if end < len(sentence) else \"\"\n",
    "\n",
    "    print(f\"Palabra detectada : '{word_raw}'\")\n",
    "    print(f\"Antes del match   : '{before}'\")\n",
    "    print(f\"DespuÃ©s del match : '{after}'\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en EspaÃ±ol.\n"
     ]
    }
   ],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "DATA_URLS = {\n",
    "    \"train\": \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/train.json\",\n",
    "    \"dev\":   \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/dev.json\",\n",
    "    \"test\":  \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/test.json\",\n",
    "}\n",
    "\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATA_URLS,\n",
    "    field=\"data\",\n",
    ")\n",
    "\n",
    "questions = []\n",
    "\n",
    "\n",
    "for i in range(0, len(raw[\"train\"])):\n",
    "  for p in raw[\"train\"][i]['paragraphs']:\n",
    "    p_questions = [qas['question'] for qas in p['qas']]\n",
    "    questions += p_questions\n",
    "\n",
    "N_QUESTIONS = 5000  # NÃºmero de preguntas a procesar\n",
    "questions = questions[:N_QUESTIONS]  # Limitar a N preguntas\n",
    "\n",
    "print(f\"Se descargaron {len(questions)} preguntas en EspaÃ±ol.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 997 oraciones en EspaÃ±ol (del dataset del notebook 10).\n"
     ]
    }
   ],
   "source": [
    "dataset_rnn = load_dataset(\"google/wmt24pp\", \"en-es_MX\", split=\"train\")\n",
    "oraciones_rnn = dataset_rnn['target'][1:]\n",
    "\n",
    "print(f\"Se descargaron {len(oraciones_rnn)} oraciones en EspaÃ±ol (del dataset del notebook 10).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1413 oraciones sintÃ©ticas.\n"
     ]
    }
   ],
   "source": [
    "oraciones_sinteticas = []\n",
    "import json\n",
    "with open('./datasets/datasets.json', 'r') as file:\n",
    "  data = json.load(file)\n",
    "\n",
    "oraciones_sinteticas = data['otros'] + data['marcas']\n",
    "print(f\"Hay {len(oraciones_sinteticas)} oraciones sintÃ©ticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- La corriente de pensamiento vigente en Francia era la IlustraciÃ³n, cuyos principios se basaban en la razÃ³n, la igualdad y la libertad.\n",
      "- La IlustraciÃ³n habÃ­a servido de impulso a las Trece Colonias norteamericanas para la independencia de su metrÃ³polis europea.\n",
      "- Tanto la influencia de la IlustraciÃ³n como el ejemplo de los Estados Unidos sirvieron de Â«trampolÃ­nÂ» ideolÃ³gico para el inicio de la revoluciÃ³n en Francia.\n",
      "- El otro gran lastre para la economÃ­a fue la deuda estatal.\n",
      "- En 1788, la relaciÃ³n entre la deuda y la renta nacional bruta en Francia era del 55,6 %, en comparaciÃ³n con el 181,8 % en Gran BretaÃ±a.\n",
      "['Argentina, oficialmente RepÃºblica Argentina,[a]\\u200b es un paÃ­s soberano de AmÃ©rica del Sur, ubicado en el extremo sur y sudeste de ese subcontinente.', 'Adopta la forma de gobierno republicana, democrÃ¡tica, representativa y federal.', 'Poseen Carta Magna, bandera y fuerzas de seguridad propias, el dominio de los recursos naturales circunscriptos en su territorio y delegan los poderes exclusivos al Gobierno Federal.', 'Hasta mediados del siglo XX, fue una de las economÃ­as mÃ¡s prÃ³speras del mundo.', 'No obstante, es la segunda economÃ­a mÃ¡s importante de SudamÃ©rica â€”detrÃ¡s de Brasilâ€” y la 24.Âº mÃ¡s grande del mundo por PIB nominal.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6648"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import json\n",
    "\n",
    "# API Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "\n",
    "def obtener_frases_wikipedia(titulo, max_frases=100):\n",
    "    try:\n",
    "        pagina = wikipedia.page(titulo)\n",
    "        texto = pagina.content\n",
    "        oraciones = re.split(r'(?<=[.!?])\\s+', texto)\n",
    "        frases = [o.strip() for o in oraciones if 5 < len(o.split()) < 30]\n",
    "        return frases[:max_frases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error al buscar '{titulo}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Ejemplo: obtener 50 frases de un artÃ­culo\n",
    "frases = obtener_frases_wikipedia(\"RevoluciÃ³n francesa\", max_frases=50)\n",
    "for f in frases[:5]:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "temas = [\n",
    "    # PaÃ­ses y lugares\n",
    "    'Argentina', 'EspaÃ±a', 'MÃ©xico', 'Colombia', 'Chile',\n",
    "    'PerÃº', 'Uruguay', 'Brasil', 'AmÃ©rica Latina', 'Europa',\n",
    "\n",
    "    # Cultura argentina\n",
    "    'Lionel Messi', 'Diego Maradona', 'Lali Esposito', 'Charly Garcia', 'Dillom',\n",
    "    'Tiempos Violentos', 'Relatos Salvajes', 'Universidad de Buenos Aires', 'Rock nacional', 'Cine argentino',\n",
    "\n",
    "    # Historia y polÃ­tica\n",
    "    'Revolucion de Mayo', 'Independencia de Argentina', 'SimÃ³n BolÃ­var', 'Segunda Guerra Mundial', 'Guerra FrÃ­a',\n",
    "    'RevoluciÃ³n Francesa', 'Guerra Civil EspaÃ±ola', 'NapoleÃ³n Bonaparte', 'Nelson Mandela', 'Dictadura militar en Argentina',\n",
    "\n",
    "    # Ciencia y tecnologÃ­a\n",
    "    'Inteligencia artificial', 'ChatGPT', 'Redes neuronales', 'RobÃ³tica', 'EnergÃ­a solar',\n",
    "    'Vacunas', 'COVID-19', 'Cambio climÃ¡tico', 'Computadora cuÃ¡ntica', 'NASA',\n",
    "\n",
    "    # Cultura general\n",
    "    'El Principito', 'Premio Nobel', 'Frida Kahlo', 'Pablo Picasso', 'Leonardo da Vinci',\n",
    "    'William Shakespeare', 'Gabriel GarcÃ­a MÃ¡rquez', 'Julio CortÃ¡zar', 'Literatura latinoamericana', 'Arte contemporÃ¡neo',\n",
    "\n",
    "    # Entretenimiento y medios\n",
    "    'Marvel', 'DC Comics', 'Netflix', 'Cine de terror', 'PelÃ­culas de ciencia ficciÃ³n',\n",
    "    'MÃºsica electrÃ³nica', 'ReguetÃ³n', 'Spotify', 'YouTube', 'TikTok',\n",
    "\n",
    "    # Deportes\n",
    "    'FÃºtbol', 'Copa Mundial de la FIFA', 'Juegos Olimpicos', 'Tenis', 'NBA',\n",
    "    'Boca Juniors', 'River Plate', 'Messi vs Ronaldo', 'FÃ³rmula 1', 'Michael Jordan',\n",
    "\n",
    "    # Sociedad y actualidad\n",
    "    'Feminismo', 'DÃ­a Internacional de la Mujer', 'Diversidad cultural', 'MigraciÃ³n', 'Pobreza',\n",
    "    'EducaciÃ³n pÃºblica', 'Salud mental', 'Medio ambiente', 'Derechos humanos', 'Trabajo remoto',\n",
    "\n",
    "    # FilosofÃ­a y pensamiento\n",
    "    'FilosofÃ­a', 'Ã‰tica', 'PsicologÃ­a', 'Sigmund Freud', 'Carl Jung',\n",
    "    'Existencialismo', 'SociologÃ­a', 'EconomÃ­a', 'PolÃ­tica', 'Democracia'\n",
    "]\n",
    "\n",
    "def cargar_json_wikipedia(archivo,temas, max_frases=100):\n",
    "    \"\"\"\n",
    "    Carga un archivo JSON con frases de Wikipedia.\n",
    "    \"\"\"\n",
    "    frases_wikipedia = []\n",
    "    for tema in temas:\n",
    "        print(f\"Obteniendo frases de Wikipedia para: {tema}\")\n",
    "        frases = obtener_frases_wikipedia(tema,max_frases=100)\n",
    "        print('Ejemplos de frases obtenidas:')\n",
    "        for f in frases[:2]:\n",
    "            print(f\"- {f}\")\n",
    "        frases_wikipedia.extend(frases)\n",
    "    # Guardar en un archivo JSON\n",
    "    with open(\"datasets/frases_wikipedia.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(frases_wikipedia, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"âœ… Frases guardadas en 'datasets/frases_wikipedia.json'\")\n",
    "    \n",
    "\n",
    "# Para actualizar la info de Wikipedia, descomentar la siguiente linea\n",
    "# cargar_json_wikipedia(\"frases_wikipedia.json\",temas, max_frases=100)\n",
    "\n",
    "# Guardar en un archivo JSON\n",
    "with open(\"datasets/frases_wikipedia.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    frases_wikipedia = json.load(f)\n",
    "\n",
    "print(frases_wikipedia[:5])  # muestra las primeras frases\n",
    "\n",
    "len(frases_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "âœ… Frases extraÃ­das y guardadas. Total: 947\n",
      "['JÃ³dete.', 'Â¡Â¡Quieren dejarse de joder!', 'Pero que tragedia tan irreparable...', 'Ah, Â¡ Â¡quelle tragedieÃ¼', 'Tan Ãºtil, tan servicial. Â¿Te acordÃ¡s? Trabajaba como una bestia todo el dÃ­a la pobre anciana. No, no tiene perdÃ³n, mirÃ¡. No, si los pecados se pagan en la tierra o se pagan en el cielo o en el infierno, no sÃ©; en algÃºn lugar se deben pagar, digo yo...', 'Yo tampoco.', 'Y bueno, te querrÃ¡ ayudar.', 'Â¡Â¡Â¡Cacho!!!', 'Pero no me en mi cuarto.', 'Con anteojos negros y un paÃ±uelo en la cabeza.']\n"
     ]
    }
   ],
   "source": [
    "def extraer_frases_dialogo(path_txt):\n",
    "    personajes = [\n",
    "    \"Jorge\", \"Susana\", \"MamÃ¡ Cora\", \"Sergio\", \"Elvira\", \"Antonio\", \"Nora\",\n",
    "    \"Matilde\", \"Dominga\", \"Felipe\", \"Emilia\", \"DoÃ±a Elisa\", \"DoÃ±a Gertrudis\",\n",
    "    \"Don Genaro\", \"La Sorda\", \"Peralta\", \"Cacho\", \"Nene Florista\"\n",
    "    ]\n",
    "\n",
    "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # Limpiar saltos mÃºltiples y unificar espacios\n",
    "    raw = re.sub(r\"\\n+\", \"\\n\", raw)\n",
    "\n",
    "    # Construir patrÃ³n para encontrar encabezados de personaje\n",
    "    nombres_pattern = \"|\".join(re.escape(p) for p in personajes)\n",
    "    pattern = re.compile(rf\"^({nombres_pattern})\\s*[\\.:â€“\\-]+\", re.MULTILINE)\n",
    "\n",
    "    frases = []\n",
    "    matches = list(pattern.finditer(raw))\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(raw)\n",
    "        bloque = raw[start:end]\n",
    "\n",
    "        # Limpiar texto: eliminar parÃ©ntesis, saltos de lÃ­nea, espacios mÃºltiples\n",
    "        bloque = re.sub(r\"\\([^)]*\\)\", \"\", bloque)\n",
    "        bloque = bloque.replace(\"\\n\", \" \")\n",
    "        bloque = re.sub(r\"\\s+\", \" \", bloque).strip()\n",
    "\n",
    "        if bloque:\n",
    "            frases.append(bloque)\n",
    "\n",
    "    print(f\"âœ… Se extrajeron {len(frases)} frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\")\n",
    "    return frases\n",
    "\n",
    "# Ejecutar la funciÃ³n y guardar a JSON\n",
    "esperando_la_carroza = extraer_frases_dialogo(\"datasets/esperando_la_carroza.txt\")\n",
    "\n",
    "with open(\"datasets/dialogos_esperando_la_carroza.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(esperando_la_carroza, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Mostrar algunas frases aleatorias\n",
    "print(\"âœ… Frases extraÃ­das y guardadas. Total:\", len(esperando_la_carroza))\n",
    "print(random.sample(esperando_la_carroza, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Se extrajeron 1000 frases de Relatos Salvajes.\n",
      "['Hola.', 'Polludo, Â¡Â¿me diga lo que vas a hacer?!', 'Pero, por favor. Por favor...', 'PerdÃ³n. Â¿Ya sabe que lo va a pedir?', 'Que me devuelvan lo que gastÃ© en el taxi hasta acÃ¡, y que me pidan las correspondientes disculpas.', 'Sentirte orgullosa, Nena.', 'Hija, Â¿estÃ¡s bien?', 'Â¿Conoces algÃºn buen lugar para que aprendemos tango?', 'No te lo voy a negar.', 'Ariel, Â¿toda la mesa 27 sabe que te coita esta mina?']\n"
     ]
    }
   ],
   "source": [
    "def extraer_frases_subtitulos(path_txt):\n",
    "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    bloques = re.split(r\"\\n\\s*\\n\", raw)\n",
    "    frases_crudas = []\n",
    "\n",
    "    for bloque in bloques:\n",
    "        lineas = bloque.strip().split(\"\\n\")\n",
    "        if len(lineas) < 3:\n",
    "            continue\n",
    "\n",
    "        texto = \" \".join(lineas[2:])\n",
    "        texto = re.sub(r\"^-\", \"\", texto).strip()\n",
    "        texto = re.sub(r\"\\s*-\\s*\", \" \", texto)\n",
    "        texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "\n",
    "        # âŒ Filtrar frases vacÃ­as o solo puntos (como \"...\" o \". . .\")\n",
    "        if not texto or re.fullmatch(r\"[. ]{2,}\", texto):\n",
    "            continue\n",
    "\n",
    "        frases_crudas.append(texto)\n",
    "\n",
    "    # ðŸ” Unir frases incompletas (que no terminan en . ! ?)\n",
    "    frases_limpias = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for frase in frases_crudas:\n",
    "        if buffer:\n",
    "            buffer += \" \" + frase\n",
    "        else:\n",
    "            buffer = frase\n",
    "\n",
    "        if re.search(r\"[.!?](['â€\\\"])?$\", buffer):\n",
    "            frases_limpias.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "\n",
    "    if buffer:\n",
    "        frases_limpias.append(buffer.strip())\n",
    "\n",
    "    print(f\"âœ… Se extrajeron {len(frases_limpias)} frases de Relatos Salvajes.\")\n",
    "    return frases_limpias\n",
    "\n",
    "# Ejemplo de uso\n",
    "frases_relatos_salvajes = extraer_frases_subtitulos(\"datasets/subt_relatos_salvajes.srt\")\n",
    "\n",
    "# Guardar como JSON\n",
    "with open(\"datasets/frases_relatos_salvajes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(frases_relatos_salvajes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Mostrar algunas frases\n",
    "import random\n",
    "print(random.sample(frases_relatos_salvajes, min(10, len(frases_relatos_salvajes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 16005\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintÃ©ticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['reacondicionando toda la tierra de los contenedores que usÃ© el aÃ±o pasado y comenzando con mis cucurbitÃ¡ceas, es una mitad de tierra para macetas usada de un amigo que cultiva marihuana y mitad de la capa superficial de los canteros de mis padres',\n",
       " 'Â¿DÃ³nde estÃ¡ la Base AÃ©rea de Osan?',\n",
       " 'Bloch, Chicago)[149]\\u200b abriÃ³ el periodo del cubismo puntillista.',\n",
       " 'Musicalmente, el Ã¡lbum mostraba un cambio estilÃ­stico de fondo, mÃ¡s complejo, conceptual y orientado al rock sinfÃ³nico.',\n",
       " 'â€œCielos, Valturiâ€, dijo Nemic. â€œPodrÃ­as pensar que a estas alturas ya habrÃ­an aprendido a despertarse cuando tÃº lo hacesâ€.',\n",
       " 'Otros lagos se encuentran en las cuencas endorreicas o a lo largo de los cursos de los rÃ­os maduros.',\n",
       " 'Tiene por avisarlo.',\n",
       " 'Asimismo estrenÃ³ en EspaÃ±a sus tÃ¡cticas de bombardeo sobre ciudades.',\n",
       " 'Â¿En quÃ© parte del dÃ­a emitirÃ¡n sus votos las mujeres? ',\n",
       " 'Â¿QuÃ© cuestiÃ³n regularÃ¡ el acuerdo?',\n",
       " 'Mauricio.',\n",
       " 'Â¿QuiÃ©n es el padre de Robin?',\n",
       " 'Ariel, Â¿toda la mesa 27 sabe que te coita esta mina?',\n",
       " 'PelÃ­culas de Marvel SecciÃ³n de artÃ­culos centrada en pelÃ­culas y series de Marvel.',\n",
       " 'La sociedad no va a cambiar.',\n",
       " 'Â¿QuÃ© piloto acabÃ³ liderando la carrera de MotoGP?',\n",
       " 'En 1961, Ussachevsky invitÃ³ a VarÃ¨se al Columbia-Princeton Studio (CPEMC), siendo asistido por Mario Davidovsky y BÃ¼lent Arel.',\n",
       " 'Veo violencia a salir a la calle, veo violencia a prender la televisiÃ³n.',\n",
       " 'Pedro I: um herÃ³i sem nenhum carÃ¡ter (en portuguÃ©s).',\n",
       " 'Es mÃ¡s fÃ¡cil determinar el inicio de esta etapa que su final, debido a los cambios fisiolÃ³gicos que se producen.',\n",
       " 'Y esta es la razÃ³n:',\n",
       " 'Â¿CÃ³mo se tradujeron las tensiones campo-ciudad durante la Ã©poca de neutralidad espaÃ±ola?',\n",
       " 'Â¿QuiÃ©n hablaba ya en el aÃ±o 138 a. C. del cÃºmulo Melotte 111?',\n",
       " 'Thassalin gruÃ±Ã³, despegÃ³ y desapareciÃ³ brevemente antes de regresar, dejando caer un cadÃ¡ver delante de Nyssi. Era un Orothrack, una criatura dracÃ³nica blanca y dorada muy rara. Pero le faltaba la cabeza. â€œOTRO MUERTO. OIGO SUS GRITOS. HUELO SU SANGRE. ME SIENTO ATRAÃDO A PROTEGER Y CAZAR. COMO EXIGE YISINI. LAS CRIATURAS, ESTÃN CERCAâ€.',\n",
       " 'Frostfell no siempre fue un solo reino. Estaba dividido en dos: el Norte y el Sur. Al Norte, el reino no mÃ¡gico. Al Sur, un reino lleno de fantÃ¡sticas muestras de poder. Los dos reinos se enfrentaron en una guerra sin fin a causa de la codicia del Norte, hasta que el Sur propuso una tregua.',\n",
       " 'Â¿De dÃ³nde provenÃ­an los habitantes de Filipinas segÃºn la teorÃ­a de la expansiÃ³n austronesia?',\n",
       " 'Demostremos que todo esto es mentira, pero lo del anillo es cierto.',\n",
       " 'Evidentemente bien no estÃ¡s.',\n",
       " 'Â¿Por quÃ© ha dejado Israel que se marcharan los 250 palestinos presos?',\n",
       " 'â€œBueno, estos compaÃ±eros de aquÃ­ del campamento de entrenamiento sÃ­ que lo aprendieronâ€. Cohren hizo un gesto en direcciÃ³n a Harris, Craith y Ravik. â€œPero claro, son lÃ­deres de escuadrÃ³n y pelotÃ³n, saben que tienen que despertarse antes que sus unidadesâ€.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def limpiar_simbolos(frases):\n",
    "    frases_limpias = []\n",
    "    for frase in frases:\n",
    "        # Eliminar cualquier secuencia de dos o mÃ¡s puntos (incluyendo con espacios): ... . .. etc.\n",
    "        frase = re.sub(r\"(\\.\\s*){2,}\", \"\", frase)\n",
    "        # Eliminar cualquier caracter que NO sea letra, nÃºmero, espacio o los signos permitidos\n",
    "        frase = re.sub(r\"[^a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃÃ‰ÃÃ“ÃšÃ±Ã‘Ã¼Ãœ0-9Â¿?,. ]+\", \"\", frase)\n",
    "        # Reemplazar mÃºltiples espacios por uno solo\n",
    "        frase = re.sub(r\"\\s+\", \" \", frase).strip()\n",
    "        frases_limpias.append(frase)\n",
    "    return frases_limpias\n",
    "\n",
    "# Eliminar signos de exclamaciÃ³n de las frases\n",
    "frases_relatos_salvajes = limpiar_simbolos(frases_relatos_salvajes)\n",
    "esperando_la_carroza = limpiar_simbolos(esperando_la_carroza)\n",
    "\n",
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintÃ©ticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "\n",
    "# Muestra algunas oraciones aleatorias\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15204\n",
      "801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class PunctuationCapitalizationRNN(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, num_punct_classes, num_cap_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model                        # ahora el modelo completo\n",
    "        self.projection = nn.Linear(\n",
    "            self.bert.config.hidden_size, hidden_dim\n",
    "        )\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=2,\n",
    "                           batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.punct_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_punct_classes)\n",
    "        )\n",
    "        self.cap_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_cap_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # BERT retorna last_hidden_state: (B, T, H_bert)\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        projected = self.projection(hidden_states)   # (B, T, hidden_dim)\n",
    "\n",
    "        # mismo packing/padding de antes\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1)\n",
    "            packed = pack_padded_sequence(projected, lengths.cpu(),\n",
    "                                          batch_first=True, enforce_sorted=False)\n",
    "            rnn_out_packed, _ = self.rnn(packed)\n",
    "            rnn_out, _ = pad_packed_sequence(\n",
    "                rnn_out_packed, batch_first=True, total_length=projected.size(1)\n",
    "            )\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(projected)\n",
    "\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        punct_logits = self.punct_classifier(rnn_out)\n",
    "        cap_logits   = self.cap_classifier(rnn_out)\n",
    "        return punct_logits, cap_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Funcion de entrenamiento\n",
    "def train(model, dataloader_train, dataloader_test, optimizer, criterion_punct, criterion_cap, device, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_punct = criterion_punct(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "            loss_cap = criterion_cap(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "            loss = loss_punct + loss_cap\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader_train)\n",
    "\n",
    "        \"\"\"\n",
    "        # EvaluaciÃ³n en test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, punct_labels, cap_labels in dataloader_test:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                punct_labels = punct_labels.to(device)\n",
    "                cap_labels = cap_labels.to(device)\n",
    "\n",
    "                punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                loss_punct = criterion(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "                loss_cap = criterion(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "                loss = loss_punct + loss_cap\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader_test)\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    inv_punct = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "    inv_cap = {v: k for k, v in CAP_TAGS.items()}\n",
    "\n",
    "    all_true_punct = []\n",
    "    all_pred_punct = []\n",
    "    all_true_cap = []\n",
    "    all_pred_cap = []\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids)\n",
    "\n",
    "            pred_punct = torch.argmax(punct_logits, dim=-1)\n",
    "            pred_cap = torch.argmax(cap_logits, dim=-1)\n",
    "\n",
    "            # MÃ¡scara para ignorar tokens sin etiqueta\n",
    "            mask = (punct_labels != -100)\n",
    "\n",
    "            # Aplicar mÃ¡scara y aplanar para comparaciÃ³n\n",
    "            all_true_punct.extend(punct_labels[mask].cpu().numpy())\n",
    "            all_pred_punct.extend(pred_punct[mask].cpu().numpy())\n",
    "\n",
    "            all_true_cap.extend(cap_labels[mask].cpu().numpy())\n",
    "            all_pred_cap.extend(pred_cap[mask].cpu().numpy())\n",
    "\n",
    "    print(\"Unique classes in true labels:\", set(all_true_cap))\n",
    "    print(\"Unique classes in predictions:\", set(all_pred_cap))\n",
    "\n",
    "    print(\"ðŸ” Unique true cap labels:\", set(all_true_cap))\n",
    "    print(\"ðŸ” Unique pred cap labels:\", set(all_pred_cap))\n",
    "    print(\"ðŸ” Unique true punct labels:\", set(all_true_punct))\n",
    "    print(\"ðŸ” Unique pred punct labels:\", set(all_pred_punct))\n",
    "\n",
    "    # Accuracy generales\n",
    "    punct_acc = np.mean(np.array(all_true_punct) == np.array(all_pred_punct))\n",
    "    cap_acc = np.mean(np.array(all_true_cap) == np.array(all_pred_cap))\n",
    "\n",
    "    print(\"ðŸ“Œ Punctuation Accuracy:     {:.4f}\".format(punct_acc))\n",
    "    print(\"ðŸ”¡ Capitalization Accuracy: {:.4f}\".format(cap_acc))\n",
    "\n",
    "    # Reportes detallados\n",
    "    print(\"\\nðŸ“Š Punctuation classification report:\")\n",
    "    print(classification_report(all_true_punct, all_pred_punct, target_names=[inv_punct[i] for i in range(len(inv_punct))]))\n",
    "\n",
    "    print(\"\\nðŸ“Š Capitalization classification report:\")\n",
    "    print(classification_report(all_true_cap, all_pred_cap, target_names=[inv_cap[i] for i in range(len(inv_cap))]))\n",
    "\n",
    "    return punct_acc, cap_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m cap_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, punct_labels, cap_labels \u001b[38;5;129;01min\u001b[39;00m dataloader_train:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Convertir a CPU y a numpy para contar\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     punct_labels_np \u001b[38;5;241m=\u001b[39m \u001b[43mpunct_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     37\u001b[0m     cap_labels_np \u001b[38;5;241m=\u001b[39m cap_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Contar etiquetas vÃ¡lidas (ignorando -100)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Embeddings de BERT\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Congelar la mayorÃ­a de los parÃ¡metros de BERT salvo los Ãºltimos N layers y el pooler\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# le quiero pasar el weight param al criterion para mejorar el desbalanceo de clases en base a un counter de tokens\n",
    "# Contar ocurrencias de cada etiqueta de puntuaciÃ³n y capitalizaciÃ³n\n",
    "punct_counter = Counter()\n",
    "cap_counter = Counter()\n",
    "for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "    # Convertir a CPU y a numpy para contar\n",
    "    punct_labels_np = punct_labels.cpu().numpy()\n",
    "    cap_labels_np = cap_labels.cpu().numpy()\n",
    "\n",
    "    # Contar etiquetas vÃ¡lidas (ignorando -100)\n",
    "    valid_punct = punct_labels_np[punct_labels_np != -100]\n",
    "    valid_cap = cap_labels_np[cap_labels_np != -100]\n",
    "\n",
    "    punct_counter.update(valid_punct)\n",
    "    cap_counter.update(valid_cap)\n",
    "\n",
    "# como las clases estan desbalanceadas, calculamos los pesos inversos\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap = sum(cap_counter.values())\n",
    "beta = 0.7\n",
    "\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap   = sum(cap_counter.values())\n",
    "\n",
    "punct_weights = {\n",
    "    tag: (total_punct / count)**beta\n",
    "    for tag, count in punct_counter.items()\n",
    "}\n",
    "cap_weights = {\n",
    "    tag: (total_cap / count)**beta\n",
    "    for tag, count in cap_counter.items()\n",
    "}\n",
    "\n",
    "# pasar a tensor (clamp opcional para evitar extremos)\n",
    "punct_weights_tensor = torch.tensor(\n",
    "    [punct_weights.get(i, 1.0) for i in range(len(PUNCT_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "cap_weights_tensor = torch.tensor(\n",
    "    [cap_weights.get(i, 1.0) for i in range(len(CAP_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100, weight=punct_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "# Entrenamiento \n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion_punct=criterion_punct, criterion_cap = criterion_cap, device=device, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in true labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "Unique classes in predictions: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "ðŸ” Unique true cap labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "ðŸ” Unique pred cap labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "ðŸ” Unique true punct labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)}\n",
      "ðŸ” Unique pred punct labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15390,) (16992,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 103\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Accuracy generales\u001b[39;00m\n\u001b[1;32m    102\u001b[0m punct_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(all_true_punct) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_pred_punct))\n\u001b[0;32m--> 103\u001b[0m cap_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_true_cap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_pred_cap\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Œ Punctuation Accuracy:     \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(punct_acc))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”¡ Capitalization Accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cap_acc))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15390,) (16992,) "
     ]
    }
   ],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_reconstruct(model, sentence, tokenizer, device, max_length=64, verbose=True):\n",
    "    model.eval()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        punct_logits, cap_logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    pred_punct = torch.argmax(punct_logits, dim=-1)[0].cpu().tolist()\n",
    "    pred_cap = torch.argmax(cap_logits, dim=-1)[0].cpu().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    INV_PUNCT_TAGS = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "\n",
    "    final_words = []\n",
    "    current_word = \"\"\n",
    "    current_cap = 0\n",
    "    current_punct = 0\n",
    "    new_word = True\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\nðŸ” PredicciÃ³n token por token:\")\n",
    "        print(f\"{'TOKEN':15s} | {'PUNCT':>5s} | {'SIGNO':>5s} | {'CAP':>3s} | {'FINAL':15s}\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "    for i, (token, punct_label, cap_label) in enumerate(zip(tokens, pred_punct, pred_cap)):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] or attention_mask[0, i].item() == 0:\n",
    "            continue\n",
    "\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += clean_token\n",
    "            if punct_label != 0:\n",
    "                current_punct = punct_label  # usar puntuaciÃ³n del Ãºltimo subtoken relevante\n",
    "        else:\n",
    "            if current_word:\n",
    "                # cerrar palabra anterior\n",
    "                word = current_word\n",
    "                # aplicar capitalizaciÃ³n a toda la palabra\n",
    "                if current_cap == 1:\n",
    "                    word = word.capitalize()\n",
    "                elif current_cap == 2:\n",
    "                    word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "                elif current_cap == 3:\n",
    "                    word = word.upper()\n",
    "                # aplicar puntuaciÃ³n del Ãºltimo subtoken\n",
    "                punct = INV_PUNCT_TAGS.get(current_punct, \"Ã˜\")\n",
    "                if punct == \"Â¿\":\n",
    "                    word = \"Â¿\" + word\n",
    "                elif punct != \"Ã˜\":\n",
    "                    word = word + punct\n",
    "                final_words.append(word)\n",
    "\n",
    "            # empezar nueva palabra\n",
    "            current_word = clean_token\n",
    "            current_cap = cap_label\n",
    "            current_punct = punct_label if punct_label != 0 else 0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{clean_token:15s} | {punct_label:5d} | {INV_PUNCT_TAGS.get(punct_label, 'Ã˜'):>5s} | {cap_label:3d} | {clean_token:15s}\")\n",
    "\n",
    "    # Procesar Ãºltima palabra\n",
    "    if current_word:\n",
    "        word = current_word\n",
    "        if current_cap == 1:\n",
    "            word = word.capitalize()\n",
    "        elif current_cap == 2:\n",
    "            word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "        elif current_cap == 3:\n",
    "            word = word.upper()\n",
    "        punct = INV_PUNCT_TAGS.get(current_punct, \"Ã˜\")\n",
    "        if punct == \"Â¿\":\n",
    "            word = \"Â¿\" + word\n",
    "        elif punct != \"Ã˜\":\n",
    "            word = word + punct\n",
    "        final_words.append(word)\n",
    "\n",
    "    return \" \".join(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola como estas => HOLA. COMO. ESTAS.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"hola como estas\"\n",
    "print(f\"{entrada} => {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba con overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m frases \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuenas tardes, quiero un APPLE por favor. Muchisimas HH\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m(frases, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m PunctuationCapitalizationRNN(\n\u001b[1;32m      6\u001b[0m     bert_model\u001b[38;5;241m=\u001b[39mbert_model,\n\u001b[1;32m      7\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      8\u001b[0m     num_punct_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      9\u001b[0m     num_cap_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)  \u001b[38;5;66;03m# Alto LR\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "frases = [\"Buenas tardes, quiero un APPLE por favor. Muchisimas HH\"]\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=64,\n",
    "    num_punct_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_cap = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(model, train_loader, train_loader,optimizer, criterion_punct, criterion_cap, device, epochs=200)\n",
    "\n",
    "entrada = \"buenas tardes quiero un apple por favor muchisimas hh\"\n",
    "print(\"PredicciÃ³n:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"modelo_fine_tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_fine_tuned_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 179,236,873\n",
      "Trainable parameters: 87,424,777\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
