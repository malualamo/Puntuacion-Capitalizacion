{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenaalamo/Personal/AA2/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "# pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Descomentar en Windows\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Descomentar en Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del DataLoader (importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast  # FAST tokenizer recomendado\n",
    "\n",
    "# Cargo tokenizar\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "PUNCT_TAGS = {\"Ã˜\": 0, \",\": 1, \".\": 2, \"?\": 3, \"Â¿\": 4}\n",
    "CAP_TAGS = {\"lower\": 0, \"init\": 1, \"mix\": 2, \"upper\": 3}\n",
    "\n",
    "def _get_capitalization_type(word):\n",
    "    if not word or word.islower(): return 0\n",
    "    if word.istitle(): return 1\n",
    "    if word.isupper(): return 3\n",
    "    if any(c.isupper() for c in word[1:]): return 2\n",
    "    return 0\n",
    "\n",
    "def get_cap_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Recibe los labels por palabra y devuelve los labels por token para capitalizacion\n",
    "    Si los subtokens pertenecen a la misma palabra, les pone el mismo label (capitalizacion) \n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for word_idx in token_word_map:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(labels_per_word[word_idx])\n",
    "    return labels\n",
    "\n",
    "def get_punct_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Asigna etiquetas de puntuaciÃ³n a los subtokens, siguiendo las reglas:\n",
    "    - Â¿ va en el primer subtoken de la palabra.\n",
    "    - ., ?, , van en el Ãºltimo subtoken de la palabra.\n",
    "    - Ã˜ no se asigna a ningÃºn subtoken (todos -100).\n",
    "    \"\"\"\n",
    "    labels = [0] * len(token_word_map)\n",
    "    word_to_token_idxs = {}\n",
    "\n",
    "    # Construimos un diccionario: word_idx -> [lista de posiciones de tokens]\n",
    "    for token_idx, word_idx in enumerate(token_word_map):\n",
    "        if word_idx is not None:\n",
    "            word_to_token_idxs.setdefault(word_idx, []).append(token_idx)\n",
    "\n",
    "    for word_idx, token_idxs in word_to_token_idxs.items():\n",
    "        punct_label = labels_per_word[word_idx]\n",
    "        if punct_label == PUNCT_TAGS[\"Â¿\"]:\n",
    "            target_idx = token_idxs[0]  # primer subtoken\n",
    "        elif punct_label in {PUNCT_TAGS[\".\"], PUNCT_TAGS[\",\"], PUNCT_TAGS[\"?\"]}:\n",
    "            target_idx = token_idxs[-1]  # Ãºltimo subtoken\n",
    "        else:\n",
    "            continue  # Ã˜: no se asigna nada\n",
    "\n",
    "        labels[target_idx] = punct_label\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataloader(oraciones_raw, max_length, batch_size, device):\n",
    "    \"\"\"\n",
    "    Crea un DataLoader para entrenar un modelo de restauraciÃ³n de puntuaciÃ³n y capitalizaciÃ³n.\n",
    "\n",
    "    A partir de una lista de oraciones correctamente escritas (con puntuaciÃ³n y mayÃºsculas),\n",
    "    esta funciÃ³n:\n",
    "        - Extrae etiquetas de puntuaciÃ³n y capitalizaciÃ³n por palabra.\n",
    "        - \"Corrompe\" el texto al eliminar la puntuaciÃ³n y poner las palabras en minÃºscula.\n",
    "        - Tokeniza las palabras corruptas usando un tokenizer BERT.\n",
    "        - Alinea las etiquetas con los subtokens del tokenizer.\n",
    "        - Crea tensores para las entradas (input_ids, attention_mask) y etiquetas (puntuaciÃ³n y capitalizaciÃ³n).\n",
    "        - Devuelve un DataLoader para entrenamiento en lotes.\n",
    "\n",
    "    ParÃ¡metros:\n",
    "        oraciones_raw (List[str]): Lista de oraciones correctamente formateadas.\n",
    "        max_length (int): Longitud mÃ¡xima de secuencia para truncar/padear.\n",
    "        batch_size (int): TamaÃ±o del batch.\n",
    "        device (str): Dispositivo donde se cargarÃ¡n los tensores ('cpu' o 'cuda').\n",
    "\n",
    "    Retorna:\n",
    "        DataLoader: DataLoader que entrega batches de (input_ids, attention_mask, punct_labels, cap_labels).\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks = []\n",
    "    punct_labels_list = []\n",
    "    cap_labels_list = []\n",
    "\n",
    "    for sent in oraciones_raw:\n",
    "        # Extraer palabras con puntuaciÃ³n\n",
    "        matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sent)) # Detecta puntuaciones y las splitea\n",
    "        words = []\n",
    "        punct_labels = []\n",
    "        cap_labels = []\n",
    "\n",
    "        for i, m in enumerate(matches): # Recorre cada palabra detectada\n",
    "            word_raw = m.group(0) \n",
    "            clean_word = re.sub(r\"[.,?Â¿]\", \"\", word_raw) # Limpia la palabra \"Hola!\" -> \"Hola\"\n",
    "\n",
    "            # PuntuaciÃ³n\n",
    "            before = sent[m.start() - 1] if m.start() > 0 else \"\" # Signo anterior\n",
    "            after = sent[m.end()] if m.end() < len(sent) else \"\"  # Signo posterior\n",
    "            if before == 'Â¿':\n",
    "                punct = PUNCT_TAGS[\"Â¿\"]\n",
    "            elif after in PUNCT_TAGS:\n",
    "                punct = PUNCT_TAGS[after]\n",
    "            else:\n",
    "                punct = PUNCT_TAGS[\"Ã˜\"]\n",
    "\n",
    "            # CapitalizaciÃ³n\n",
    "            cap = _get_capitalization_type(word_raw)\n",
    "\n",
    "            clean_word = clean_word.lower() # Limpia la palabra Hola -> hola\n",
    "\n",
    "            words.append(clean_word)\n",
    "            punct_labels.append(punct)\n",
    "            cap_labels.append(cap)\n",
    "\n",
    "        # TokenizaciÃ³n con BERT\n",
    "        encoding = tokenizer(words,\n",
    "                             is_split_into_words=True,\n",
    "                             return_tensors='pt',\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             max_length=max_length,\n",
    "                             return_attention_mask=True)\n",
    "\n",
    "        # Extraer datos que nos sirven del encoding\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "        attention_mask = encoding['attention_mask'][0]\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Mapea cada subtoken a su palabra\n",
    "\n",
    "        # Alinear etiquetas a subtokens (hasta ahora las teniamos en palabras)\n",
    "        punct_labels_aligned = get_punct_labels_for_tokens(punct_labels, word_ids)\n",
    "        cap_labels_aligned = get_cap_labels_for_tokens(cap_labels, word_ids)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        punct_tensor = torch.tensor(punct_labels_aligned)\n",
    "        cap_tensor = torch.tensor(cap_labels_aligned)\n",
    "\n",
    "        # Aplicar -100 a posiciones de padding\n",
    "        punct_tensor[attention_mask == 0] = -100\n",
    "        cap_tensor[attention_mask == 0] = -100\n",
    "\n",
    "        # Agregar a listas (por oracion)\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "        punct_labels_list.append(punct_tensor)\n",
    "        cap_labels_list.append(cap_tensor)\n",
    "\n",
    "    # Stackear tensores (por batch)\n",
    "    input_ids = torch.stack(input_ids_list).to(device)\n",
    "    attention_masks = torch.stack(attention_masks).to(device)\n",
    "    punct_labels = torch.stack(punct_labels_list).to(device)\n",
    "    cap_labels = torch.stack(cap_labels_list).to(device)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, punct_labels, cap_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra detectada : 'que'\n",
      "Antes del match   : 'Â¿'\n",
      "DespuÃ©s del match : '?'\n",
      "---\n",
      "Palabra detectada : 'Que'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'raro'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ','\n",
      "---\n",
      "Palabra detectada : 'a'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mi'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'me'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'gusta'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mas'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'tomar'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ' '\n",
      "---\n",
      "Palabra detectada : 'CocaCola'\n",
      "Antes del match   : ' '\n",
      "DespuÃ©s del match : ''\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Linea para entender como el get loader determina las puntuaciones\n",
    "\n",
    "import re\n",
    "\n",
    "# Ejemplo de oraciÃ³n\n",
    "sentence = \"Â¿que? Que raro, a mi me gusta mas tomar CocaCola\"\n",
    "\n",
    "# Encuentra palabras con posible puntuaciÃ³n al final\n",
    "matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sentence))\n",
    "\n",
    "# Mostramos los resultados\n",
    "for match in matches:\n",
    "    word_raw = match.group(0)\n",
    "    start = match.start()\n",
    "    end = match.end()\n",
    "\n",
    "    # Caracter anterior y posterior\n",
    "    before = sentence[start - 1] if start > 0 else \"\"\n",
    "    after = sentence[end] if end < len(sentence) else \"\"\n",
    "\n",
    "    print(f\"Palabra detectada : '{word_raw}'\")\n",
    "    print(f\"Antes del match   : '{before}'\")\n",
    "    print(f\"DespuÃ©s del match : '{after}'\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en EspaÃ±ol.\n"
     ]
    }
   ],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "DATA_URLS = {\n",
    "    \"train\": \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/train.json\",\n",
    "    \"dev\":   \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/dev.json\",\n",
    "    \"test\":  \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/test.json\",\n",
    "}\n",
    "\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATA_URLS,\n",
    "    field=\"data\",\n",
    ")\n",
    "\n",
    "questions = []\n",
    "\n",
    "\n",
    "for i in range(0, len(raw[\"train\"])):\n",
    "  for p in raw[\"train\"][i]['paragraphs']:\n",
    "    p_questions = [qas['question'] for qas in p['qas']]\n",
    "    questions += p_questions\n",
    "\n",
    "N_QUESTIONS = 5000  # NÃºmero de preguntas a procesar\n",
    "questions = questions[:N_QUESTIONS]  # Limitar a N preguntas\n",
    "\n",
    "print(f\"Se descargaron {len(questions)} preguntas en EspaÃ±ol.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 997 oraciones en EspaÃ±ol (del dataset del notebook 10).\n"
     ]
    }
   ],
   "source": [
    "dataset_rnn = load_dataset(\"google/wmt24pp\", \"en-es_MX\", split=\"train\")\n",
    "oraciones_rnn = dataset_rnn['target'][1:]\n",
    "\n",
    "print(f\"Se descargaron {len(oraciones_rnn)} oraciones en EspaÃ±ol (del dataset del notebook 10).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1413 oraciones sintÃ©ticas.\n"
     ]
    }
   ],
   "source": [
    "oraciones_sinteticas = []\n",
    "import json\n",
    "with open('./datasets/datasets.json', 'r') as file:\n",
    "  data = json.load(file)\n",
    "\n",
    "oraciones_sinteticas = data['otros'] + data['marcas']\n",
    "print(f\"Hay {len(oraciones_sinteticas)} oraciones sintÃ©ticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- La corriente de pensamiento vigente en Francia era la IlustraciÃ³n, cuyos principios se basaban en la razÃ³n, la igualdad y la libertad.\n",
      "- La IlustraciÃ³n habÃ­a servido de impulso a las Trece Colonias norteamericanas para la independencia de su metrÃ³polis europea.\n",
      "- Tanto la influencia de la IlustraciÃ³n como el ejemplo de los Estados Unidos sirvieron de Â«trampolÃ­nÂ» ideolÃ³gico para el inicio de la revoluciÃ³n en Francia.\n",
      "- El otro gran lastre para la economÃ­a fue la deuda estatal.\n",
      "- En 1788, la relaciÃ³n entre la deuda y la renta nacional bruta en Francia era del 55,6 %, en comparaciÃ³n con el 181,8 % en Gran BretaÃ±a.\n",
      "['Argentina, oficialmente RepÃºblica Argentina,[a]\\u200b es un paÃ­s soberano de AmÃ©rica del Sur, ubicado en el extremo sur y sudeste de ese subcontinente.', 'Adopta la forma de gobierno republicana, democrÃ¡tica, representativa y federal.', 'Poseen Carta Magna, bandera y fuerzas de seguridad propias, el dominio de los recursos naturales circunscriptos en su territorio y delegan los poderes exclusivos al Gobierno Federal.', 'Hasta mediados del siglo XX, fue una de las economÃ­as mÃ¡s prÃ³speras del mundo.', 'No obstante, es la segunda economÃ­a mÃ¡s importante de SudamÃ©rica â€”detrÃ¡s de Brasilâ€” y la 24.Âº mÃ¡s grande del mundo por PIB nominal.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6648"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import json\n",
    "\n",
    "# API Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "\n",
    "def obtener_frases_wikipedia(titulo, max_frases=100):\n",
    "    try:\n",
    "        pagina = wikipedia.page(titulo)\n",
    "        texto = pagina.content\n",
    "        oraciones = re.split(r'(?<=[.!?])\\s+', texto)\n",
    "        frases = [o.strip() for o in oraciones if 5 < len(o.split()) < 30]\n",
    "        return frases[:max_frases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error al buscar '{titulo}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Ejemplo: obtener 50 frases de un artÃ­culo\n",
    "frases = obtener_frases_wikipedia(\"RevoluciÃ³n francesa\", max_frases=50)\n",
    "for f in frases[:5]:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "temas = [\n",
    "    # PaÃ­ses y lugares\n",
    "    'Argentina', 'EspaÃ±a', 'MÃ©xico', 'Colombia', 'Chile',\n",
    "    'PerÃº', 'Uruguay', 'Brasil', 'AmÃ©rica Latina', 'Europa',\n",
    "\n",
    "    # Cultura argentina\n",
    "    'Lionel Messi', 'Diego Maradona', 'Lali Esposito', 'Charly Garcia', 'Dillom',\n",
    "    'Tiempos Violentos', 'Relatos Salvajes', 'Universidad de Buenos Aires', 'Rock nacional', 'Cine argentino',\n",
    "\n",
    "    # Historia y polÃ­tica\n",
    "    'Revolucion de Mayo', 'Independencia de Argentina', 'SimÃ³n BolÃ­var', 'Segunda Guerra Mundial', 'Guerra FrÃ­a',\n",
    "    'RevoluciÃ³n Francesa', 'Guerra Civil EspaÃ±ola', 'NapoleÃ³n Bonaparte', 'Nelson Mandela', 'Dictadura militar en Argentina',\n",
    "\n",
    "    # Ciencia y tecnologÃ­a\n",
    "    'Inteligencia artificial', 'ChatGPT', 'Redes neuronales', 'RobÃ³tica', 'EnergÃ­a solar',\n",
    "    'Vacunas', 'COVID-19', 'Cambio climÃ¡tico', 'Computadora cuÃ¡ntica', 'NASA',\n",
    "\n",
    "    # Cultura general\n",
    "    'El Principito', 'Premio Nobel', 'Frida Kahlo', 'Pablo Picasso', 'Leonardo da Vinci',\n",
    "    'William Shakespeare', 'Gabriel GarcÃ­a MÃ¡rquez', 'Julio CortÃ¡zar', 'Literatura latinoamericana', 'Arte contemporÃ¡neo',\n",
    "\n",
    "    # Entretenimiento y medios\n",
    "    'Marvel', 'DC Comics', 'Netflix', 'Cine de terror', 'PelÃ­culas de ciencia ficciÃ³n',\n",
    "    'MÃºsica electrÃ³nica', 'ReguetÃ³n', 'Spotify', 'YouTube', 'TikTok',\n",
    "\n",
    "    # Deportes\n",
    "    'FÃºtbol', 'Copa Mundial de la FIFA', 'Juegos Olimpicos', 'Tenis', 'NBA',\n",
    "    'Boca Juniors', 'River Plate', 'Messi vs Ronaldo', 'FÃ³rmula 1', 'Michael Jordan',\n",
    "\n",
    "    # Sociedad y actualidad\n",
    "    'Feminismo', 'DÃ­a Internacional de la Mujer', 'Diversidad cultural', 'MigraciÃ³n', 'Pobreza',\n",
    "    'EducaciÃ³n pÃºblica', 'Salud mental', 'Medio ambiente', 'Derechos humanos', 'Trabajo remoto',\n",
    "\n",
    "    # FilosofÃ­a y pensamiento\n",
    "    'FilosofÃ­a', 'Ã‰tica', 'PsicologÃ­a', 'Sigmund Freud', 'Carl Jung',\n",
    "    'Existencialismo', 'SociologÃ­a', 'EconomÃ­a', 'PolÃ­tica', 'Democracia'\n",
    "]\n",
    "\n",
    "def cargar_json_wikipedia(archivo,temas, max_frases=100):\n",
    "    \"\"\"\n",
    "    Carga un archivo JSON con frases de Wikipedia.\n",
    "    \"\"\"\n",
    "    frases_wikipedia = []\n",
    "    for tema in temas:\n",
    "        print(f\"Obteniendo frases de Wikipedia para: {tema}\")\n",
    "        frases = obtener_frases_wikipedia(tema,max_frases=100)\n",
    "        print('Ejemplos de frases obtenidas:')\n",
    "        for f in frases[:2]:\n",
    "            print(f\"- {f}\")\n",
    "        frases_wikipedia.extend(frases)\n",
    "    # Guardar en un archivo JSON\n",
    "    with open(\"datasets/frases_wikipedia.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(frases_wikipedia, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"âœ… Frases guardadas en 'datasets/frases_wikipedia.json'\")\n",
    "    \n",
    "\n",
    "# Para actualizar la info de Wikipedia, descomentar la siguiente linea\n",
    "# cargar_json_wikipedia(\"frases_wikipedia.json\",temas, max_frases=100)\n",
    "\n",
    "# Guardar en un archivo JSON\n",
    "with open(\"datasets/frases_wikipedia.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    frases_wikipedia = json.load(f)\n",
    "\n",
    "print(frases_wikipedia[:5])  # muestra las primeras frases\n",
    "\n",
    "len(frases_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "âœ… Frases extraÃ­das y guardadas. Total: 947\n",
      "['Matildlta: Â¿vos la querÃ©s a la abuellta?', 'Ah, me parece una idea de lo mÃ¡s sensata. Al fin y al cabo se matÃ³ por eso, Â¿no?', 'Â¿QuÃ© sabÃ©s de mÃ­? Â¡HablÃ©! Pero antes lavate bien la boca con lavandina, porque yo no tengo nada que reprochar de los 18 aÃ±os que llevo de casada.', 'Ya te dije que no voy y esto es definitivo y final.', 'Internaron a mi suegra, a la madre.', 'Antonio... Sergio: dejÃ¡ que la vele en mi casa.', 'En el cuartlto del fondo.', 'Â¿Te das cuenta con lo que hay que lidiar, no?', 'MamÃ¡: dice doÃ±a Elisa que nos vayamos todos a la mierda.', 'Â¡Adelante! Â¿CÃ³mo estÃ¡s, querida? Ay masas, si serÃ¡s mala.']\n"
     ]
    }
   ],
   "source": [
    "def extraer_frases_dialogo(path_txt):\n",
    "    personajes = [\n",
    "    \"Jorge\", \"Susana\", \"MamÃ¡ Cora\", \"Sergio\", \"Elvira\", \"Antonio\", \"Nora\",\n",
    "    \"Matilde\", \"Dominga\", \"Felipe\", \"Emilia\", \"DoÃ±a Elisa\", \"DoÃ±a Gertrudis\",\n",
    "    \"Don Genaro\", \"La Sorda\", \"Peralta\", \"Cacho\", \"Nene Florista\"\n",
    "    ]\n",
    "\n",
    "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # Limpiar saltos mÃºltiples y unificar espacios\n",
    "    raw = re.sub(r\"\\n+\", \"\\n\", raw)\n",
    "\n",
    "    # Construir patrÃ³n para encontrar encabezados de personaje\n",
    "    nombres_pattern = \"|\".join(re.escape(p) for p in personajes)\n",
    "    pattern = re.compile(rf\"^({nombres_pattern})\\s*[\\.:â€“\\-]+\", re.MULTILINE)\n",
    "\n",
    "    frases = []\n",
    "    matches = list(pattern.finditer(raw))\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(raw)\n",
    "        bloque = raw[start:end]\n",
    "\n",
    "        # Limpiar texto: eliminar parÃ©ntesis, saltos de lÃ­nea, espacios mÃºltiples\n",
    "        bloque = re.sub(r\"\\([^)]*\\)\", \"\", bloque)\n",
    "        bloque = bloque.replace(\"\\n\", \" \")\n",
    "        bloque = re.sub(r\"\\s+\", \" \", bloque).strip()\n",
    "\n",
    "        if bloque:\n",
    "            frases.append(bloque)\n",
    "\n",
    "    print(f\"âœ… Se extrajeron {len(frases)} frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\")\n",
    "    return frases\n",
    "\n",
    "# Ejecutar la funciÃ³n y guardar a JSON\n",
    "esperando_la_carroza = extraer_frases_dialogo(\"datasets/esperando_la_carroza.txt\")\n",
    "\n",
    "with open(\"datasets/dialogos_esperando_la_carroza.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(esperando_la_carroza, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Mostrar algunas frases aleatorias\n",
    "print(\"âœ… Frases extraÃ­das y guardadas. Total:\", len(esperando_la_carroza))\n",
    "print(random.sample(esperando_la_carroza, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Se extrajeron 1000 frases de Relatos Salvajes.\n",
      "['Â¿Ustedes sacaron los pasajes? No.', 'Tiene que mirar mÃ¡s al salÃ³n, bebe.', 'DÃ©me una papa fritas a caballo. Se me quedo con hambre te pido otra cosa.', 'Â¡Forro!', 'Te dan de comer, vive sin preocupaciones, y se queda con un bon grupo hasta le pasa bien.', 'Esto es una casualidad increÃ­ble.', 'Esta blindado, no lo vas a poder romper.', 'Igual le tengo cariÃ±o. No era una mala persona.', 'Las dos cosas espero.', 'Â¡Te voy buscar y te voy a matar!']\n"
     ]
    }
   ],
   "source": [
    "def extraer_frases_subtitulos(path_txt):\n",
    "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    bloques = re.split(r\"\\n\\s*\\n\", raw)\n",
    "    frases_crudas = []\n",
    "\n",
    "    for bloque in bloques:\n",
    "        lineas = bloque.strip().split(\"\\n\")\n",
    "        if len(lineas) < 3:\n",
    "            continue\n",
    "\n",
    "        texto = \" \".join(lineas[2:])\n",
    "        texto = re.sub(r\"^-\", \"\", texto).strip()\n",
    "        texto = re.sub(r\"\\s*-\\s*\", \" \", texto)\n",
    "        texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "\n",
    "        # âŒ Filtrar frases vacÃ­as o solo puntos (como \"...\" o \". . .\")\n",
    "        if not texto or re.fullmatch(r\"[. ]{2,}\", texto):\n",
    "            continue\n",
    "\n",
    "        frases_crudas.append(texto)\n",
    "\n",
    "    # ðŸ” Unir frases incompletas (que no terminan en . ! ?)\n",
    "    frases_limpias = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for frase in frases_crudas:\n",
    "        if buffer:\n",
    "            buffer += \" \" + frase\n",
    "        else:\n",
    "            buffer = frase\n",
    "\n",
    "        if re.search(r\"[.!?](['â€\\\"])?$\", buffer):\n",
    "            frases_limpias.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "\n",
    "    if buffer:\n",
    "        frases_limpias.append(buffer.strip())\n",
    "\n",
    "    print(f\"âœ… Se extrajeron {len(frases_limpias)} frases de Relatos Salvajes.\")\n",
    "    return frases_limpias\n",
    "\n",
    "# Ejemplo de uso\n",
    "frases_relatos_salvajes = extraer_frases_subtitulos(\"datasets/subt_relatos_salvajes.srt\")\n",
    "\n",
    "# Guardar como JSON\n",
    "with open(\"datasets/frases_relatos_salvajes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(frases_relatos_salvajes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Mostrar algunas frases\n",
    "import random\n",
    "print(random.sample(frases_relatos_salvajes, min(10, len(frases_relatos_salvajes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 16005\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintÃ©ticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Â¿Por quÃ© no quiere Stoiber que TurquÃ­a entre en la UE?',\n",
       " 'Poco despuÃ©s de comenzar la revoluciÃ³n, NapoleÃ³n se encontraba en CÃ³rcega.',\n",
       " 'El juez ya habÃ­a recibido amenazas de muerte con anterioridad por este motivo.',\n",
       " 'El tiempo atmosfÃ©rico, por otro lado, es la condiciÃ³n actual de estos mismos elementos durante perÃ­odos de hasta dos semanas.',\n",
       " 'Â¿CuÃ¡les son otros tÃ©rminos empleados para referirse al Medioevo?',\n",
       " 'Consultado el 13 de abril de 2013.',\n",
       " 'Simon Spillane, jefe de operaciones del grupo de presiÃ³n, declarÃ³ que las cerveceras estÃ¡n â€œya liderando el caminoâ€ en cuanto a la sustentabilidad del envasado, citando el ejemplo de los barriles de cerveza que son reutilizados alrededor de 165 veces antes de ser aplastados y reciclados tras 30 aÃ±os. Asegura que ellos apoyan los sistemas de reciclaje de reutilizaciÃ³n en muchos paÃ­ses.',\n",
       " 'Llama la seguridad ahora',\n",
       " 'El gÃ©nero ha sido producido alrededor de todo el mundo, variando en contenido y estilo entre regiones.',\n",
       " 'Â¿CÃ³mo son los sistemas sanitarios de los paÃ­ses?',\n",
       " 'Le dio sobre todo sus manuscritos, cuadernillos, documentos e instrumentos.',\n",
       " 'Â¿QuÃ© medio ha dado la noticia de las muertes en las minas chinas?',\n",
       " 'Ambos discuten acerca de los riesgos que comporta su actividad y, finalmente, se deciden a atracar la misma cafeterÃ­a en la que estÃ¡n.',\n",
       " 'Â¿QuÃ© cargo ejerce Margarethe Wahlstrom?',\n",
       " 'Francesco, despuÃ©s de haberlo acompaÃ±ado durante su estancia en Francia, se quedÃ³ con Leonardo hasta su muerte, y administrÃ³ su herencia durante los cincuenta aÃ±os posteriores.',\n",
       " 'Pero Mauricio pregunta si no se puede desincluir al casero, y dice que tiene que haber un responsable pues hubo dos muertes y que saliÃ³ en los noticieros.',\n",
       " 'La principal de estas armas era la bomba atÃ³mica.',\n",
       " 'En 1987 Hitachi desarrollo el Sendai Subway 1000, el primer tren autÃ³nomo de la historia.',\n",
       " 'Â¿Verdad? Forense. Una vez que el juez dÃ© el permiso, podrÃ¡n llevarla a',\n",
       " 'Esto se debe al problema del consenso entre especialistas en la definiciÃ³n de los conceptos involucrados y en los procedimientos y tÃ©cnicas a utilizar.',\n",
       " 'Hoy en dÃ­a, las democracias existentes son bastante distintas al sistema de gobierno ateniense del que heredan su nombre.',\n",
       " 'Buscaroli de Musicardi.',\n",
       " 'El Golfo de OmÃ¡n conecta el mar ArÃ¡bigo con el estrecho de Ormuz.',\n",
       " 'No fotografiÃ© la pila o el cargador, pero aquÃ­ tienen una foto del panel.',\n",
       " 'En HerÃ³doto Europa es el mayor de los continentes, extendiÃ©ndose al norte del MediterrÃ¡neo desde las Columnas de HÃ©rcules del estrecho de Gibraltar hasta mÃ¡s allÃ¡ del rÃ­o Indo.',\n",
       " 'Jawed Karim, cofundador de YouTube, calificÃ³ la actualizaciÃ³n como \"una idea estÃºpida\" y dijo que el verdadero motivo del cambio \"no es bueno y no se harÃ¡ pÃºblico\".',\n",
       " 'Y a usted mis condolencias. Pero por quÃ© hizo semejante cosa.',\n",
       " 'Â¿CÃ³mo cree Dely ValdÃ©s que se siente alguna gente de Oviedo?',\n",
       " 'Â¿QuÃ© novela de la saga no habÃ­a salido a la luz durante la escritura del guion de la pelÃ­cula de Harry Potter y la piedra filosofal?',\n",
       " 'Â¿CuÃ¡ndo se empezÃ³ a trabajar en las teorÃ­as de la mÃºsica?']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def limpiar_simbolos(frases):\n",
    "    frases_limpias = []\n",
    "    for frase in frases:\n",
    "        # Eliminar cualquier secuencia de dos o mÃ¡s puntos (incluyendo con espacios): ... . .. etc.\n",
    "        frase = re.sub(r\"(\\.\\s*){2,}\", \"\", frase)\n",
    "        # Eliminar cualquier caracter que NO sea letra, nÃºmero, espacio o los signos permitidos\n",
    "        frase = re.sub(r\"[^a-zA-ZÃ¡Ã©Ã­Ã³ÃºÃÃ‰ÃÃ“ÃšÃ±Ã‘Ã¼Ãœ0-9Â¿?,. ]+\", \"\", frase)\n",
    "        # Reemplazar mÃºltiples espacios por uno solo\n",
    "        frase = re.sub(r\"\\s+\", \" \", frase).strip()\n",
    "        frases_limpias.append(frase)\n",
    "    return frases_limpias\n",
    "\n",
    "# Eliminar signos de exclamaciÃ³n de las frases\n",
    "frases_relatos_salvajes = limpiar_simbolos(frases_relatos_salvajes)\n",
    "esperando_la_carroza = limpiar_simbolos(esperando_la_carroza)\n",
    "\n",
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintÃ©ticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "\n",
    "# Muestra algunas oraciones aleatorias\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15204\n",
      "801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class PunctuationCapitalizationRNN(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, num_punct_classes, num_cap_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model                        # ahora el modelo completo\n",
    "        self.projection = nn.Linear(\n",
    "            self.bert.config.hidden_size, hidden_dim\n",
    "        )\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=2,\n",
    "                           batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.punct_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_punct_classes)\n",
    "        )\n",
    "        self.cap_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_cap_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # BERT retorna last_hidden_state: (B, T, H_bert)\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        projected = self.projection(hidden_states)   # (B, T, hidden_dim)\n",
    "\n",
    "        # mismo packing/padding de antes\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1)\n",
    "            packed = pack_padded_sequence(projected, lengths.cpu(),\n",
    "                                          batch_first=True, enforce_sorted=False)\n",
    "            rnn_out_packed, _ = self.rnn(packed)\n",
    "            rnn_out, _ = pad_packed_sequence(\n",
    "                rnn_out_packed, batch_first=True, total_length=projected.size(1)\n",
    "            )\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(projected)\n",
    "\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        punct_logits = self.punct_classifier(rnn_out)\n",
    "        cap_logits   = self.cap_classifier(rnn_out)\n",
    "        return punct_logits, cap_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Funcion de entrenamiento\n",
    "def train(model, dataloader_train, dataloader_test, optimizer, criterion_punct, criterion_cap, device, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_punct = criterion_punct(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "            loss_cap = criterion_cap(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "            loss = loss_punct + loss_cap\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader_train)\n",
    "\n",
    "        \"\"\"\n",
    "        # EvaluaciÃ³n en test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, punct_labels, cap_labels in dataloader_test:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                punct_labels = punct_labels.to(device)\n",
    "                cap_labels = cap_labels.to(device)\n",
    "\n",
    "                punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                loss_punct = criterion(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "                loss_cap = criterion(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "                loss = loss_punct + loss_cap\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader_test)\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    inv_punct = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "    inv_cap = {v: k for k, v in CAP_TAGS.items()}\n",
    "\n",
    "    all_true_punct = []\n",
    "    all_pred_punct = []\n",
    "    all_true_cap = []\n",
    "    all_pred_cap = []\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids)\n",
    "\n",
    "            pred_punct = torch.argmax(punct_logits, dim=-1)\n",
    "            pred_cap = torch.argmax(cap_logits, dim=-1)\n",
    "\n",
    "            # MÃ¡scara para ignorar tokens sin etiqueta\n",
    "            mask = (punct_labels != -100)\n",
    "\n",
    "            # Aplicar mÃ¡scara y aplanar para comparaciÃ³n\n",
    "            all_true_punct.extend(punct_labels[mask].cpu().numpy())\n",
    "            all_pred_punct.extend(pred_punct[mask].cpu().numpy())\n",
    "\n",
    "            all_true_cap.extend(cap_labels[mask].cpu().numpy())\n",
    "            all_pred_cap.extend(pred_cap[mask].cpu().numpy())\n",
    "\n",
    "    print(\"Unique classes in true labels:\", set(all_true_cap))\n",
    "    print(\"Unique classes in predictions:\", set(all_pred_cap))\n",
    "\n",
    "    print(\"ðŸ” Unique true cap labels:\", set(all_true_cap))\n",
    "    print(\"ðŸ” Unique pred cap labels:\", set(all_pred_cap))\n",
    "    print(\"ðŸ” Unique true punct labels:\", set(all_true_punct))\n",
    "    print(\"ðŸ” Unique pred punct labels:\", set(all_pred_punct))\n",
    "\n",
    "    # Accuracy generales\n",
    "    punct_acc = np.mean(np.array(all_true_punct) == np.array(all_pred_punct))\n",
    "    cap_acc = np.mean(np.array(all_true_cap) == np.array(all_pred_cap))\n",
    "\n",
    "    print(\"ðŸ“Œ Punctuation Accuracy:     {:.4f}\".format(punct_acc))\n",
    "    print(\"ðŸ”¡ Capitalization Accuracy: {:.4f}\".format(cap_acc))\n",
    "\n",
    "    # Reportes detallados\n",
    "    print(\"\\nðŸ“Š Punctuation classification report:\")\n",
    "    print(classification_report(all_true_punct, all_pred_punct, target_names=[inv_punct[i] for i in range(len(inv_punct))]))\n",
    "\n",
    "    print(\"\\nðŸ“Š Capitalization classification report:\")\n",
    "    print(classification_report(all_true_cap, all_pred_cap, target_names=[inv_cap[i] for i in range(len(inv_cap))]))\n",
    "\n",
    "    return punct_acc, cap_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m cap_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, punct_labels, cap_labels \u001b[38;5;129;01min\u001b[39;00m dataloader_train:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Convertir a CPU y a numpy para contar\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     punct_labels_np \u001b[38;5;241m=\u001b[39m \u001b[43mpunct_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     37\u001b[0m     cap_labels_np \u001b[38;5;241m=\u001b[39m cap_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Contar etiquetas vÃ¡lidas (ignorando -100)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Embeddings de BERT\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Congelar la mayorÃ­a de los parÃ¡metros de BERT salvo los Ãºltimos N layers y el pooler\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# le quiero pasar el weight param al criterion para mejorar el desbalanceo de clases en base a un counter de tokens\n",
    "# Contar ocurrencias de cada etiqueta de puntuaciÃ³n y capitalizaciÃ³n\n",
    "punct_counter = Counter()\n",
    "cap_counter = Counter()\n",
    "for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "    # Convertir a CPU y a numpy para contar\n",
    "    punct_labels_np = punct_labels.cpu().numpy()\n",
    "    cap_labels_np = cap_labels.cpu().numpy()\n",
    "\n",
    "    # Contar etiquetas vÃ¡lidas (ignorando -100)\n",
    "    valid_punct = punct_labels_np[punct_labels_np != -100]\n",
    "    valid_cap = cap_labels_np[cap_labels_np != -100]\n",
    "\n",
    "    punct_counter.update(valid_punct)\n",
    "    cap_counter.update(valid_cap)\n",
    "\n",
    "# como las clases estan desbalanceadas, calculamos los pesos inversos\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap = sum(cap_counter.values())\n",
    "beta = 0.7\n",
    "\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap   = sum(cap_counter.values())\n",
    "\n",
    "punct_weights = {\n",
    "    tag: (total_punct / count)**beta\n",
    "    for tag, count in punct_counter.items()\n",
    "}\n",
    "cap_weights = {\n",
    "    tag: (total_cap / count)**beta\n",
    "    for tag, count in cap_counter.items()\n",
    "}\n",
    "\n",
    "# pasar a tensor (clamp opcional para evitar extremos)\n",
    "punct_weights_tensor = torch.tensor(\n",
    "    [punct_weights.get(i, 1.0) for i in range(len(PUNCT_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "cap_weights_tensor = torch.tensor(\n",
    "    [cap_weights.get(i, 1.0) for i in range(len(CAP_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100, weight=punct_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "# Entrenamiento \n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion_punct=criterion_punct, criterion_cap = criterion_cap, device=device, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in true labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "Unique classes in predictions: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "ðŸ” Unique true cap labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "ðŸ” Unique pred cap labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "ðŸ” Unique true punct labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)}\n",
      "ðŸ” Unique pred punct labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15390,) (16992,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 103\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Accuracy generales\u001b[39;00m\n\u001b[1;32m    102\u001b[0m punct_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(all_true_punct) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_pred_punct))\n\u001b[0;32m--> 103\u001b[0m cap_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_true_cap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_pred_cap\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Œ Punctuation Accuracy:     \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(punct_acc))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ”¡ Capitalization Accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cap_acc))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15390,) (16992,) "
     ]
    }
   ],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_reconstruct(model, sentence, tokenizer, device, max_length=64, verbose=True):\n",
    "    model.eval()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        punct_logits, cap_logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    pred_punct = torch.argmax(punct_logits, dim=-1)[0].cpu().tolist()\n",
    "    pred_cap = torch.argmax(cap_logits, dim=-1)[0].cpu().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    INV_PUNCT_TAGS = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "\n",
    "    final_words = []\n",
    "    current_word = \"\"\n",
    "    current_cap = 0\n",
    "    current_punct = 0\n",
    "    new_word = True\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\nðŸ” PredicciÃ³n token por token:\")\n",
    "        print(f\"{'TOKEN':15s} | {'PUNCT':>5s} | {'SIGNO':>5s} | {'CAP':>3s} | {'FINAL':15s}\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "    for i, (token, punct_label, cap_label) in enumerate(zip(tokens, pred_punct, pred_cap)):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] or attention_mask[0, i].item() == 0:\n",
    "            continue\n",
    "\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += clean_token\n",
    "            if punct_label != 0:\n",
    "                current_punct = punct_label  # usar puntuaciÃ³n del Ãºltimo subtoken relevante\n",
    "        else:\n",
    "            if current_word:\n",
    "                # cerrar palabra anterior\n",
    "                word = current_word\n",
    "                # aplicar capitalizaciÃ³n a toda la palabra\n",
    "                if current_cap == 1:\n",
    "                    word = word.capitalize()\n",
    "                elif current_cap == 2:\n",
    "                    word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "                elif current_cap == 3:\n",
    "                    word = word.upper()\n",
    "                # aplicar puntuaciÃ³n del Ãºltimo subtoken\n",
    "                punct = INV_PUNCT_TAGS.get(current_punct, \"Ã˜\")\n",
    "                if punct == \"Â¿\":\n",
    "                    word = \"Â¿\" + word\n",
    "                elif punct != \"Ã˜\":\n",
    "                    word = word + punct\n",
    "                final_words.append(word)\n",
    "\n",
    "            # empezar nueva palabra\n",
    "            current_word = clean_token\n",
    "            current_cap = cap_label\n",
    "            current_punct = punct_label if punct_label != 0 else 0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{clean_token:15s} | {punct_label:5d} | {INV_PUNCT_TAGS.get(punct_label, 'Ã˜'):>5s} | {cap_label:3d} | {clean_token:15s}\")\n",
    "\n",
    "    # Procesar Ãºltima palabra\n",
    "    if current_word:\n",
    "        word = current_word\n",
    "        if current_cap == 1:\n",
    "            word = word.capitalize()\n",
    "        elif current_cap == 2:\n",
    "            word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "        elif current_cap == 3:\n",
    "            word = word.upper()\n",
    "        punct = INV_PUNCT_TAGS.get(current_punct, \"Ã˜\")\n",
    "        if punct == \"Â¿\":\n",
    "            word = \"Â¿\" + word\n",
    "        elif punct != \"Ã˜\":\n",
    "            word = word + punct\n",
    "        final_words.append(word)\n",
    "\n",
    "    return \" \".join(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola como estas => HOLA. COMO. ESTAS.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"hola como estas\"\n",
    "print(f\"{entrada} => {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.9041\n",
      "Epoch 2 | Train Loss: 2.7525\n",
      "Epoch 3 | Train Loss: 2.8042\n",
      "Epoch 4 | Train Loss: 2.4886\n",
      "Epoch 5 | Train Loss: 2.0364\n",
      "Epoch 6 | Train Loss: 1.7816\n",
      "Epoch 7 | Train Loss: 1.6085\n",
      "Epoch 8 | Train Loss: 1.9414\n",
      "Epoch 9 | Train Loss: 1.6012\n",
      "Epoch 10 | Train Loss: 1.5608\n",
      "Epoch 11 | Train Loss: 1.5680\n",
      "Epoch 12 | Train Loss: 1.6294\n",
      "Epoch 13 | Train Loss: 1.6771\n",
      "Epoch 14 | Train Loss: 1.5966\n",
      "Epoch 15 | Train Loss: 1.5496\n",
      "Epoch 16 | Train Loss: 1.4163\n",
      "Epoch 17 | Train Loss: 1.6198\n",
      "Epoch 18 | Train Loss: 1.6080\n",
      "Epoch 19 | Train Loss: 1.4456\n",
      "Epoch 20 | Train Loss: 1.4271\n",
      "Epoch 21 | Train Loss: 1.4963\n",
      "Epoch 22 | Train Loss: 1.6729\n",
      "Epoch 23 | Train Loss: 1.5201\n",
      "Epoch 24 | Train Loss: 1.5008\n",
      "Epoch 25 | Train Loss: 1.4461\n",
      "Epoch 26 | Train Loss: 1.5343\n",
      "Epoch 27 | Train Loss: 1.4170\n",
      "Epoch 28 | Train Loss: 1.5351\n",
      "Epoch 29 | Train Loss: 1.2816\n",
      "Epoch 30 | Train Loss: 1.3075\n",
      "Epoch 31 | Train Loss: 1.5001\n",
      "Epoch 32 | Train Loss: 1.3044\n",
      "Epoch 33 | Train Loss: 1.3603\n",
      "Epoch 34 | Train Loss: 1.6303\n",
      "Epoch 35 | Train Loss: 1.3930\n",
      "Epoch 36 | Train Loss: 1.4275\n",
      "Epoch 37 | Train Loss: 1.2022\n",
      "Epoch 38 | Train Loss: 1.3402\n",
      "Epoch 39 | Train Loss: 1.2695\n",
      "Epoch 40 | Train Loss: 1.3167\n",
      "Epoch 41 | Train Loss: 1.3456\n",
      "Epoch 42 | Train Loss: 1.0623\n",
      "Epoch 43 | Train Loss: 1.2127\n",
      "Epoch 44 | Train Loss: 1.3160\n",
      "Epoch 45 | Train Loss: 1.4177\n",
      "Epoch 46 | Train Loss: 1.3338\n",
      "Epoch 47 | Train Loss: 1.2174\n",
      "Epoch 48 | Train Loss: 1.2938\n",
      "Epoch 49 | Train Loss: 1.2318\n",
      "Epoch 50 | Train Loss: 1.3380\n",
      "Epoch 51 | Train Loss: 1.1478\n",
      "Epoch 52 | Train Loss: 1.0414\n",
      "Epoch 53 | Train Loss: 1.0351\n",
      "Epoch 54 | Train Loss: 1.0681\n",
      "Epoch 55 | Train Loss: 1.0730\n",
      "Epoch 56 | Train Loss: 1.0859\n",
      "Epoch 57 | Train Loss: 1.1499\n",
      "Epoch 58 | Train Loss: 1.1939\n",
      "Epoch 59 | Train Loss: 1.0210\n",
      "Epoch 60 | Train Loss: 1.0601\n",
      "Epoch 61 | Train Loss: 1.1526\n",
      "Epoch 62 | Train Loss: 0.9718\n",
      "Epoch 63 | Train Loss: 1.2524\n",
      "Epoch 64 | Train Loss: 0.9741\n",
      "Epoch 65 | Train Loss: 0.9509\n",
      "Epoch 66 | Train Loss: 1.2484\n",
      "Epoch 67 | Train Loss: 0.8212\n",
      "Epoch 68 | Train Loss: 0.9991\n",
      "Epoch 69 | Train Loss: 0.8325\n",
      "Epoch 70 | Train Loss: 1.0543\n",
      "Epoch 71 | Train Loss: 0.9744\n",
      "Epoch 72 | Train Loss: 0.8999\n",
      "Epoch 73 | Train Loss: 1.0224\n",
      "Epoch 74 | Train Loss: 0.8276\n",
      "Epoch 75 | Train Loss: 0.7658\n",
      "Epoch 76 | Train Loss: 0.7095\n",
      "Epoch 77 | Train Loss: 0.5454\n",
      "Epoch 78 | Train Loss: 0.6062\n",
      "Epoch 79 | Train Loss: 0.6741\n",
      "Epoch 80 | Train Loss: 0.7357\n",
      "Epoch 81 | Train Loss: 0.8277\n",
      "Epoch 82 | Train Loss: 0.5813\n",
      "Epoch 83 | Train Loss: 0.7103\n",
      "Epoch 84 | Train Loss: 0.7607\n",
      "Epoch 85 | Train Loss: 1.2008\n",
      "Epoch 86 | Train Loss: 1.0331\n",
      "Epoch 87 | Train Loss: 0.8042\n",
      "Epoch 88 | Train Loss: 0.7970\n",
      "Epoch 89 | Train Loss: 0.8939\n",
      "Epoch 90 | Train Loss: 0.7259\n",
      "Epoch 91 | Train Loss: 0.6029\n",
      "Epoch 92 | Train Loss: 0.5131\n",
      "Epoch 93 | Train Loss: 0.6188\n",
      "Epoch 94 | Train Loss: 0.7534\n",
      "Epoch 95 | Train Loss: 0.9649\n",
      "Epoch 96 | Train Loss: 0.6132\n",
      "Epoch 97 | Train Loss: 0.7953\n",
      "Epoch 98 | Train Loss: 0.5279\n",
      "Epoch 99 | Train Loss: 0.5407\n",
      "Epoch 100 | Train Loss: 0.4666\n",
      "Epoch 101 | Train Loss: 0.5466\n",
      "Epoch 102 | Train Loss: 0.5077\n",
      "Epoch 103 | Train Loss: 0.4878\n",
      "Epoch 104 | Train Loss: 0.6902\n",
      "Epoch 105 | Train Loss: 0.5986\n",
      "Epoch 106 | Train Loss: 0.5989\n",
      "Epoch 107 | Train Loss: 0.8205\n",
      "Epoch 108 | Train Loss: 0.6459\n",
      "Epoch 109 | Train Loss: 0.6935\n",
      "Epoch 110 | Train Loss: 0.8354\n",
      "Epoch 111 | Train Loss: 0.3638\n",
      "Epoch 112 | Train Loss: 0.4980\n",
      "Epoch 113 | Train Loss: 0.4571\n",
      "Epoch 114 | Train Loss: 0.6826\n",
      "Epoch 115 | Train Loss: 0.3637\n",
      "Epoch 116 | Train Loss: 0.4391\n",
      "Epoch 117 | Train Loss: 0.4531\n",
      "Epoch 118 | Train Loss: 0.3039\n",
      "Epoch 119 | Train Loss: 0.4022\n",
      "Epoch 120 | Train Loss: 0.5950\n",
      "Epoch 121 | Train Loss: 0.4299\n",
      "Epoch 122 | Train Loss: 0.3995\n",
      "Epoch 123 | Train Loss: 0.4307\n",
      "Epoch 124 | Train Loss: 0.3340\n",
      "Epoch 125 | Train Loss: 0.3994\n",
      "Epoch 126 | Train Loss: 0.3369\n",
      "Epoch 127 | Train Loss: 0.4564\n",
      "Epoch 128 | Train Loss: 0.4133\n",
      "Epoch 129 | Train Loss: 0.2869\n",
      "Epoch 130 | Train Loss: 0.3591\n",
      "Epoch 131 | Train Loss: 0.2112\n",
      "Epoch 132 | Train Loss: 0.5459\n",
      "Epoch 133 | Train Loss: 0.4612\n",
      "Epoch 134 | Train Loss: 0.2449\n",
      "Epoch 135 | Train Loss: 0.1646\n",
      "Epoch 136 | Train Loss: 0.2365\n",
      "Epoch 137 | Train Loss: 0.3438\n",
      "Epoch 138 | Train Loss: 0.2881\n",
      "Epoch 139 | Train Loss: 0.4048\n",
      "Epoch 140 | Train Loss: 0.0992\n",
      "Epoch 141 | Train Loss: 0.1102\n",
      "Epoch 142 | Train Loss: 0.2197\n",
      "Epoch 143 | Train Loss: 0.3388\n",
      "Epoch 144 | Train Loss: 0.1429\n",
      "Epoch 145 | Train Loss: 0.2174\n",
      "Epoch 146 | Train Loss: 0.3072\n",
      "Epoch 147 | Train Loss: 0.1916\n",
      "Epoch 148 | Train Loss: 0.2822\n",
      "Epoch 149 | Train Loss: 0.2708\n",
      "Epoch 150 | Train Loss: 0.1534\n",
      "Epoch 151 | Train Loss: 0.0734\n",
      "Epoch 152 | Train Loss: 0.2549\n",
      "Epoch 153 | Train Loss: 0.0962\n",
      "Epoch 154 | Train Loss: 0.3004\n",
      "Epoch 155 | Train Loss: 0.5569\n",
      "Epoch 156 | Train Loss: 0.8348\n",
      "Epoch 157 | Train Loss: 0.3751\n",
      "Epoch 158 | Train Loss: 0.1608\n",
      "Epoch 159 | Train Loss: 0.1561\n",
      "Epoch 160 | Train Loss: 0.4134\n",
      "Epoch 161 | Train Loss: 0.2224\n",
      "Epoch 162 | Train Loss: 0.6320\n",
      "Epoch 163 | Train Loss: 1.1164\n",
      "Epoch 164 | Train Loss: 0.3918\n",
      "Epoch 165 | Train Loss: 0.1651\n",
      "Epoch 166 | Train Loss: 1.6350\n",
      "Epoch 167 | Train Loss: 0.3820\n",
      "Epoch 168 | Train Loss: 0.1999\n",
      "Epoch 169 | Train Loss: 1.0317\n",
      "Epoch 170 | Train Loss: 0.6835\n",
      "Epoch 171 | Train Loss: 0.1613\n",
      "Epoch 172 | Train Loss: 0.1460\n",
      "Epoch 173 | Train Loss: 0.4859\n",
      "Epoch 174 | Train Loss: 0.3728\n",
      "Epoch 175 | Train Loss: 0.2635\n",
      "Epoch 176 | Train Loss: 0.4728\n",
      "Epoch 177 | Train Loss: 0.2598\n",
      "Epoch 178 | Train Loss: 0.6604\n",
      "Epoch 179 | Train Loss: 0.2222\n",
      "Epoch 180 | Train Loss: 0.3213\n",
      "Epoch 181 | Train Loss: 0.2924\n",
      "Epoch 182 | Train Loss: 0.3553\n",
      "Epoch 183 | Train Loss: 0.2332\n",
      "Epoch 184 | Train Loss: 0.2246\n",
      "Epoch 185 | Train Loss: 0.1460\n",
      "Epoch 186 | Train Loss: 0.2774\n",
      "Epoch 187 | Train Loss: 0.3304\n",
      "Epoch 188 | Train Loss: 0.3077\n",
      "Epoch 189 | Train Loss: 0.2289\n",
      "Epoch 190 | Train Loss: 0.2031\n",
      "Epoch 191 | Train Loss: 0.2062\n",
      "Epoch 192 | Train Loss: 0.1219\n",
      "Epoch 193 | Train Loss: 0.2051\n",
      "Epoch 194 | Train Loss: 0.3155\n",
      "Epoch 195 | Train Loss: 0.2491\n",
      "Epoch 196 | Train Loss: 0.0833\n",
      "Epoch 197 | Train Loss: 0.1502\n",
      "Epoch 198 | Train Loss: 0.1815\n",
      "Epoch 199 | Train Loss: 0.1731\n",
      "Epoch 200 | Train Loss: 0.0969\n",
      "\n",
      "ðŸ” PredicciÃ³n token por token:\n",
      "TOKEN           | PUNCT | SIGNO | CAP | FINAL          \n",
      "-------------------------------------------------------\n",
      "buenas          |     0 |     Ã˜ |   1 | buenas         \n",
      "tarde           |     0 |     Ã˜ |   0 | tarde          \n",
      "s               |     1 |     , |   0 | s              \n",
      "qui             |     0 |     Ã˜ |   0 | qui            \n",
      "ero             |     0 |     Ã˜ |   0 | ero            \n",
      "un              |     0 |     Ã˜ |   0 | un             \n",
      "app             |     0 |     Ã˜ |   3 | app            \n",
      "le              |     0 |     Ã˜ |   3 | le             \n",
      "por             |     0 |     Ã˜ |   0 | por            \n",
      "favor           |     2 |     . |   0 | favor          \n",
      "much            |     0 |     Ã˜ |   1 | much           \n",
      "isi             |     0 |     Ã˜ |   1 | isi            \n",
      "mas             |     0 |     Ã˜ |   1 | mas            \n",
      "h               |     0 |     Ã˜ |   3 | h              \n",
      "h               |     0 |     Ã˜ |   3 | h              \n",
      "PredicciÃ³n: Buenas tardes, quiero un APPLE por favor. Muchisimas HH\n"
     ]
    }
   ],
   "source": [
    "frases = [\"Buenas tardes, quiero un APPLE por favor. Muchisimas HH\"]\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=64,\n",
    "    num_punct_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_cap = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(model, train_loader, train_loader,optimizer, criterion_punct, criterion_cap, device, epochs=200)\n",
    "\n",
    "entrada = \"buenas tardes quiero un apple por favor muchisimas hh\"\n",
    "print(\"PredicciÃ³n:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"modelo_fine_tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_fine_tuned_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 179,236,873\n",
      "Trainable parameters: 87,424,777\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
