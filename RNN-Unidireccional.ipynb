{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malenaalamo/Personal/AA2/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "# pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Descomentar en Windows\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Descomentar en Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del DataLoader (importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast  # FAST tokenizer recomendado\n",
    "\n",
    "# Cargo tokenizar\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "PUNCT_TAGS = {\"Ø\": 0, \",\": 1, \".\": 2, \"?\": 3, \"¿\": 4}\n",
    "CAP_TAGS = {\"lower\": 0, \"init\": 1, \"mix\": 2, \"upper\": 3}\n",
    "\n",
    "def _get_capitalization_type(word):\n",
    "    if not word or word.islower(): return 0\n",
    "    if word.istitle(): return 1\n",
    "    if word.isupper(): return 3\n",
    "    if any(c.isupper() for c in word[1:]): return 2\n",
    "    return 0\n",
    "\n",
    "def get_cap_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Recibe los labels por palabra y devuelve los labels por token para capitalizacion\n",
    "    Si los subtokens pertenecen a la misma palabra, les pone el mismo label (capitalizacion) \n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for word_idx in token_word_map:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(labels_per_word[word_idx])\n",
    "    return labels\n",
    "\n",
    "def get_punct_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Asigna etiquetas de puntuación a los subtokens, siguiendo las reglas:\n",
    "    - ¿ va en el primer subtoken de la palabra.\n",
    "    - ., ?, , van en el último subtoken de la palabra.\n",
    "    - Ø no se asigna a ningún subtoken (todos -100).\n",
    "    \"\"\"\n",
    "    labels = [0] * len(token_word_map)\n",
    "    word_to_token_idxs = {}\n",
    "\n",
    "    # Construimos un diccionario: word_idx -> [lista de posiciones de tokens]\n",
    "    for token_idx, word_idx in enumerate(token_word_map):\n",
    "        if word_idx is not None:\n",
    "            word_to_token_idxs.setdefault(word_idx, []).append(token_idx)\n",
    "\n",
    "    for word_idx, token_idxs in word_to_token_idxs.items():\n",
    "        punct_label = labels_per_word[word_idx]\n",
    "        if punct_label == PUNCT_TAGS[\"¿\"]:\n",
    "            target_idx = token_idxs[0]  # primer subtoken\n",
    "        elif punct_label in {PUNCT_TAGS[\".\"], PUNCT_TAGS[\",\"], PUNCT_TAGS[\"?\"]}:\n",
    "            target_idx = token_idxs[-1]  # último subtoken\n",
    "        else:\n",
    "            continue  # Ø: no se asigna nada\n",
    "\n",
    "        labels[target_idx] = punct_label\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataloader(oraciones_raw, max_length, batch_size, device):\n",
    "    \"\"\"\n",
    "    Crea un DataLoader para entrenar un modelo de restauración de puntuación y capitalización.\n",
    "\n",
    "    A partir de una lista de oraciones correctamente escritas (con puntuación y mayúsculas),\n",
    "    esta función:\n",
    "        - Extrae etiquetas de puntuación y capitalización por palabra.\n",
    "        - \"Corrompe\" el texto al eliminar la puntuación y poner las palabras en minúscula.\n",
    "        - Tokeniza las palabras corruptas usando un tokenizer BERT.\n",
    "        - Alinea las etiquetas con los subtokens del tokenizer.\n",
    "        - Crea tensores para las entradas (input_ids, attention_mask) y etiquetas (puntuación y capitalización).\n",
    "        - Devuelve un DataLoader para entrenamiento en lotes.\n",
    "\n",
    "    Parámetros:\n",
    "        oraciones_raw (List[str]): Lista de oraciones correctamente formateadas.\n",
    "        max_length (int): Longitud máxima de secuencia para truncar/padear.\n",
    "        batch_size (int): Tamaño del batch.\n",
    "        device (str): Dispositivo donde se cargarán los tensores ('cpu' o 'cuda').\n",
    "\n",
    "    Retorna:\n",
    "        DataLoader: DataLoader que entrega batches de (input_ids, attention_mask, punct_labels, cap_labels).\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks = []\n",
    "    punct_labels_list = []\n",
    "    cap_labels_list = []\n",
    "\n",
    "    for sent in oraciones_raw:\n",
    "        # Extraer palabras con puntuación\n",
    "        matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sent)) # Detecta puntuaciones y las splitea\n",
    "        words = []\n",
    "        punct_labels = []\n",
    "        cap_labels = []\n",
    "\n",
    "        for i, m in enumerate(matches): # Recorre cada palabra detectada\n",
    "            word_raw = m.group(0) \n",
    "            clean_word = re.sub(r\"[.,?¿]\", \"\", word_raw) # Limpia la palabra \"Hola!\" -> \"Hola\"\n",
    "\n",
    "            # Puntuación\n",
    "            before = sent[m.start() - 1] if m.start() > 0 else \"\" # Signo anterior\n",
    "            after = sent[m.end()] if m.end() < len(sent) else \"\"  # Signo posterior\n",
    "            if before == '¿':\n",
    "                punct = PUNCT_TAGS[\"¿\"]\n",
    "            elif after in PUNCT_TAGS:\n",
    "                punct = PUNCT_TAGS[after]\n",
    "            else:\n",
    "                punct = PUNCT_TAGS[\"Ø\"]\n",
    "\n",
    "            # Capitalización\n",
    "            cap = _get_capitalization_type(word_raw)\n",
    "\n",
    "            clean_word = clean_word.lower() # Limpia la palabra Hola -> hola\n",
    "\n",
    "            words.append(clean_word)\n",
    "            punct_labels.append(punct)\n",
    "            cap_labels.append(cap)\n",
    "\n",
    "        # Tokenización con BERT\n",
    "        encoding = tokenizer(words,\n",
    "                             is_split_into_words=True,\n",
    "                             return_tensors='pt',\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             max_length=max_length,\n",
    "                             return_attention_mask=True)\n",
    "\n",
    "        # Extraer datos que nos sirven del encoding\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "        attention_mask = encoding['attention_mask'][0]\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Mapea cada subtoken a su palabra\n",
    "\n",
    "        # Alinear etiquetas a subtokens (hasta ahora las teniamos en palabras)\n",
    "        punct_labels_aligned = get_punct_labels_for_tokens(punct_labels, word_ids)\n",
    "        cap_labels_aligned = get_cap_labels_for_tokens(cap_labels, word_ids)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        punct_tensor = torch.tensor(punct_labels_aligned)\n",
    "        cap_tensor = torch.tensor(cap_labels_aligned)\n",
    "\n",
    "        # Aplicar -100 a posiciones de padding\n",
    "        punct_tensor[attention_mask == 0] = -100\n",
    "        cap_tensor[attention_mask == 0] = -100\n",
    "\n",
    "        # Agregar a listas (por oracion)\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "        punct_labels_list.append(punct_tensor)\n",
    "        cap_labels_list.append(cap_tensor)\n",
    "\n",
    "    # Stackear tensores (por batch)\n",
    "    input_ids = torch.stack(input_ids_list).to(device)\n",
    "    attention_masks = torch.stack(attention_masks).to(device)\n",
    "    punct_labels = torch.stack(punct_labels_list).to(device)\n",
    "    cap_labels = torch.stack(cap_labels_list).to(device)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, punct_labels, cap_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra detectada : 'que'\n",
      "Antes del match   : '¿'\n",
      "Después del match : '?'\n",
      "---\n",
      "Palabra detectada : 'Que'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'raro'\n",
      "Antes del match   : ' '\n",
      "Después del match : ','\n",
      "---\n",
      "Palabra detectada : 'a'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mi'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'me'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'gusta'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mas'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'tomar'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'CocaCola'\n",
      "Antes del match   : ' '\n",
      "Después del match : ''\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Linea para entender como el get loader determina las puntuaciones\n",
    "\n",
    "import re\n",
    "\n",
    "# Ejemplo de oración\n",
    "sentence = \"¿que? Que raro, a mi me gusta mas tomar CocaCola\"\n",
    "\n",
    "# Encuentra palabras con posible puntuación al final\n",
    "matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sentence))\n",
    "\n",
    "# Mostramos los resultados\n",
    "for match in matches:\n",
    "    word_raw = match.group(0)\n",
    "    start = match.start()\n",
    "    end = match.end()\n",
    "\n",
    "    # Caracter anterior y posterior\n",
    "    before = sentence[start - 1] if start > 0 else \"\"\n",
    "    after = sentence[end] if end < len(sentence) else \"\"\n",
    "\n",
    "    print(f\"Palabra detectada : '{word_raw}'\")\n",
    "    print(f\"Antes del match   : '{before}'\")\n",
    "    print(f\"Después del match : '{after}'\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n"
     ]
    }
   ],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "DATA_URLS = {\n",
    "    \"train\": \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/train.json\",\n",
    "    \"dev\":   \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/dev.json\",\n",
    "    \"test\":  \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/test.json\",\n",
    "}\n",
    "\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATA_URLS,\n",
    "    field=\"data\",\n",
    ")\n",
    "\n",
    "questions = []\n",
    "\n",
    "\n",
    "for i in range(0, len(raw[\"train\"])):\n",
    "  for p in raw[\"train\"][i]['paragraphs']:\n",
    "    p_questions = [qas['question'] for qas in p['qas']]\n",
    "    questions += p_questions\n",
    "\n",
    "N_QUESTIONS = 5000  # Número de preguntas a procesar\n",
    "questions = questions[:N_QUESTIONS]  # Limitar a N preguntas\n",
    "\n",
    "print(f\"Se descargaron {len(questions)} preguntas en Español.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n"
     ]
    }
   ],
   "source": [
    "dataset_rnn = load_dataset(\"google/wmt24pp\", \"en-es_MX\", split=\"train\")\n",
    "oraciones_rnn = dataset_rnn['target'][1:]\n",
    "\n",
    "print(f\"Se descargaron {len(oraciones_rnn)} oraciones en Español (del dataset del notebook 10).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1413 oraciones sintéticas.\n"
     ]
    }
   ],
   "source": [
    "oraciones_sinteticas = []\n",
    "import json\n",
    "with open('./datasets/datasets.json', 'r') as file:\n",
    "  data = json.load(file)\n",
    "\n",
    "oraciones_sinteticas = data['otros'] + data['marcas']\n",
    "print(f\"Hay {len(oraciones_sinteticas)} oraciones sintéticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- La corriente de pensamiento vigente en Francia era la Ilustración, cuyos principios se basaban en la razón, la igualdad y la libertad.\n",
      "- La Ilustración había servido de impulso a las Trece Colonias norteamericanas para la independencia de su metrópolis europea.\n",
      "- Tanto la influencia de la Ilustración como el ejemplo de los Estados Unidos sirvieron de «trampolín» ideológico para el inicio de la revolución en Francia.\n",
      "- El otro gran lastre para la economía fue la deuda estatal.\n",
      "- En 1788, la relación entre la deuda y la renta nacional bruta en Francia era del 55,6 %, en comparación con el 181,8 % en Gran Bretaña.\n",
      "['Argentina, oficialmente República Argentina,[a]\\u200b es un país soberano de América del Sur, ubicado en el extremo sur y sudeste de ese subcontinente.', 'Adopta la forma de gobierno republicana, democrática, representativa y federal.', 'Poseen Carta Magna, bandera y fuerzas de seguridad propias, el dominio de los recursos naturales circunscriptos en su territorio y delegan los poderes exclusivos al Gobierno Federal.', 'Hasta mediados del siglo XX, fue una de las economías más prósperas del mundo.', 'No obstante, es la segunda economía más importante de Sudamérica —detrás de Brasil— y la 24.º más grande del mundo por PIB nominal.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6648"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import json\n",
    "\n",
    "# API Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "\n",
    "def obtener_frases_wikipedia(titulo, max_frases=100):\n",
    "    try:\n",
    "        pagina = wikipedia.page(titulo)\n",
    "        texto = pagina.content\n",
    "        oraciones = re.split(r'(?<=[.!?])\\s+', texto)\n",
    "        frases = [o.strip() for o in oraciones if 5 < len(o.split()) < 30]\n",
    "        return frases[:max_frases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error al buscar '{titulo}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Ejemplo: obtener 50 frases de un artículo\n",
    "frases = obtener_frases_wikipedia(\"Revolución francesa\", max_frases=50)\n",
    "for f in frases[:5]:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "temas = [\n",
    "    # Países y lugares\n",
    "    'Argentina', 'España', 'México', 'Colombia', 'Chile',\n",
    "    'Perú', 'Uruguay', 'Brasil', 'América Latina', 'Europa',\n",
    "\n",
    "    # Cultura argentina\n",
    "    'Lionel Messi', 'Diego Maradona', 'Lali Esposito', 'Charly Garcia', 'Dillom',\n",
    "    'Tiempos Violentos', 'Relatos Salvajes', 'Universidad de Buenos Aires', 'Rock nacional', 'Cine argentino',\n",
    "\n",
    "    # Historia y política\n",
    "    'Revolucion de Mayo', 'Independencia de Argentina', 'Simón Bolívar', 'Segunda Guerra Mundial', 'Guerra Fría',\n",
    "    'Revolución Francesa', 'Guerra Civil Española', 'Napoleón Bonaparte', 'Nelson Mandela', 'Dictadura militar en Argentina',\n",
    "\n",
    "    # Ciencia y tecnología\n",
    "    'Inteligencia artificial', 'ChatGPT', 'Redes neuronales', 'Robótica', 'Energía solar',\n",
    "    'Vacunas', 'COVID-19', 'Cambio climático', 'Computadora cuántica', 'NASA',\n",
    "\n",
    "    # Cultura general\n",
    "    'El Principito', 'Premio Nobel', 'Frida Kahlo', 'Pablo Picasso', 'Leonardo da Vinci',\n",
    "    'William Shakespeare', 'Gabriel García Márquez', 'Julio Cortázar', 'Literatura latinoamericana', 'Arte contemporáneo',\n",
    "\n",
    "    # Entretenimiento y medios\n",
    "    'Marvel', 'DC Comics', 'Netflix', 'Cine de terror', 'Películas de ciencia ficción',\n",
    "    'Música electrónica', 'Reguetón', 'Spotify', 'YouTube', 'TikTok',\n",
    "\n",
    "    # Deportes\n",
    "    'Fútbol', 'Copa Mundial de la FIFA', 'Juegos Olimpicos', 'Tenis', 'NBA',\n",
    "    'Boca Juniors', 'River Plate', 'Messi vs Ronaldo', 'Fórmula 1', 'Michael Jordan',\n",
    "\n",
    "    # Sociedad y actualidad\n",
    "    'Feminismo', 'Día Internacional de la Mujer', 'Diversidad cultural', 'Migración', 'Pobreza',\n",
    "    'Educación pública', 'Salud mental', 'Medio ambiente', 'Derechos humanos', 'Trabajo remoto',\n",
    "\n",
    "    # Filosofía y pensamiento\n",
    "    'Filosofía', 'Ética', 'Psicología', 'Sigmund Freud', 'Carl Jung',\n",
    "    'Existencialismo', 'Sociología', 'Economía', 'Política', 'Democracia'\n",
    "]\n",
    "\n",
    "def cargar_json_wikipedia(archivo,temas, max_frases=100):\n",
    "    \"\"\"\n",
    "    Carga un archivo JSON con frases de Wikipedia.\n",
    "    \"\"\"\n",
    "    frases_wikipedia = []\n",
    "    for tema in temas:\n",
    "        print(f\"Obteniendo frases de Wikipedia para: {tema}\")\n",
    "        frases = obtener_frases_wikipedia(tema,max_frases=100)\n",
    "        print('Ejemplos de frases obtenidas:')\n",
    "        for f in frases[:2]:\n",
    "            print(f\"- {f}\")\n",
    "        frases_wikipedia.extend(frases)\n",
    "    # Guardar en un archivo JSON\n",
    "    with open(\"datasets/frases_wikipedia.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(frases_wikipedia, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"✅ Frases guardadas en 'datasets/frases_wikipedia.json'\")\n",
    "    \n",
    "\n",
    "# Para actualizar la info de Wikipedia, descomentar la siguiente linea\n",
    "# cargar_json_wikipedia(\"frases_wikipedia.json\",temas, max_frases=100)\n",
    "\n",
    "# Guardar en un archivo JSON\n",
    "with open(\"datasets/frases_wikipedia.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    frases_wikipedia = json.load(f)\n",
    "\n",
    "print(frases_wikipedia[:5])  # muestra las primeras frases\n",
    "\n",
    "len(frases_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "✅ Frases extraídas y guardadas. Total: 947\n",
      "['Matildlta: ¿vos la querés a la abuellta?', 'Ah, me parece una idea de lo más sensata. Al fin y al cabo se mató por eso, ¿no?', '¿Qué sabés de mí? ¡Hablé! Pero antes lavate bien la boca con lavandina, porque yo no tengo nada que reprochar de los 18 años que llevo de casada.', 'Ya te dije que no voy y esto es definitivo y final.', 'Internaron a mi suegra, a la madre.', 'Antonio... Sergio: dejá que la vele en mi casa.', 'En el cuartlto del fondo.', '¿Te das cuenta con lo que hay que lidiar, no?', 'Mamá: dice doña Elisa que nos vayamos todos a la mierda.', '¡Adelante! ¿Cómo estás, querida? Ay masas, si serás mala.']\n"
     ]
    }
   ],
   "source": [
    "def extraer_frases_dialogo(path_txt):\n",
    "    personajes = [\n",
    "    \"Jorge\", \"Susana\", \"Mamá Cora\", \"Sergio\", \"Elvira\", \"Antonio\", \"Nora\",\n",
    "    \"Matilde\", \"Dominga\", \"Felipe\", \"Emilia\", \"Doña Elisa\", \"Doña Gertrudis\",\n",
    "    \"Don Genaro\", \"La Sorda\", \"Peralta\", \"Cacho\", \"Nene Florista\"\n",
    "    ]\n",
    "\n",
    "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # Limpiar saltos múltiples y unificar espacios\n",
    "    raw = re.sub(r\"\\n+\", \"\\n\", raw)\n",
    "\n",
    "    # Construir patrón para encontrar encabezados de personaje\n",
    "    nombres_pattern = \"|\".join(re.escape(p) for p in personajes)\n",
    "    pattern = re.compile(rf\"^({nombres_pattern})\\s*[\\.:–\\-]+\", re.MULTILINE)\n",
    "\n",
    "    frases = []\n",
    "    matches = list(pattern.finditer(raw))\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(raw)\n",
    "        bloque = raw[start:end]\n",
    "\n",
    "        # Limpiar texto: eliminar paréntesis, saltos de línea, espacios múltiples\n",
    "        bloque = re.sub(r\"\\([^)]*\\)\", \"\", bloque)\n",
    "        bloque = bloque.replace(\"\\n\", \" \")\n",
    "        bloque = re.sub(r\"\\s+\", \" \", bloque).strip()\n",
    "\n",
    "        if bloque:\n",
    "            frases.append(bloque)\n",
    "\n",
    "    print(f\"✅ Se extrajeron {len(frases)} frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\")\n",
    "    return frases\n",
    "\n",
    "# Ejecutar la función y guardar a JSON\n",
    "esperando_la_carroza = extraer_frases_dialogo(\"datasets/esperando_la_carroza.txt\")\n",
    "\n",
    "with open(\"datasets/dialogos_esperando_la_carroza.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(esperando_la_carroza, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Mostrar algunas frases aleatorias\n",
    "print(\"✅ Frases extraídas y guardadas. Total:\", len(esperando_la_carroza))\n",
    "print(random.sample(esperando_la_carroza, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Se extrajeron 1000 frases de Relatos Salvajes.\n",
      "['¿Ustedes sacaron los pasajes? No.', 'Tiene que mirar más al salón, bebe.', 'Déme una papa fritas a caballo. Se me quedo con hambre te pido otra cosa.', '¡Forro!', 'Te dan de comer, vive sin preocupaciones, y se queda con un bon grupo hasta le pasa bien.', 'Esto es una casualidad increíble.', 'Esta blindado, no lo vas a poder romper.', 'Igual le tengo cariño. No era una mala persona.', 'Las dos cosas espero.', '¡Te voy buscar y te voy a matar!']\n"
     ]
    }
   ],
   "source": [
    "def extraer_frases_subtitulos(path_txt):\n",
    "    with open(path_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    bloques = re.split(r\"\\n\\s*\\n\", raw)\n",
    "    frases_crudas = []\n",
    "\n",
    "    for bloque in bloques:\n",
    "        lineas = bloque.strip().split(\"\\n\")\n",
    "        if len(lineas) < 3:\n",
    "            continue\n",
    "\n",
    "        texto = \" \".join(lineas[2:])\n",
    "        texto = re.sub(r\"^-\", \"\", texto).strip()\n",
    "        texto = re.sub(r\"\\s*-\\s*\", \" \", texto)\n",
    "        texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "\n",
    "        # ❌ Filtrar frases vacías o solo puntos (como \"...\" o \". . .\")\n",
    "        if not texto or re.fullmatch(r\"[. ]{2,}\", texto):\n",
    "            continue\n",
    "\n",
    "        frases_crudas.append(texto)\n",
    "\n",
    "    # 🔁 Unir frases incompletas (que no terminan en . ! ?)\n",
    "    frases_limpias = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for frase in frases_crudas:\n",
    "        if buffer:\n",
    "            buffer += \" \" + frase\n",
    "        else:\n",
    "            buffer = frase\n",
    "\n",
    "        if re.search(r\"[.!?](['”\\\"])?$\", buffer):\n",
    "            frases_limpias.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "\n",
    "    if buffer:\n",
    "        frases_limpias.append(buffer.strip())\n",
    "\n",
    "    print(f\"✅ Se extrajeron {len(frases_limpias)} frases de Relatos Salvajes.\")\n",
    "    return frases_limpias\n",
    "\n",
    "# Ejemplo de uso\n",
    "frases_relatos_salvajes = extraer_frases_subtitulos(\"datasets/subt_relatos_salvajes.srt\")\n",
    "\n",
    "# Guardar como JSON\n",
    "with open(\"datasets/frases_relatos_salvajes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(frases_relatos_salvajes, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Mostrar algunas frases\n",
    "import random\n",
    "print(random.sample(frases_relatos_salvajes, min(10, len(frases_relatos_salvajes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 16005\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintéticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['¿Por qué no quiere Stoiber que Turquía entre en la UE?',\n",
       " 'Poco después de comenzar la revolución, Napoleón se encontraba en Córcega.',\n",
       " 'El juez ya había recibido amenazas de muerte con anterioridad por este motivo.',\n",
       " 'El tiempo atmosférico, por otro lado, es la condición actual de estos mismos elementos durante períodos de hasta dos semanas.',\n",
       " '¿Cuáles son otros términos empleados para referirse al Medioevo?',\n",
       " 'Consultado el 13 de abril de 2013.',\n",
       " 'Simon Spillane, jefe de operaciones del grupo de presión, declaró que las cerveceras están “ya liderando el camino” en cuanto a la sustentabilidad del envasado, citando el ejemplo de los barriles de cerveza que son reutilizados alrededor de 165 veces antes de ser aplastados y reciclados tras 30 años. Asegura que ellos apoyan los sistemas de reciclaje de reutilización en muchos países.',\n",
       " 'Llama la seguridad ahora',\n",
       " 'El género ha sido producido alrededor de todo el mundo, variando en contenido y estilo entre regiones.',\n",
       " '¿Cómo son los sistemas sanitarios de los países?',\n",
       " 'Le dio sobre todo sus manuscritos, cuadernillos, documentos e instrumentos.',\n",
       " '¿Qué medio ha dado la noticia de las muertes en las minas chinas?',\n",
       " 'Ambos discuten acerca de los riesgos que comporta su actividad y, finalmente, se deciden a atracar la misma cafetería en la que están.',\n",
       " '¿Qué cargo ejerce Margarethe Wahlstrom?',\n",
       " 'Francesco, después de haberlo acompañado durante su estancia en Francia, se quedó con Leonardo hasta su muerte, y administró su herencia durante los cincuenta años posteriores.',\n",
       " 'Pero Mauricio pregunta si no se puede desincluir al casero, y dice que tiene que haber un responsable pues hubo dos muertes y que salió en los noticieros.',\n",
       " 'La principal de estas armas era la bomba atómica.',\n",
       " 'En 1987 Hitachi desarrollo el Sendai Subway 1000, el primer tren autónomo de la historia.',\n",
       " '¿Verdad? Forense. Una vez que el juez dé el permiso, podrán llevarla a',\n",
       " 'Esto se debe al problema del consenso entre especialistas en la definición de los conceptos involucrados y en los procedimientos y técnicas a utilizar.',\n",
       " 'Hoy en día, las democracias existentes son bastante distintas al sistema de gobierno ateniense del que heredan su nombre.',\n",
       " 'Buscaroli de Musicardi.',\n",
       " 'El Golfo de Omán conecta el mar Arábigo con el estrecho de Ormuz.',\n",
       " 'No fotografié la pila o el cargador, pero aquí tienen una foto del panel.',\n",
       " 'En Heródoto Europa es el mayor de los continentes, extendiéndose al norte del Mediterráneo desde las Columnas de Hércules del estrecho de Gibraltar hasta más allá del río Indo.',\n",
       " 'Jawed Karim, cofundador de YouTube, calificó la actualización como \"una idea estúpida\" y dijo que el verdadero motivo del cambio \"no es bueno y no se hará público\".',\n",
       " 'Y a usted mis condolencias. Pero por qué hizo semejante cosa.',\n",
       " '¿Cómo cree Dely Valdés que se siente alguna gente de Oviedo?',\n",
       " '¿Qué novela de la saga no había salido a la luz durante la escritura del guion de la película de Harry Potter y la piedra filosofal?',\n",
       " '¿Cuándo se empezó a trabajar en las teorías de la música?']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def limpiar_simbolos(frases):\n",
    "    frases_limpias = []\n",
    "    for frase in frases:\n",
    "        # Eliminar cualquier secuencia de dos o más puntos (incluyendo con espacios): ... . .. etc.\n",
    "        frase = re.sub(r\"(\\.\\s*){2,}\", \"\", frase)\n",
    "        # Eliminar cualquier caracter que NO sea letra, número, espacio o los signos permitidos\n",
    "        frase = re.sub(r\"[^a-zA-ZáéíóúÁÉÍÓÚñÑüÜ0-9¿?,. ]+\", \"\", frase)\n",
    "        # Reemplazar múltiples espacios por uno solo\n",
    "        frase = re.sub(r\"\\s+\", \" \", frase).strip()\n",
    "        frases_limpias.append(frase)\n",
    "    return frases_limpias\n",
    "\n",
    "# Eliminar signos de exclamación de las frases\n",
    "frases_relatos_salvajes = limpiar_simbolos(frases_relatos_salvajes)\n",
    "esperando_la_carroza = limpiar_simbolos(esperando_la_carroza)\n",
    "\n",
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintéticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "\n",
    "# Muestra algunas oraciones aleatorias\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15204\n",
      "801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class PunctuationCapitalizationRNN(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, num_punct_classes, num_cap_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model                        # ahora el modelo completo\n",
    "        self.projection = nn.Linear(\n",
    "            self.bert.config.hidden_size, hidden_dim\n",
    "        )\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=2,\n",
    "                           batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.punct_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_punct_classes)\n",
    "        )\n",
    "        self.cap_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_cap_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # BERT retorna last_hidden_state: (B, T, H_bert)\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        projected = self.projection(hidden_states)   # (B, T, hidden_dim)\n",
    "\n",
    "        # mismo packing/padding de antes\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1)\n",
    "            packed = pack_padded_sequence(projected, lengths.cpu(),\n",
    "                                          batch_first=True, enforce_sorted=False)\n",
    "            rnn_out_packed, _ = self.rnn(packed)\n",
    "            rnn_out, _ = pad_packed_sequence(\n",
    "                rnn_out_packed, batch_first=True, total_length=projected.size(1)\n",
    "            )\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(projected)\n",
    "\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        punct_logits = self.punct_classifier(rnn_out)\n",
    "        cap_logits   = self.cap_classifier(rnn_out)\n",
    "        return punct_logits, cap_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Funcion de entrenamiento\n",
    "def train(model, dataloader_train, dataloader_test, optimizer, criterion_punct, criterion_cap, device, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_punct = criterion_punct(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "            loss_cap = criterion_cap(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "            loss = loss_punct + loss_cap\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader_train)\n",
    "\n",
    "        \"\"\"\n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, punct_labels, cap_labels in dataloader_test:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                punct_labels = punct_labels.to(device)\n",
    "                cap_labels = cap_labels.to(device)\n",
    "\n",
    "                punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                loss_punct = criterion(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "                loss_cap = criterion(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "                loss = loss_punct + loss_cap\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader_test)\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    inv_punct = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "    inv_cap = {v: k for k, v in CAP_TAGS.items()}\n",
    "\n",
    "    all_true_punct = []\n",
    "    all_pred_punct = []\n",
    "    all_true_cap = []\n",
    "    all_pred_cap = []\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids)\n",
    "\n",
    "            pred_punct = torch.argmax(punct_logits, dim=-1)\n",
    "            pred_cap = torch.argmax(cap_logits, dim=-1)\n",
    "\n",
    "            # Máscara para ignorar tokens sin etiqueta\n",
    "            mask = (punct_labels != -100)\n",
    "\n",
    "            # Aplicar máscara y aplanar para comparación\n",
    "            all_true_punct.extend(punct_labels[mask].cpu().numpy())\n",
    "            all_pred_punct.extend(pred_punct[mask].cpu().numpy())\n",
    "\n",
    "            all_true_cap.extend(cap_labels[mask].cpu().numpy())\n",
    "            all_pred_cap.extend(pred_cap[mask].cpu().numpy())\n",
    "\n",
    "    print(\"Unique classes in true labels:\", set(all_true_cap))\n",
    "    print(\"Unique classes in predictions:\", set(all_pred_cap))\n",
    "\n",
    "    print(\"🔍 Unique true cap labels:\", set(all_true_cap))\n",
    "    print(\"🔍 Unique pred cap labels:\", set(all_pred_cap))\n",
    "    print(\"🔍 Unique true punct labels:\", set(all_true_punct))\n",
    "    print(\"🔍 Unique pred punct labels:\", set(all_pred_punct))\n",
    "\n",
    "    # Accuracy generales\n",
    "    punct_acc = np.mean(np.array(all_true_punct) == np.array(all_pred_punct))\n",
    "    cap_acc = np.mean(np.array(all_true_cap) == np.array(all_pred_cap))\n",
    "\n",
    "    print(\"📌 Punctuation Accuracy:     {:.4f}\".format(punct_acc))\n",
    "    print(\"🔡 Capitalization Accuracy: {:.4f}\".format(cap_acc))\n",
    "\n",
    "    # Reportes detallados\n",
    "    print(\"\\n📊 Punctuation classification report:\")\n",
    "    print(classification_report(all_true_punct, all_pred_punct, target_names=[inv_punct[i] for i in range(len(inv_punct))]))\n",
    "\n",
    "    print(\"\\n📊 Capitalization classification report:\")\n",
    "    print(classification_report(all_true_cap, all_pred_cap, target_names=[inv_cap[i] for i in range(len(inv_cap))]))\n",
    "\n",
    "    return punct_acc, cap_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m cap_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, punct_labels, cap_labels \u001b[38;5;129;01min\u001b[39;00m dataloader_train:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Convertir a CPU y a numpy para contar\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     punct_labels_np \u001b[38;5;241m=\u001b[39m \u001b[43mpunct_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     37\u001b[0m     cap_labels_np \u001b[38;5;241m=\u001b[39m cap_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Contar etiquetas válidas (ignorando -100)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Embeddings de BERT\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Congelar la mayoría de los parámetros de BERT salvo los últimos N layers y el pooler\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# le quiero pasar el weight param al criterion para mejorar el desbalanceo de clases en base a un counter de tokens\n",
    "# Contar ocurrencias de cada etiqueta de puntuación y capitalización\n",
    "punct_counter = Counter()\n",
    "cap_counter = Counter()\n",
    "for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "    # Convertir a CPU y a numpy para contar\n",
    "    punct_labels_np = punct_labels.cpu().numpy()\n",
    "    cap_labels_np = cap_labels.cpu().numpy()\n",
    "\n",
    "    # Contar etiquetas válidas (ignorando -100)\n",
    "    valid_punct = punct_labels_np[punct_labels_np != -100]\n",
    "    valid_cap = cap_labels_np[cap_labels_np != -100]\n",
    "\n",
    "    punct_counter.update(valid_punct)\n",
    "    cap_counter.update(valid_cap)\n",
    "\n",
    "# como las clases estan desbalanceadas, calculamos los pesos inversos\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap = sum(cap_counter.values())\n",
    "beta = 0.7\n",
    "\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap   = sum(cap_counter.values())\n",
    "\n",
    "punct_weights = {\n",
    "    tag: (total_punct / count)**beta\n",
    "    for tag, count in punct_counter.items()\n",
    "}\n",
    "cap_weights = {\n",
    "    tag: (total_cap / count)**beta\n",
    "    for tag, count in cap_counter.items()\n",
    "}\n",
    "\n",
    "# pasar a tensor (clamp opcional para evitar extremos)\n",
    "punct_weights_tensor = torch.tensor(\n",
    "    [punct_weights.get(i, 1.0) for i in range(len(PUNCT_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "cap_weights_tensor = torch.tensor(\n",
    "    [cap_weights.get(i, 1.0) for i in range(len(CAP_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100, weight=punct_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "# Entrenamiento \n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion_punct=criterion_punct, criterion_cap = criterion_cap, device=device, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in true labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "Unique classes in predictions: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "🔍 Unique true cap labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "🔍 Unique pred cap labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3)}\n",
      "🔍 Unique true punct labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)}\n",
      "🔍 Unique pred punct labels: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (15390,) (16992,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 103\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Accuracy generales\u001b[39;00m\n\u001b[1;32m    102\u001b[0m punct_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(all_true_punct) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_pred_punct))\n\u001b[0;32m--> 103\u001b[0m cap_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_true_cap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_pred_cap\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Punctuation Accuracy:     \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(punct_acc))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔡 Capitalization Accuracy: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cap_acc))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (15390,) (16992,) "
     ]
    }
   ],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_reconstruct(model, sentence, tokenizer, device, max_length=64, verbose=True):\n",
    "    model.eval()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        punct_logits, cap_logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    pred_punct = torch.argmax(punct_logits, dim=-1)[0].cpu().tolist()\n",
    "    pred_cap = torch.argmax(cap_logits, dim=-1)[0].cpu().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    INV_PUNCT_TAGS = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "\n",
    "    final_words = []\n",
    "    current_word = \"\"\n",
    "    current_cap = 0\n",
    "    current_punct = 0\n",
    "    new_word = True\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\n🔍 Predicción token por token:\")\n",
    "        print(f\"{'TOKEN':15s} | {'PUNCT':>5s} | {'SIGNO':>5s} | {'CAP':>3s} | {'FINAL':15s}\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "    for i, (token, punct_label, cap_label) in enumerate(zip(tokens, pred_punct, pred_cap)):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] or attention_mask[0, i].item() == 0:\n",
    "            continue\n",
    "\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += clean_token\n",
    "            if punct_label != 0:\n",
    "                current_punct = punct_label  # usar puntuación del último subtoken relevante\n",
    "        else:\n",
    "            if current_word:\n",
    "                # cerrar palabra anterior\n",
    "                word = current_word\n",
    "                # aplicar capitalización a toda la palabra\n",
    "                if current_cap == 1:\n",
    "                    word = word.capitalize()\n",
    "                elif current_cap == 2:\n",
    "                    word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "                elif current_cap == 3:\n",
    "                    word = word.upper()\n",
    "                # aplicar puntuación del último subtoken\n",
    "                punct = INV_PUNCT_TAGS.get(current_punct, \"Ø\")\n",
    "                if punct == \"¿\":\n",
    "                    word = \"¿\" + word\n",
    "                elif punct != \"Ø\":\n",
    "                    word = word + punct\n",
    "                final_words.append(word)\n",
    "\n",
    "            # empezar nueva palabra\n",
    "            current_word = clean_token\n",
    "            current_cap = cap_label\n",
    "            current_punct = punct_label if punct_label != 0 else 0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{clean_token:15s} | {punct_label:5d} | {INV_PUNCT_TAGS.get(punct_label, 'Ø'):>5s} | {cap_label:3d} | {clean_token:15s}\")\n",
    "\n",
    "    # Procesar última palabra\n",
    "    if current_word:\n",
    "        word = current_word\n",
    "        if current_cap == 1:\n",
    "            word = word.capitalize()\n",
    "        elif current_cap == 2:\n",
    "            word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "        elif current_cap == 3:\n",
    "            word = word.upper()\n",
    "        punct = INV_PUNCT_TAGS.get(current_punct, \"Ø\")\n",
    "        if punct == \"¿\":\n",
    "            word = \"¿\" + word\n",
    "        elif punct != \"Ø\":\n",
    "            word = word + punct\n",
    "        final_words.append(word)\n",
    "\n",
    "    return \" \".join(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola como estas => HOLA. COMO. ESTAS.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"hola como estas\"\n",
    "print(f\"{entrada} => {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.9041\n",
      "Epoch 2 | Train Loss: 2.7525\n",
      "Epoch 3 | Train Loss: 2.8042\n",
      "Epoch 4 | Train Loss: 2.4886\n",
      "Epoch 5 | Train Loss: 2.0364\n",
      "Epoch 6 | Train Loss: 1.7816\n",
      "Epoch 7 | Train Loss: 1.6085\n",
      "Epoch 8 | Train Loss: 1.9414\n",
      "Epoch 9 | Train Loss: 1.6012\n",
      "Epoch 10 | Train Loss: 1.5608\n",
      "Epoch 11 | Train Loss: 1.5680\n",
      "Epoch 12 | Train Loss: 1.6294\n",
      "Epoch 13 | Train Loss: 1.6771\n",
      "Epoch 14 | Train Loss: 1.5966\n",
      "Epoch 15 | Train Loss: 1.5496\n",
      "Epoch 16 | Train Loss: 1.4163\n",
      "Epoch 17 | Train Loss: 1.6198\n",
      "Epoch 18 | Train Loss: 1.6080\n",
      "Epoch 19 | Train Loss: 1.4456\n",
      "Epoch 20 | Train Loss: 1.4271\n",
      "Epoch 21 | Train Loss: 1.4963\n",
      "Epoch 22 | Train Loss: 1.6729\n",
      "Epoch 23 | Train Loss: 1.5201\n",
      "Epoch 24 | Train Loss: 1.5008\n",
      "Epoch 25 | Train Loss: 1.4461\n",
      "Epoch 26 | Train Loss: 1.5343\n",
      "Epoch 27 | Train Loss: 1.4170\n",
      "Epoch 28 | Train Loss: 1.5351\n",
      "Epoch 29 | Train Loss: 1.2816\n",
      "Epoch 30 | Train Loss: 1.3075\n",
      "Epoch 31 | Train Loss: 1.5001\n",
      "Epoch 32 | Train Loss: 1.3044\n",
      "Epoch 33 | Train Loss: 1.3603\n",
      "Epoch 34 | Train Loss: 1.6303\n",
      "Epoch 35 | Train Loss: 1.3930\n",
      "Epoch 36 | Train Loss: 1.4275\n",
      "Epoch 37 | Train Loss: 1.2022\n",
      "Epoch 38 | Train Loss: 1.3402\n",
      "Epoch 39 | Train Loss: 1.2695\n",
      "Epoch 40 | Train Loss: 1.3167\n",
      "Epoch 41 | Train Loss: 1.3456\n",
      "Epoch 42 | Train Loss: 1.0623\n",
      "Epoch 43 | Train Loss: 1.2127\n",
      "Epoch 44 | Train Loss: 1.3160\n",
      "Epoch 45 | Train Loss: 1.4177\n",
      "Epoch 46 | Train Loss: 1.3338\n",
      "Epoch 47 | Train Loss: 1.2174\n",
      "Epoch 48 | Train Loss: 1.2938\n",
      "Epoch 49 | Train Loss: 1.2318\n",
      "Epoch 50 | Train Loss: 1.3380\n",
      "Epoch 51 | Train Loss: 1.1478\n",
      "Epoch 52 | Train Loss: 1.0414\n",
      "Epoch 53 | Train Loss: 1.0351\n",
      "Epoch 54 | Train Loss: 1.0681\n",
      "Epoch 55 | Train Loss: 1.0730\n",
      "Epoch 56 | Train Loss: 1.0859\n",
      "Epoch 57 | Train Loss: 1.1499\n",
      "Epoch 58 | Train Loss: 1.1939\n",
      "Epoch 59 | Train Loss: 1.0210\n",
      "Epoch 60 | Train Loss: 1.0601\n",
      "Epoch 61 | Train Loss: 1.1526\n",
      "Epoch 62 | Train Loss: 0.9718\n",
      "Epoch 63 | Train Loss: 1.2524\n",
      "Epoch 64 | Train Loss: 0.9741\n",
      "Epoch 65 | Train Loss: 0.9509\n",
      "Epoch 66 | Train Loss: 1.2484\n",
      "Epoch 67 | Train Loss: 0.8212\n",
      "Epoch 68 | Train Loss: 0.9991\n",
      "Epoch 69 | Train Loss: 0.8325\n",
      "Epoch 70 | Train Loss: 1.0543\n",
      "Epoch 71 | Train Loss: 0.9744\n",
      "Epoch 72 | Train Loss: 0.8999\n",
      "Epoch 73 | Train Loss: 1.0224\n",
      "Epoch 74 | Train Loss: 0.8276\n",
      "Epoch 75 | Train Loss: 0.7658\n",
      "Epoch 76 | Train Loss: 0.7095\n",
      "Epoch 77 | Train Loss: 0.5454\n",
      "Epoch 78 | Train Loss: 0.6062\n",
      "Epoch 79 | Train Loss: 0.6741\n",
      "Epoch 80 | Train Loss: 0.7357\n",
      "Epoch 81 | Train Loss: 0.8277\n",
      "Epoch 82 | Train Loss: 0.5813\n",
      "Epoch 83 | Train Loss: 0.7103\n",
      "Epoch 84 | Train Loss: 0.7607\n",
      "Epoch 85 | Train Loss: 1.2008\n",
      "Epoch 86 | Train Loss: 1.0331\n",
      "Epoch 87 | Train Loss: 0.8042\n",
      "Epoch 88 | Train Loss: 0.7970\n",
      "Epoch 89 | Train Loss: 0.8939\n",
      "Epoch 90 | Train Loss: 0.7259\n",
      "Epoch 91 | Train Loss: 0.6029\n",
      "Epoch 92 | Train Loss: 0.5131\n",
      "Epoch 93 | Train Loss: 0.6188\n",
      "Epoch 94 | Train Loss: 0.7534\n",
      "Epoch 95 | Train Loss: 0.9649\n",
      "Epoch 96 | Train Loss: 0.6132\n",
      "Epoch 97 | Train Loss: 0.7953\n",
      "Epoch 98 | Train Loss: 0.5279\n",
      "Epoch 99 | Train Loss: 0.5407\n",
      "Epoch 100 | Train Loss: 0.4666\n",
      "Epoch 101 | Train Loss: 0.5466\n",
      "Epoch 102 | Train Loss: 0.5077\n",
      "Epoch 103 | Train Loss: 0.4878\n",
      "Epoch 104 | Train Loss: 0.6902\n",
      "Epoch 105 | Train Loss: 0.5986\n",
      "Epoch 106 | Train Loss: 0.5989\n",
      "Epoch 107 | Train Loss: 0.8205\n",
      "Epoch 108 | Train Loss: 0.6459\n",
      "Epoch 109 | Train Loss: 0.6935\n",
      "Epoch 110 | Train Loss: 0.8354\n",
      "Epoch 111 | Train Loss: 0.3638\n",
      "Epoch 112 | Train Loss: 0.4980\n",
      "Epoch 113 | Train Loss: 0.4571\n",
      "Epoch 114 | Train Loss: 0.6826\n",
      "Epoch 115 | Train Loss: 0.3637\n",
      "Epoch 116 | Train Loss: 0.4391\n",
      "Epoch 117 | Train Loss: 0.4531\n",
      "Epoch 118 | Train Loss: 0.3039\n",
      "Epoch 119 | Train Loss: 0.4022\n",
      "Epoch 120 | Train Loss: 0.5950\n",
      "Epoch 121 | Train Loss: 0.4299\n",
      "Epoch 122 | Train Loss: 0.3995\n",
      "Epoch 123 | Train Loss: 0.4307\n",
      "Epoch 124 | Train Loss: 0.3340\n",
      "Epoch 125 | Train Loss: 0.3994\n",
      "Epoch 126 | Train Loss: 0.3369\n",
      "Epoch 127 | Train Loss: 0.4564\n",
      "Epoch 128 | Train Loss: 0.4133\n",
      "Epoch 129 | Train Loss: 0.2869\n",
      "Epoch 130 | Train Loss: 0.3591\n",
      "Epoch 131 | Train Loss: 0.2112\n",
      "Epoch 132 | Train Loss: 0.5459\n",
      "Epoch 133 | Train Loss: 0.4612\n",
      "Epoch 134 | Train Loss: 0.2449\n",
      "Epoch 135 | Train Loss: 0.1646\n",
      "Epoch 136 | Train Loss: 0.2365\n",
      "Epoch 137 | Train Loss: 0.3438\n",
      "Epoch 138 | Train Loss: 0.2881\n",
      "Epoch 139 | Train Loss: 0.4048\n",
      "Epoch 140 | Train Loss: 0.0992\n",
      "Epoch 141 | Train Loss: 0.1102\n",
      "Epoch 142 | Train Loss: 0.2197\n",
      "Epoch 143 | Train Loss: 0.3388\n",
      "Epoch 144 | Train Loss: 0.1429\n",
      "Epoch 145 | Train Loss: 0.2174\n",
      "Epoch 146 | Train Loss: 0.3072\n",
      "Epoch 147 | Train Loss: 0.1916\n",
      "Epoch 148 | Train Loss: 0.2822\n",
      "Epoch 149 | Train Loss: 0.2708\n",
      "Epoch 150 | Train Loss: 0.1534\n",
      "Epoch 151 | Train Loss: 0.0734\n",
      "Epoch 152 | Train Loss: 0.2549\n",
      "Epoch 153 | Train Loss: 0.0962\n",
      "Epoch 154 | Train Loss: 0.3004\n",
      "Epoch 155 | Train Loss: 0.5569\n",
      "Epoch 156 | Train Loss: 0.8348\n",
      "Epoch 157 | Train Loss: 0.3751\n",
      "Epoch 158 | Train Loss: 0.1608\n",
      "Epoch 159 | Train Loss: 0.1561\n",
      "Epoch 160 | Train Loss: 0.4134\n",
      "Epoch 161 | Train Loss: 0.2224\n",
      "Epoch 162 | Train Loss: 0.6320\n",
      "Epoch 163 | Train Loss: 1.1164\n",
      "Epoch 164 | Train Loss: 0.3918\n",
      "Epoch 165 | Train Loss: 0.1651\n",
      "Epoch 166 | Train Loss: 1.6350\n",
      "Epoch 167 | Train Loss: 0.3820\n",
      "Epoch 168 | Train Loss: 0.1999\n",
      "Epoch 169 | Train Loss: 1.0317\n",
      "Epoch 170 | Train Loss: 0.6835\n",
      "Epoch 171 | Train Loss: 0.1613\n",
      "Epoch 172 | Train Loss: 0.1460\n",
      "Epoch 173 | Train Loss: 0.4859\n",
      "Epoch 174 | Train Loss: 0.3728\n",
      "Epoch 175 | Train Loss: 0.2635\n",
      "Epoch 176 | Train Loss: 0.4728\n",
      "Epoch 177 | Train Loss: 0.2598\n",
      "Epoch 178 | Train Loss: 0.6604\n",
      "Epoch 179 | Train Loss: 0.2222\n",
      "Epoch 180 | Train Loss: 0.3213\n",
      "Epoch 181 | Train Loss: 0.2924\n",
      "Epoch 182 | Train Loss: 0.3553\n",
      "Epoch 183 | Train Loss: 0.2332\n",
      "Epoch 184 | Train Loss: 0.2246\n",
      "Epoch 185 | Train Loss: 0.1460\n",
      "Epoch 186 | Train Loss: 0.2774\n",
      "Epoch 187 | Train Loss: 0.3304\n",
      "Epoch 188 | Train Loss: 0.3077\n",
      "Epoch 189 | Train Loss: 0.2289\n",
      "Epoch 190 | Train Loss: 0.2031\n",
      "Epoch 191 | Train Loss: 0.2062\n",
      "Epoch 192 | Train Loss: 0.1219\n",
      "Epoch 193 | Train Loss: 0.2051\n",
      "Epoch 194 | Train Loss: 0.3155\n",
      "Epoch 195 | Train Loss: 0.2491\n",
      "Epoch 196 | Train Loss: 0.0833\n",
      "Epoch 197 | Train Loss: 0.1502\n",
      "Epoch 198 | Train Loss: 0.1815\n",
      "Epoch 199 | Train Loss: 0.1731\n",
      "Epoch 200 | Train Loss: 0.0969\n",
      "\n",
      "🔍 Predicción token por token:\n",
      "TOKEN           | PUNCT | SIGNO | CAP | FINAL          \n",
      "-------------------------------------------------------\n",
      "buenas          |     0 |     Ø |   1 | buenas         \n",
      "tarde           |     0 |     Ø |   0 | tarde          \n",
      "s               |     1 |     , |   0 | s              \n",
      "qui             |     0 |     Ø |   0 | qui            \n",
      "ero             |     0 |     Ø |   0 | ero            \n",
      "un              |     0 |     Ø |   0 | un             \n",
      "app             |     0 |     Ø |   3 | app            \n",
      "le              |     0 |     Ø |   3 | le             \n",
      "por             |     0 |     Ø |   0 | por            \n",
      "favor           |     2 |     . |   0 | favor          \n",
      "much            |     0 |     Ø |   1 | much           \n",
      "isi             |     0 |     Ø |   1 | isi            \n",
      "mas             |     0 |     Ø |   1 | mas            \n",
      "h               |     0 |     Ø |   3 | h              \n",
      "h               |     0 |     Ø |   3 | h              \n",
      "Predicción: Buenas tardes, quiero un APPLE por favor. Muchisimas HH\n"
     ]
    }
   ],
   "source": [
    "frases = [\"Buenas tardes, quiero un APPLE por favor. Muchisimas HH\"]\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=64,\n",
    "    num_punct_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_cap = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(model, train_loader, train_loader,optimizer, criterion_punct, criterion_cap, device, epochs=200)\n",
    "\n",
    "entrada = \"buenas tardes quiero un apple por favor muchisimas hh\"\n",
    "print(\"Predicción:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"modelo_fine_tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_fine_tuned_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 179,236,873\n",
      "Trainable parameters: 87,424,777\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
