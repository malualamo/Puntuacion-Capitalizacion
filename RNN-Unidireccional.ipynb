{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malepersonal/Git/Puntualizacion-Capitalizacion/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/malepersonal/Git/Puntualizacion-Capitalizacion/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.datautils import *\n",
    "from utils.MLutils import *\n",
    "from utils.resources import *\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "#pip install --upgrade datasets\n",
    "\n",
    "# linea que corre torch acelerado en mac\n",
    "# pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando: mps\n"
     ]
    }
   ],
   "source": [
    "linux = False\n",
    "device = None\n",
    "\n",
    "if linux:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 1: Conjunto de preguntas en espa;ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n"
     ]
    }
   ],
   "source": [
    "questions, question_for_mixture = get_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 2: Dataset provisto para Notebook 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n"
     ]
    }
   ],
   "source": [
    "oraciones_rnn = get_notebook_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 3: Dataset sintetico generado con Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1413 oraciones sintéticas.\n"
     ]
    }
   ],
   "source": [
    "oraciones_sinteticas = get_gemini_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 4: Articulos de Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Argentina, oficialmente República Argentina,[a]\\u200b es un país soberano de América del Sur, ubicado en el extremo sur y sudeste de ese subcontinente.', 'Adopta la forma de gobierno republicana, democrática, representativa y federal.', 'Poseen Carta Magna, bandera y fuerzas de seguridad propias, el dominio de los recursos naturales circunscriptos en su territorio y delegan los poderes exclusivos al Gobierno Federal.', 'Hasta mediados del siglo XX, fue una de las economías más prósperas del mundo.', 'No obstante, es la segunda economía más importante de Sudamérica —detrás de Brasil— y la 24.º más grande del mundo por PIB nominal.']\n"
     ]
    }
   ],
   "source": [
    "frases_wikipedia = get_wikipedia_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 5: Subtitulos de peliculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "✅ Frases extraídas y guardadas. Total: 947\n",
      "['Claro.', 'Que no. Ojalá se te atragante la comida y te tengan que llevar al hospital medio ahogado, mirá.', '¿Cómo? Tengo que poner la edad justa.', 'Preguntó por Benigno, ese amigo tuyo que es tan importante.', 'Sí, soy la esposa. ¿Qué? ¡¿Qué?! ¡¿Pero qué dice?! Pero, señor, usted dice cada cosa que... Además hable claro. ¡Hable claro! Bueno sáquese lo que tiene en la boca y hable claro. Repítamelo, quiere. ¿Pero es un chiste? Ah, no sería la primera... No sería la primera vez. No le da vergüenza en un Pero señor: no oigo. No lo que pasa es que yo no me fijé. Pero si los propios hijos, que son la sangre de su sangre, la carne de su carne, no se dieron cuenta...', 'Seguramente mamá Cora debe estar en lo de Emilia y los hombres deben haber ido a la rotisería a comprar comida.', 'Y bueno, te querrá ayudar.', 'Matildlta: traé las masltas, querida, para convidar a tus queridos tíos.', 'No.', 'Yo que soy la menos culpable de las tres tengo unos remordimientos espantosos.']\n",
      "✅ Se extrajeron 1000 frases de Relatos Salvajes.\n"
     ]
    }
   ],
   "source": [
    "esperando_la_carroza, frases_relatos_salvajes = get_pelis_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 6: Mixture of preguntas y afirmaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿De qué ha sido acusado Bill Cosby? La resolución de la ONU fue vetada por un miembro permanente.',\n",
       " 'Carmen, ¿sabes si va a llover esta tarde en la ciudad? ¿Cómo se abrevia comúnmente el nombre de la catedral de San Esteban?',\n",
       " 'La ciudad de Cartago en Túnez fue una gran potencia marítima. ¿Dónde se darán cita los representantes de ambos grupos?',\n",
       " '¿Cuándo fue derrotada Amida? El anuncio de Coca-Cola era muy emotivo.',\n",
       " '¿Qué qué color es la leyenda de Hollywood? ¿Cómo es el clima de Villahermosa?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "\n",
    "cant_oraciones = len(oraciones_sinteticas)\n",
    "question_for_mixture = [re.sub(r'[\\\\\\(\\)!¡“]', '', unicodedata.normalize(\"NFC\", q).strip()) for q in question_for_mixture]\n",
    "oraciones_sinteticas = [re.sub(r'[\\\\\\(\\)!¡“]', '', unicodedata.normalize(\"NFC\", a).strip()) for a in oraciones_sinteticas]\n",
    "\n",
    "tanda_1 = question_for_mixture[:cant_oraciones]\n",
    "question_affirmation = [f\"{q} {a}\" for q, a in zip(tanda_1, oraciones_sinteticas)]\n",
    "\n",
    "tanda_2 = question_for_mixture[cant_oraciones:2*cant_oraciones]\n",
    "affirmation_question = [f\"{a} {q}\" for q, a in zip(tanda_2, oraciones_sinteticas)]\n",
    "\n",
    "tanda_3 = question_for_mixture[2*cant_oraciones:3*cant_oraciones]\n",
    "tanda_3_shuffled = random.sample(tanda_3, len(tanda_3))\n",
    "question_question = [f\"{q} {p}\" for q, p in zip(tanda_3, tanda_3_shuffled)]\n",
    "\n",
    "mixtures = question_affirmation + affirmation_question + question_question\n",
    "\n",
    "random.sample(mixtures, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos las fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 20244\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintéticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Cantidad de oraciones de mixture: 4239\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['¿Qué ciudades inglesas fracasaron en su tentativa de albergar unos Juegos Olímpicos? La cumbre de la OTAN se centró en la amenaza de Rusia.',\n",
       " 'Valencia es el mejor 10 y debe jugar, sin desmerecer a Alonso ni a Villa.',\n",
       " 'El 25 de febrero de 1912, Jung fundó la Sociedad de Intereses Psicoanalíticos y con este acto formalizó el camino hacia su propia versión del psicoanálisis.',\n",
       " 'Pulp Fiction en Rotten Tomatoes (en inglés).',\n",
       " 'Conducir un BMW es una gran experiencia.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes + mixtures\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintéticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "print('Cantidad de oraciones de mixture:',len(mixtures))\n",
    "\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos en conjuntos de `train` y `test` con el tokenizer de `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19231\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m punct_end_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     39\u001b[0m cap_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, punct_start_labels, punct_end_labels, cap_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader_train\u001b[49m:\n\u001b[1;32m     41\u001b[0m     punct_start_np \u001b[38;5;241m=\u001b[39m punct_start_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     42\u001b[0m     punct_end_np   \u001b[38;5;241m=\u001b[39m punct_end_labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader_train' is not defined"
     ]
    }
   ],
   "source": [
    "from train.RNN import PunctuationCapitalizationRNN\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Congelar todo primero\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Congelar embeddings explícitamente (ya congelados, pero para mayor claridad)\n",
    "for param in bert_model.embeddings.word_embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar últimas N capas\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Descongelar pooler\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_start_classes=len(PUNCT_START_TAGS),\n",
    "    num_punct_end_classes=len(PUNCT_END_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# le quiero pasar el weight param al criterion para mejorar el desbalanceo de clases en base a un counter de tokens\n",
    "# Contar ocurrencias de cada etiqueta de puntuación y capitalización\n",
    "punct_start_counter = Counter()\n",
    "punct_end_counter = Counter()\n",
    "cap_counter = Counter()\n",
    "for input_ids, attention_mask, punct_start_labels, punct_end_labels, cap_labels in dataloader_train:\n",
    "    punct_start_np = punct_start_labels.cpu().numpy()\n",
    "    punct_end_np   = punct_end_labels.cpu().numpy()\n",
    "    cap_np         = cap_labels.cpu().numpy()\n",
    "\n",
    "    # Contar etiquetas válidas (ignorando -100)\n",
    "    valid_start = punct_start_np[punct_start_np != -100]\n",
    "    valid_end   = punct_end_np[punct_end_np != -100]\n",
    "    valid_cap   = cap_np[cap_np != -100]\n",
    "\n",
    "    punct_start_counter.update(valid_start)\n",
    "    punct_end_counter.update(valid_end)\n",
    "    cap_counter.update(valid_cap)\n",
    "\n",
    "# como las clases estan desbalanceadas, calculamos los pesos inversos\n",
    "total_start = sum(punct_start_counter.values())\n",
    "total_end   = sum(punct_end_counter.values())\n",
    "total_cap   = sum(cap_counter.values())\n",
    "beta = 0.7\n",
    "\n",
    "start_weights = {\n",
    "    tag: (total_start / count)**beta\n",
    "    for tag, count in punct_start_counter.items()\n",
    "}\n",
    "end_weights = {\n",
    "    tag: (total_end / count)**beta\n",
    "    for tag, count in punct_end_counter.items()\n",
    "}\n",
    "cap_weights = {\n",
    "    tag: (total_cap / count)**beta\n",
    "    for tag, count in cap_counter.items()\n",
    "}\n",
    "\n",
    "# Tensores\n",
    "start_weights_tensor = torch.tensor(\n",
    "    [start_weights.get(i, 1.0) for i in range(len(PUNCT_START_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "end_weights_tensor = torch.tensor(\n",
    "    [end_weights.get(i, 1.0) for i in range(len(PUNCT_END_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "cap_weights_tensor = torch.tensor(\n",
    "    [cap_weights.get(i, 1.0) for i in range(len(CAP_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "\n",
    "criterion_start = nn.CrossEntropyLoss(ignore_index=-100, weight=start_weights_tensor)\n",
    "criterion_end   = nn.CrossEntropyLoss(ignore_index=-100, weight=end_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_start_classifier.parameters()) \\\n",
    "  + list(model.punct_end_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "# Entrenamiento \n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion_start=criterion_start,criterion_end=criterion_end, criterion_cap = criterion_cap, device=device, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ssssss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = \"hola como estas\"\n",
    "print(f\"{entrada} => {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de control de Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explicacion de del corro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PunctuationCapitalizationRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_dataloader(frases, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice,tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPunctuationCapitalizationRNN\u001b[49m(\n\u001b[1;32m      7\u001b[0m     bert_model\u001b[38;5;241m=\u001b[39mbert_model,\n\u001b[1;32m      8\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      9\u001b[0m     num_punct_start_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_punct_end_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     num_cap_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)  \u001b[38;5;66;03m# Alto LR\u001b[39;00m\n\u001b[1;32m     16\u001b[0m criterion_punct_start \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PunctuationCapitalizationRNN' is not defined"
     ]
    }
   ],
   "source": [
    "frases = [\"¿QUE?\"]\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device,tokenizer=tokenizer)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=64,\n",
    "    num_punct_start_classes=5,\n",
    "    num_punct_end_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "\n",
    "criterion_punct_start = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_punct_end   = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_cap         = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    dataloader_train=train_loader,\n",
    "    dataloader_test=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion_start=criterion_punct_start,\n",
    "    criterion_end=criterion_punct_end,\n",
    "    criterion_cap=criterion_cap,\n",
    "    device=device,\n",
    "    epochs=200\n",
    ")\n",
    "\n",
    "entrada = \"que\"\n",
    "print(\"Predicción:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"modelo_fine_tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_fine_tuned_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'train.PunctuationCapitalizationRNNBidirectional'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cargar la clase del modelo\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPunctuationCapitalizationRNNBidirectional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PunctuationCapitalizationRNNBidirectional\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train.PunctuationCapitalizationRNNBidirectional'"
     ]
    }
   ],
   "source": [
    "# Cargar la clase del modelo\n",
    "from train.PunctuationCapitalizationRNNBidirectional import PunctuationCapitalizationRNNBidirectional\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Crear la instancia\n",
    "model = PunctuationCapitalizationRNNBidirectional(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "# Cargar los pesos en CPU\n",
    "state_dict = torch.load(\"models/modelo_fine_tuned_state_dict_bidirec.pt\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola como estas => Hola, como estas.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"hola como estas\"\n",
    "print(f\"{entrada} => {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion CSV TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datautils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instancia 1: has visto la última película de marvel es increíble\n",
      "Token ID | Token           | Punt Inicial | Punt Final | Capitalizacion\n",
      "----------------------------------------------------------------------\n",
      "       1 | has             | Ø            | ,          | init         \n",
      "       2 | visto           | Ø            | ,          | upper        \n",
      "       3 | la              | Ø            | ,          | upper        \n",
      "       4 | última          | Ø            | ,          | upper        \n",
      "       5 | película        | Ø            | ,          | upper        \n",
      "       6 | de              | Ø            | ,          | upper        \n",
      "       7 | mar             | Ø            | ,          | upper        \n",
      "       8 | ##vel           | Ø            | ,          | upper        \n",
      "       9 | es              | Ø            | ,          | upper        \n",
      "      10 | in              | Ø            | ,          | upper        \n",
      "      11 | ##cre           | Ø            | ,          | upper        \n",
      "      12 | ##í             | Ø            | Ø          | upper        \n",
      "      13 | ##ble           | Ø            | Ø          | upper        \n",
      "\n",
      "Instancia 2: buenas tardes quiero un apple por favor\n",
      "Token ID | Token           | Punt Inicial | Punt Final | Capitalizacion\n",
      "----------------------------------------------------------------------\n",
      "       1 | buenas          | Ø            | ,          | init         \n",
      "       2 | tarde           | Ø            | Ø          | init         \n",
      "       3 | ##s             | Ø            | Ø          | init         \n",
      "       4 | qui             | Ø            | Ø          | upper        \n",
      "       5 | ##ero           | Ø            | ,          | upper        \n",
      "       6 | un              | Ø            | ,          | upper        \n",
      "       7 | app             | Ø            | ,          | init         \n",
      "       8 | ##le            | Ø            | ,          | init         \n",
      "       9 | por             | Ø            | ,          | init         \n",
      "      10 | favor           | Ø            | ,          | upper        \n",
      "holaaaaaaaakasdlfkjasdlkfasdjfs\n",
      "\n",
      "Predicciones guardadas en: predict/prueba_predicciones.csv\n",
      "    instancia_id  token_id     token punt_inicial punt_final capitalización\n",
      "0              1     10393       has                       ,           init\n",
      "1              1     22076     visto                       ,          upper\n",
      "2              1     10109        la                       ,          upper\n",
      "3              1     16077    última                       ,          upper\n",
      "4              1     14970  película                       ,          upper\n",
      "5              1     10104        de                       ,          upper\n",
      "6              1     12318       mar                       ,          upper\n",
      "7              1     13128     ##vel                       ,          upper\n",
      "8              1     10196        es                       ,          upper\n",
      "9              1     10106        in                       ,          upper\n",
      "10             1     27794     ##cre                       ,          upper\n",
      "11             1     10545       ##í                       Ø          upper\n",
      "12             1     11203     ##ble                       Ø          upper\n",
      "13             2    106483    buenas                       ,           init\n",
      "14             2     14002     tarde                       Ø           init\n",
      "15             2     10107       ##s                       Ø           init\n",
      "16             2     10355       qui                       Ø          upper\n",
      "17             2     13739     ##ero                       ,          upper\n",
      "18             2     10119        un                       ,          upper\n",
      "19             2     72894       app                       ,           init\n",
      "20             2     10284      ##le                       ,           init\n",
      "21             2     10183       por                       ,           init\n",
      "22             2     19122     favor                       ,          upper\n"
     ]
    }
   ],
   "source": [
    "# Ruta a un archivo TXT con párrafos (una instancia por párrafo)\n",
    "ruta_txt = \"predict/prueba.txt\"\n",
    "\n",
    "# Ejecutar predicciones y guardar CSV\n",
    "df_predicciones = predicciones_TP(ruta_txt, model, tokenizer, device, max_length=128, verbose=True)\n",
    "\n",
    "# Mostrar las primeras filas del dataframe con las predicciones\n",
    "print(df_predicciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
