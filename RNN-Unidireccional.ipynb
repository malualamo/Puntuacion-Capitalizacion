{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabro/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.datautils import *\n",
    "from utils.MLutils import *\n",
    "from utils.resources import *\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel\n",
    "from data.variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando: cuda\n"
     ]
    }
   ],
   "source": [
    "linux = True\n",
    "device = None\n",
    "\n",
    "if linux:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"usando:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fuente 1: Conjunto de preguntas en espa;ol\n",
    "- Fuente 2: Dataset provisto para Notebook 10\n",
    "- Fuente 3: Dataset sintetico generado con Gemini\n",
    "- Fuente 4: Articulos de Wikipedia\n",
    "- Fuente 5: Subtitulos de peliculas\n",
    "- Fuente 6: Mixture of preguntas y afirmaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n",
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n",
      "Hay 1413 oraciones sintéticas.\n",
      "Se cargaron 6648 frases de Wikipedia.\n",
      "✅ Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "Frases extraídas en total: 947\n",
      "✅ Se extrajeron 1000 frases de Relatos Salvajes.\n"
     ]
    }
   ],
   "source": [
    "questions, question_for_mixture = get_questions()\n",
    "oraciones_rnn = get_notebook_dataset()\n",
    "oraciones_sinteticas = get_gemini_dataset()\n",
    "frases_wikipedia = get_wikipedia_dataset()\n",
    "esperando_la_carroza, frases_relatos_salvajes = get_pelis_dataset()\n",
    "mixtures = get_mixture_dataset(oraciones_sinteticas, question_for_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos las fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 20244\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintéticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Cantidad de oraciones de mixture: 4239\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['La puerta esta cerrada con llave, lo piloto no contacta. Yo estoy desesperada, no sé que hacer.',\n",
       " '¿Sabes si la panadería de la esquina vende pan integral? ¿Cuándo se legalizó el matrimonio homosexual en España?',\n",
       " 'Vos quiere defender a mi hijo, fija tus honorarios con él. Si te lo puede pagar, yo no tengo nada que ver.',\n",
       " 'História da Civilização Brasileira (en portugués).',\n",
       " '¿De quién ha sido la iniciativa? Mi hermano Andrés trabaja en una empresa de tecnología en California.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes + mixtures\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintéticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "print('Cantidad de oraciones de mixture:',len(mixtures))\n",
    "\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos en conjuntos de `train` y `test` con el tokenizer de `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19231\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.RNN import PunctuationCapitalizationRNN\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_start_classes=len(PUNCT_START_TAGS),\n",
    "    num_punct_end_classes=len(PUNCT_END_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "start_w, end_w, cap_w = compute_class_weights(\n",
    "    dataloader_train,\n",
    "    num_classes_list=[\n",
    "        len(PUNCT_START_TAGS),\n",
    "        len(PUNCT_END_TAGS),\n",
    "        len(CAP_TAGS)\n",
    "    ],\n",
    "    device=device,\n",
    "    beta=0.7\n",
    ")\n",
    "\n",
    "criterion_start = nn.CrossEntropyLoss(ignore_index=-100, weight=start_w)\n",
    "criterion_end   = nn.CrossEntropyLoss(ignore_index=-100, weight=end_w)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_w)\n",
    "\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_start_classifier.parameters()) \\\n",
    "  + list(model.punct_end_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "      model, \n",
    "      dataloader_train=dataloader_train, \n",
    "      optimizer=optimizer, \n",
    "      criterion_start=criterion_start,\n",
    "      criterion_end=criterion_end, \n",
    "      criterion_cap = criterion_cap, \n",
    "      device=device, \n",
    "      epochs=20\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_start_classes=len(PUNCT_START_TAGS),\n",
    "    num_punct_end_classes=len(PUNCT_END_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_rnn_tres_cabezas.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: no te lo puedo creer messi se va a retirar\n",
      "Salida: No, te lo puedo, creer, Messi, se va a retirar.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"no te lo puedo creer messi se va a retirar\"\n",
    "\n",
    "print(f\"Entrada: {entrada}\")\n",
    "print(f\"Salida: {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Acc: 0.9980\n",
      "End   Acc: 0.9708\n",
      "Cap   Acc: 0.9838\n",
      "\n",
      "Start report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           Ø       1.00      1.00      1.00     21745\n",
      "           ¿       0.95      0.98      0.96       579\n",
      "\n",
      "    accuracy                           1.00     22324\n",
      "   macro avg       0.97      0.99      0.98     22324\n",
      "weighted avg       1.00      1.00      1.00     22324\n",
      "\n",
      "\n",
      "End report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           Ø       1.00      0.97      0.99     20602\n",
      "           ,       0.49      0.90      0.64       500\n",
      "           .       0.86      0.94      0.90       650\n",
      "           ?       0.92      0.97      0.94       572\n",
      "\n",
      "    accuracy                           0.97     22324\n",
      "   macro avg       0.82      0.95      0.87     22324\n",
      "weighted avg       0.98      0.97      0.97     22324\n",
      "\n",
      "\n",
      "Cap report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       lower       0.99      0.99      0.99     15203\n",
      "        init       0.96      0.98      0.97      4762\n",
      "         mix       0.98      0.82      0.90        73\n",
      "       upper       0.85      0.97      0.90       260\n",
      "\n",
      "    accuracy                           0.98     20298\n",
      "   macro avg       0.95      0.94      0.94     20298\n",
      "weighted avg       0.98      0.98      0.98     20298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de control de Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\"¿QUE?\"]\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device,tokenizer=tokenizer)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=64,\n",
    "    num_punct_start_classes=5,\n",
    "    num_punct_end_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "\n",
    "criterion_punct_start = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_punct_end   = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_cap         = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    dataloader_train=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion_start=criterion_punct_start,\n",
    "    criterion_end=criterion_punct_end,\n",
    "    criterion_cap=criterion_cap,\n",
    "    device=device,\n",
    "    epochs=200\n",
    ")\n",
    "\n",
    "entrada = \"que\"\n",
    "print(\"Predicción:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_rnn_tres_cabezas.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion CSV TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datautils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instancia 1: hola juan\n",
      "Token ID | Token           | Punt Inicial | Punt Final | Capitalizacion\n",
      "----------------------------------------------------------------------\n",
      "       1 | hol             | Ø            | Ø          | init         \n",
      "       2 | ##a             | Ø            | Ø          | init         \n",
      "       3 | ju              | Ø            | Ø          | init         \n",
      "       4 | ##an            | Ø            | .          | init         \n",
      "\n",
      "Predicciones guardadas en: predict/prueba_predicciones.csv\n",
      "   instancia_id  token_id token punt_inicial punt_final capitalización\n",
      "0             1    110516   hol            Ø          Ø           init\n",
      "1             1     10113   ##a            Ø          Ø           init\n",
      "2             1     23005    ju            Ø          Ø           init\n",
      "3             1     10206  ##an            Ø          .           init\n"
     ]
    }
   ],
   "source": [
    "# Ruta a un archivo TXT con párrafos (una instancia por párrafo)\n",
    "ruta_txt = \"predict/prueba.txt\"\n",
    "\n",
    "# Ejecutar predicciones y guardar CSV\n",
    "df_predicciones = predicciones_TP(ruta_txt, model, tokenizer, device, max_length=128, verbose=True)\n",
    "\n",
    "# Mostrar las primeras filas del dataframe con las predicciones\n",
    "print(df_predicciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
