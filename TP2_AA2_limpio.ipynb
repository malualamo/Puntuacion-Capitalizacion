{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabro/Documents/uba/aprendizaje_automatico/aa/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "# pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Descomentar en Windows\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Descomentar en Mac\n",
    "#if torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del DataLoader (importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast  # FAST tokenizer recomendado\n",
    "\n",
    "# Cargo tokenizar\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "PUNCT_TAGS = {\"Ø\": 0, \",\": 1, \".\": 2, \"?\": 3, \"¿\": 4}\n",
    "CAP_TAGS = {\"lower\": 0, \"init\": 1, \"mix\": 2, \"upper\": 3}\n",
    "\n",
    "def _get_capitalization_type(word):\n",
    "    if not word or word.islower(): return 0\n",
    "    if word.istitle(): return 1\n",
    "    if word.isupper(): return 3\n",
    "    if any(c.isupper() for c in word[1:]): return 2\n",
    "    return 0\n",
    "\n",
    "def get_cap_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Recibe los labels por palabra y devuelve los labels por token para capitalizacion\n",
    "    Si los subtokens pertenecen a la misma palabra, les pone el mismo label (capitalizacion) \n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for word_idx in token_word_map:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(labels_per_word[word_idx])\n",
    "    return labels\n",
    "\n",
    "def get_punct_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Asigna etiquetas de puntuación a los subtokens, siguiendo las reglas:\n",
    "    - ¿ va en el primer subtoken de la palabra.\n",
    "    - ., ?, , van en el último subtoken de la palabra.\n",
    "    - Ø no se asigna a ningún subtoken (todos -100).\n",
    "    \"\"\"\n",
    "    labels = [0] * len(token_word_map)\n",
    "    word_to_token_idxs = {}\n",
    "\n",
    "    # Construimos un diccionario: word_idx -> [lista de posiciones de tokens]\n",
    "    for token_idx, word_idx in enumerate(token_word_map):\n",
    "        if word_idx is not None:\n",
    "            word_to_token_idxs.setdefault(word_idx, []).append(token_idx)\n",
    "\n",
    "    for word_idx, token_idxs in word_to_token_idxs.items():\n",
    "        punct_label = labels_per_word[word_idx]\n",
    "        if punct_label == PUNCT_TAGS[\"¿\"]:\n",
    "            target_idx = token_idxs[0]  # primer subtoken\n",
    "        elif punct_label in {PUNCT_TAGS[\".\"], PUNCT_TAGS[\",\"], PUNCT_TAGS[\"?\"]}:\n",
    "            target_idx = token_idxs[-1]  # último subtoken\n",
    "        else:\n",
    "            continue  # Ø: no se asigna nada\n",
    "\n",
    "        labels[target_idx] = punct_label\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataloader(oraciones_raw, max_length, batch_size, device):\n",
    "    \"\"\"\n",
    "    Crea un DataLoader para entrenar un modelo de restauración de puntuación y capitalización.\n",
    "\n",
    "    A partir de una lista de oraciones correctamente escritas (con puntuación y mayúsculas),\n",
    "    esta función:\n",
    "        - Extrae etiquetas de puntuación y capitalización por palabra.\n",
    "        - \"Corrompe\" el texto al eliminar la puntuación y poner las palabras en minúscula.\n",
    "        - Tokeniza las palabras corruptas usando un tokenizer BERT.\n",
    "        - Alinea las etiquetas con los subtokens del tokenizer.\n",
    "        - Crea tensores para las entradas (input_ids, attention_mask) y etiquetas (puntuación y capitalización).\n",
    "        - Devuelve un DataLoader para entrenamiento en lotes.\n",
    "\n",
    "    Parámetros:\n",
    "        oraciones_raw (List[str]): Lista de oraciones correctamente formateadas.\n",
    "        max_length (int): Longitud máxima de secuencia para truncar/padear.\n",
    "        batch_size (int): Tamaño del batch.\n",
    "        device (str): Dispositivo donde se cargarán los tensores ('cpu' o 'cuda').\n",
    "\n",
    "    Retorna:\n",
    "        DataLoader: DataLoader que entrega batches de (input_ids, attention_mask, punct_labels, cap_labels).\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks = []\n",
    "    punct_labels_list = []\n",
    "    cap_labels_list = []\n",
    "\n",
    "    for sent in oraciones_raw:\n",
    "        # Extraer palabras con puntuación\n",
    "        matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sent)) # Detecta puntuaciones y las splitea\n",
    "        words = []\n",
    "        punct_labels = []\n",
    "        cap_labels = []\n",
    "\n",
    "        for i, m in enumerate(matches): # Recorre cada palabra detectada\n",
    "            word_raw = m.group(0) \n",
    "            clean_word = re.sub(r\"[.,?¿]\", \"\", word_raw) # Limpia la palabra \"Hola!\" -> \"Hola\"\n",
    "\n",
    "            # Puntuación\n",
    "            before = sent[m.start() - 1] if m.start() > 0 else \"\" # Signo anterior\n",
    "            after = sent[m.end()] if m.end() < len(sent) else \"\"  # Signo posterior\n",
    "            if before == '¿':\n",
    "                punct = PUNCT_TAGS[\"¿\"]\n",
    "            elif after in PUNCT_TAGS:\n",
    "                punct = PUNCT_TAGS[after]\n",
    "            else:\n",
    "                punct = PUNCT_TAGS[\"Ø\"]\n",
    "\n",
    "            # Capitalización\n",
    "            cap = _get_capitalization_type(word_raw)\n",
    "\n",
    "            clean_word = clean_word.lower() # Limpia la palabra Hola -> hola\n",
    "\n",
    "            words.append(clean_word)\n",
    "            punct_labels.append(punct)\n",
    "            cap_labels.append(cap)\n",
    "\n",
    "        # Tokenización con BERT\n",
    "        encoding = tokenizer(words,\n",
    "                             is_split_into_words=True,\n",
    "                             return_tensors='pt',\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             max_length=max_length,\n",
    "                             return_attention_mask=True)\n",
    "\n",
    "        # Extraer datos que nos sirven del encoding\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "        attention_mask = encoding['attention_mask'][0]\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Mapea cada subtoken a su palabra\n",
    "\n",
    "        # Alinear etiquetas a subtokens (hasta ahora las teniamos en palabras)\n",
    "        punct_labels_aligned = get_punct_labels_for_tokens(punct_labels, word_ids)\n",
    "        cap_labels_aligned = get_cap_labels_for_tokens(cap_labels, word_ids)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        punct_tensor = torch.tensor(punct_labels_aligned)\n",
    "        cap_tensor = torch.tensor(cap_labels_aligned)\n",
    "\n",
    "        # Aplicar -100 a posiciones de padding\n",
    "        punct_tensor[attention_mask == 0] = -100\n",
    "        cap_tensor[attention_mask == 0] = -100\n",
    "\n",
    "        # Agregar a listas (por oracion)\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "        punct_labels_list.append(punct_tensor)\n",
    "        cap_labels_list.append(cap_tensor)\n",
    "\n",
    "    # Stackear tensores (por batch)\n",
    "    input_ids = torch.stack(input_ids_list).to(device)\n",
    "    attention_masks = torch.stack(attention_masks).to(device)\n",
    "    punct_labels = torch.stack(punct_labels_list).to(device)\n",
    "    cap_labels = torch.stack(cap_labels_list).to(device)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, punct_labels, cap_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra detectada : 'que'\n",
      "Antes del match   : '¿'\n",
      "Después del match : '?'\n",
      "---\n",
      "Palabra detectada : 'Que'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'raro'\n",
      "Antes del match   : ' '\n",
      "Después del match : ','\n",
      "---\n",
      "Palabra detectada : 'a'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mi'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'me'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'gusta'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'mas'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'tomar'\n",
      "Antes del match   : ' '\n",
      "Después del match : ' '\n",
      "---\n",
      "Palabra detectada : 'CocaCola'\n",
      "Antes del match   : ' '\n",
      "Después del match : ''\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Linea para entender como el get loader determina las puntuaciones\n",
    "\n",
    "import re\n",
    "\n",
    "# Ejemplo de oración\n",
    "sentence = \"¿que? Que raro, a mi me gusta mas tomar CocaCola\"\n",
    "\n",
    "# Encuentra palabras con posible puntuación al final\n",
    "matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sentence))\n",
    "\n",
    "# Mostramos los resultados\n",
    "for match in matches:\n",
    "    word_raw = match.group(0)\n",
    "    start = match.start()\n",
    "    end = match.end()\n",
    "\n",
    "    # Caracter anterior y posterior\n",
    "    before = sentence[start - 1] if start > 0 else \"\"\n",
    "    after = sentence[end] if end < len(sentence) else \"\"\n",
    "\n",
    "    print(f\"Palabra detectada : '{word_raw}'\")\n",
    "    print(f\"Antes del match   : '{before}'\")\n",
    "    print(f\"Después del match : '{after}'\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n"
     ]
    }
   ],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "DATA_URLS = {\n",
    "    \"train\": \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/train.json\",\n",
    "    \"dev\":   \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/dev.json\",\n",
    "    \"test\":  \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/test.json\",\n",
    "}\n",
    "\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATA_URLS,\n",
    "    field=\"data\",\n",
    ")\n",
    "\n",
    "questions = []\n",
    "\n",
    "\n",
    "for i in range(0, len(raw[\"train\"])):\n",
    "  for p in raw[\"train\"][i]['paragraphs']:\n",
    "    p_questions = [qas['question'] for qas in p['qas']]\n",
    "    questions += p_questions\n",
    "\n",
    "N_QUESTIONS = 5000\n",
    "questions = questions[:N_QUESTIONS]\n",
    "print(f\"Se descargaron {len(questions)} preguntas en Español.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n"
     ]
    }
   ],
   "source": [
    "dataset_rnn = load_dataset(\"google/wmt24pp\", \"en-es_MX\", split=\"train\")\n",
    "oraciones_rnn = dataset_rnn['target'][1:]\n",
    "\n",
    "print(f\"Se descargaron {len(oraciones_rnn)} oraciones en Español (del dataset del notebook 10).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1413 oraciones sintéticas.\n"
     ]
    }
   ],
   "source": [
    "oraciones_sinteticas = []\n",
    "import json\n",
    "with open('./datasets.json', 'r') as file:\n",
    "  data = json.load(file)\n",
    "\n",
    "oraciones_sinteticas = data['otros'] + data['marcas']\n",
    "print(f\"Hay {len(oraciones_sinteticas)} oraciones sintéticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- La corriente de pensamiento vigente en Francia era la Ilustración, cuyos principios se basaban en la razón, la igualdad y la libertad.\n",
      "- La Ilustración había servido de impulso a las Trece Colonias norteamericanas para la independencia de su metrópolis europea.\n",
      "- Tanto la influencia de la Ilustración como el ejemplo de los Estados Unidos sirvieron de «trampolín» ideológico para el inicio de la revolución en Francia.\n",
      "- El otro gran lastre para la economía fue la deuda estatal.\n",
      "- En 1788, la relación entre la deuda y la renta nacional bruta en Francia era del 55,6 %, en comparación con el 181,8 % en Gran Bretaña.\n",
      "Obteniendo frases de Wikipedia para: Argentina\n",
      "Ejemplos de frases obtenidas:\n",
      "- Argentina, oficialmente República Argentina,[a]​ es un país soberano de América del Sur, ubicado en el extremo sur y sudeste de ese subcontinente.\n",
      "- Adopta la forma de gobierno republicana, democrática, representativa y federal.\n",
      "Obteniendo frases de Wikipedia para: España\n",
      "Ejemplos de frases obtenidas:\n",
      "- España, oficialmente Reino de España,[nota 1]​ es un país soberano transcontinental, constituido en Estado social y democrático de derecho y cuya forma de gobierno es la monarquía parlamentaria.\n",
      "- Es uno de los veintisiete Estados soberanos que forman la Unión Europea.\n",
      "Obteniendo frases de Wikipedia para: México\n",
      "Ejemplos de frases obtenidas:\n",
      "- Tras la dominación española, Nueva España inició la lucha por su independencia política en 1810, que culminó en 1821.\n",
      "- Deriva del vocablo náhuatl Mexihko[32]​ (AFI: [meːʃiʔkoˀ]), que designaba la capital de los mexicas.\n",
      "Obteniendo frases de Wikipedia para: Colombia\n",
      "Ejemplos de frases obtenidas:\n",
      "- Colombia, oficialmente República de Colombia, es un país soberano situado en la región noroccidental de América del Sur.\n",
      "- Su capital y ciudad más poblada es Bogotá.[13]​ Es una república organizada políticamente en treinta y dos departamentos descentralizados y el Distrito Capital de Bogotá,[14]​ sede del Gobierno nacional.\n",
      "Obteniendo frases de Wikipedia para: Chile\n",
      "Ejemplos de frases obtenidas:\n",
      "- Chile, oficialmente República de Chile,[20]​ es un país soberano ubicado en el extremo sur de América del Sur.\n",
      "- Adopta la forma de gobierno republicana, democrática, unitaria y presidencialista.[21]​ Su capital y ciudad más poblada es Santiago.\n",
      "Obteniendo frases de Wikipedia para: Perú\n",
      "Ejemplos de frases obtenidas:\n",
      "- Perú (en quechua y en aimara: Piruw), oficialmente República del Perú,  [1] es un país soberano, ubicado en el oeste de América del Sur.\n",
      "- C.[15]​ El Imperio incaico fue el último Estado autóctono o indígena, el cual dominó gran parte del occidente sudamericano hacia el siglo XV.\n",
      "Obteniendo frases de Wikipedia para: Uruguay\n",
      "Ejemplos de frases obtenidas:\n",
      "- Uruguay, oficialmente República Oriental del Uruguay, es un país soberano de América del Sur, situado en la parte oriental del Cono Sur.\n",
      "- Su capital y ciudad más poblada es Montevideo.\n",
      "Obteniendo frases de Wikipedia para: Brasil\n",
      "Ejemplos de frases obtenidas:\n",
      "- Su capital es Brasilia y su ciudad más poblada es São Paulo.\n",
      "- Es el tercer país más grande de América.\n",
      "Obteniendo frases de Wikipedia para: América Latina\n",
      "Ejemplos de frases obtenidas:\n",
      "- En este libro suscita la idea de que al igual que en Europa existía una diferencia cultural entre anglosajones al norte y latinos al sur.\n",
      "- Bajo esta idea Francia se siente como la dueña y protectora de toda la latinidad.\n",
      "Obteniendo frases de Wikipedia para: Europa\n",
      "Ejemplos de frases obtenidas:\n",
      "- Europa es un continente ubicado enteramente en el hemisferio norte y mayoritariamente en el hemisferio oriental.\n",
      "- Los límites de Europa están situados en la mitad occidental del hemisferio norte, limitada por el océano Ártico en el norte, hasta el mar Mediterráneo por el sur.\n",
      "Obteniendo frases de Wikipedia para: Lionel Messi\n",
      "Ejemplos de frases obtenidas:\n",
      "- Lionel Andrés Messi Cuccittini (Rosario, 24 de junio de 1987), conocido como Leo Messi, es un futbolista argentino que juega como delantero o centrocampista.\n",
      "- Desde 2023, integra el plantel del Inter Miami de la MLS canadoestadounidense.\n",
      "Obteniendo frases de Wikipedia para: Diego Maradona\n",
      "Ejemplos de frases obtenidas:\n",
      "- En 1981, fue traspasado a Boca Juniors,[24]​ donde obtuvo el Campeonato Metropolitano, su único título en Argentina.\n",
      "- El 28 de septiembre de 1971, con solo diez años, apareció por primera vez en el diario Clarín.\n",
      "Obteniendo frases de Wikipedia para: Lali Esposito\n",
      "Ejemplos de frases obtenidas:\n",
      "- Siguió su carrera en televisión con papeles secundarios en Cuando me sonreís (2011) y Solamente vos (2013-2014) y obtuvo su primer protagónico en Esperanza mía (2015-2016).\n",
      "- Desde 2013, es también cantante solista.\n",
      "Obteniendo frases de Wikipedia para: Charly Garcia\n",
      "Ejemplos de frases obtenidas:\n",
      "- Tras la despedida pasó a formar parte de dos supergrupos: el efímero PorSuiGieco (de formato mayoritariamente acústico) y La Máquina de Hacer Pájaros.\n",
      "- Con este último publicó dos álbumes que intentaron instaurar el rock progresivo en la escena musical latinoamericana, tendencia que ya había abrazado en el último disco de Sui Generis.\n",
      "Obteniendo frases de Wikipedia para: Dillom\n",
      "Ejemplos de frases obtenidas:\n",
      "Obteniendo frases de Wikipedia para: Tiempos Violentos\n",
      "Ejemplos de frases obtenidas:\n",
      "- Pulp Fiction (conocida como Tiempos violentos en Hispanoamérica) es una película estadounidense de 1994[4]​ escrita y dirigida por Quentin Tarantino.\n",
      "- A partir de una narrativa no lineal, la película entrelaza varias historias cuyos protagonistas son miembros del crimen organizado de Los Ángeles.\n",
      "Obteniendo frases de Wikipedia para: Relatos Salvajes\n",
      "Ejemplos de frases obtenidas:\n",
      "- La película expone seis historias independientes protagonizadas por un reparto coral que incluye a Ricardo Darín, Óscar Martínez, Leonardo Sbaraglia, Érica Rivas, Rita Cortese, Julieta Zylberberg y Darío Grandinetti.\n",
      "- En febrero de 2016, fue premiada con el BAFTA a la mejor película de habla no inglesa en Londres.\n",
      "Obteniendo frases de Wikipedia para: Universidad de Buenos Aires\n",
      "Ejemplos de frases obtenidas:\n",
      "- La Universidad de Buenos Aires (UBA) es una universidad pública argentina con sede en la ciudad de Buenos Aires.\n",
      "- Fue fundada el 12 de agosto de 1821 por el gobernador de la provincia de Buenos Aires, Martín Rodríguez, y su ministro de gobierno, Bernardino Rivadavia.\n",
      "Obteniendo frases de Wikipedia para: Rock nacional\n",
      "Ejemplos de frases obtenidas:\n",
      "- Piancioli comentó: “Esta canción fue un desafío porque no teníamos mucho contacto de raíz con la obra original de Rata Blanca.\n",
      "- Pero puesto en la mesa de trabajo me resultó muy placentero cantarla y me parece que dimos con una versión muy personal y muy Tipitos.\n",
      "Obteniendo frases de Wikipedia para: Cine argentino\n",
      "Ejemplos de frases obtenidas:\n",
      "- El cine de Argentina es uno de los más desarrollados del cine latinoamericano.\n",
      "- Los primeros largometrajes animados, mudos y sonoros fueron realizados por Quirino Cristiani.\n",
      "Obteniendo frases de Wikipedia para: Revolucion de Mayo\n",
      "Ejemplos de frases obtenidas:\n",
      "- La nueva Junta declaraba actuar «a nombre del Sr.\n",
      "- La declaración de independencia de la Argentina tuvo lugar seis años después durante el Congreso de Tucumán el 9 de julio de 1816.\n",
      "Obteniendo frases de Wikipedia para: Independencia de Argentina\n",
      "Ejemplos de frases obtenidas:\n",
      "- El virrey Rafael de Sobremonte se retiró hacia el interior del país a organizar tropas para la reconquista, pero en Buenos Aires el gesto fue interpretado como una huida.\n",
      "- Allí se consultó a los asistentes si Cisneros debía continuar en el mando, y –en caso de respuesta negativa– en quién debería este recaer.\n",
      "Obteniendo frases de Wikipedia para: Simón Bolívar\n",
      "Ejemplos de frases obtenidas:\n",
      "- Lideró las campañas que dieron la independencia a varias naciones americanas, además fue fundador de la Gran Colombia.\n",
      "- También fue legislador y redactor de constituciones, ambientalista y jurista.\n",
      "Obteniendo frases de Wikipedia para: Segunda Guerra Mundial\n",
      "Ejemplos de frases obtenidas:\n",
      "- La Segunda Guerra Mundial[nota 2]​ fue un conflicto militar global que se desarrolló entre 1939 y 1945.\n",
      "- Roosevelt (Carta del Atlántico, 14 de agosto de 1941).\n",
      "Obteniendo frases de Wikipedia para: Guerra Fría\n",
      "Ejemplos de frases obtenidas:\n",
      "- Estos bloques estaban liderados por los Estados Unidos y la Unión Soviética, respectivamente.\n",
      "- El inicio de este periodo se remonta a 1945.\n",
      "Obteniendo frases de Wikipedia para: Revolución Francesa\n",
      "Ejemplos de frases obtenidas:\n",
      "- La corriente de pensamiento vigente en Francia era la Ilustración, cuyos principios se basaban en la razón, la igualdad y la libertad.\n",
      "- La Ilustración había servido de impulso a las Trece Colonias norteamericanas para la independencia de su metrópolis europea.\n",
      "Obteniendo frases de Wikipedia para: Guerra Civil Española\n",
      "Ejemplos de frases obtenidas:\n",
      "- Era apoyado por el movimiento obrero, la FAI y los sindicatos UGT y CNT, los cuales también perseguían realizar la revolución social.\n",
      "- El nuevo gobierno se formó tras la elección de Niceto Alcalá-Zamora como presidente de la República, quien confirmó a Manuel Azaña como presidente del Gobierno.\n",
      "Obteniendo frases de Wikipedia para: Napoleón Bonaparte\n",
      "Ejemplos de frases obtenidas:\n",
      "- El legado político y cultural de Napoleón perdura hasta nuestros días, como líder tan célebre como controvertido.\n",
      "- Inició muchas reformas liberales que han perdurado en la sociedad, y se le considera uno de los más grandes comandantes militares de la historia.\n",
      "Obteniendo frases de Wikipedia para: Nelson Mandela\n",
      "Ejemplos de frases obtenidas:\n",
      "- Fue el primer mandatario de raza negra que encabezó el poder ejecutivo, y el primero en resultar elegido por sufragio universal en su país.\n",
      "- Originario del pueblo xhosa y parte de la casa real tembu, Mandela estudió Derecho en la Universidad de Fort Hare y la Universidad de Witwatersrand.\n",
      "Obteniendo frases de Wikipedia para: Dictadura militar en Argentina\n",
      "Ejemplos de frases obtenidas:\n",
      "- 1]​ (PRN)[1]​\n",
      "fue una dictadura cívico-militar[n.\n",
      "- Los profesionalistas integrados —Carcagno, Anaya y Laplane— sostenían que las Fuerzas Armadas debían integrarse al orden institucional bajo las órdenes del poder político.\n",
      "Obteniendo frases de Wikipedia para: Inteligencia artificial\n",
      "Ejemplos de frases obtenidas:\n",
      "- La inteligencia artificial sintetiza y automatiza tareas que en principio son intelectuales y, por lo tanto, es potencialmente relevante para cualquier ámbito de actividades intelectuales humanas.\n",
      "- [27]​Otros modelos generativos de IA incluyen sistemas de arte de inteligencia artificial como Stable Diffusion, Midjourney y DALL-E, que permiten crear imágenes.\n",
      "Obteniendo frases de Wikipedia para: ChatGPT\n",
      "Ejemplos de frases obtenidas:\n",
      "- ChatGPT (acrónimo del inglés Chat Generative Pre-Trained) es una aplicación de chatbot de inteligencia artificial desarrollada en el año 2022 por OpenAI.\n",
      "- ChatGPT se lanzó el 30 de noviembre de 2022[2]​ y ha llamado la atención por sus respuestas detalladas y articuladas.\n",
      "Obteniendo frases de Wikipedia para: Redes neuronales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabro/Documents/uba/aprendizaje_automatico/aa/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/fabro/Documents/uba/aprendizaje_automatico/aa/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al buscar 'Redes neuronales': \"Red neuronal\" may refer to: \n",
      "red neuronal biológica\n",
      "red neuronal artificial\n",
      "Ejemplos de frases obtenidas:\n",
      "Obteniendo frases de Wikipedia para: Robótica\n",
      "Ejemplos de frases obtenidas:\n",
      "- La robótica es la disciplina que se ocupa del diseño, operación, manufacturación, estudio y aplicación de autómatas o robots.\n",
      "- (Robots Universales Rossum), escrita por Karel Čapek en 1920.\n",
      "Obteniendo frases de Wikipedia para: Energía solar\n",
      "Ejemplos de frases obtenidas:\n",
      "- La energía solar es una energía renovable, obtenida a partir del aprovechamiento de la radiación electromagnética procedente del Sol.\n",
      "- La radiación solar que alcanza la Tierra ha sido aprovechada por el ser humano desde la antigüedad, mediante diferentes tecnologías que han ido evolucionando.\n",
      "Obteniendo frases de Wikipedia para: Vacunas\n",
      "Ejemplos de frases obtenidas:\n",
      "- Existen cuatro tipos de vacunas principales:[10]​\n",
      "\n",
      "Vivas atenuadas: microorganismos que han sido cultivados expresamente bajo condiciones en las cuales pierden o atenúan sus propiedades patógenas.\n",
      "- Suelen provocar una respuesta inmunológica más duradera y son las más usuales en los adultos.\n",
      "Obteniendo frases de Wikipedia para: COVID-19\n",
      "Ejemplos de frases obtenidas:\n",
      "- El choque séptico es la forma más común enestos casos, pero los otros tipos también pueden ocurrir.\n",
      "- Esta simplificación se explicaría por el fenómeno de la economía lingüística, especialmente en el registro informal y discurso hablado.\n",
      "Obteniendo frases de Wikipedia para: Cambio climático\n",
      "Ejemplos de frases obtenidas:\n",
      "- Puede afectar tanto a los valores medios meteorológicos como a su variabilidad y extremos.\n",
      "- Cualquier variación a largo plazo observado a partir de estos indicadores (proxies) puede indicar un cambio climático.\n",
      "Obteniendo frases de Wikipedia para: Computadora cuántica\n",
      "Ejemplos de frases obtenidas:\n",
      "- La computación cuántica o informática cuántica[1]​ es un paradigma de computación distinto al de la informática clásica.\n",
      "- Se basa en el uso de cúbits (qubits en inglés), una especial combinación de dos y tres.\n",
      "Obteniendo frases de Wikipedia para: NASA\n",
      "Ejemplos de frases obtenidas:\n",
      "- Algunos elementos de la Army Ballistic Missile Agency y el Laboratorio de Investigación Naval de los Estados Unidos se incorporaron a la nueva agencia espacial.\n",
      "- Comandos Espaciales\n",
      "\n",
      "\n",
      "== Misiones tripuladas ==\n",
      "Los programas experimentales de aviones cohetes iniciados por el NACA fueron extendidos por la NASA como apoyo para los vuelos espaciales tripulados.\n",
      "Obteniendo frases de Wikipedia para: El Principito\n",
      "Ejemplos de frases obtenidas:\n",
      "- Ahí tenía la misión personal de persuadir al gobierno de dicho país para que le declarara la guerra a la Alemania nazi.\n",
      "- La historia tiene una temática filosófica, donde se incluyen críticas sociales dirigidas a la «extrañeza» con la que los adultos ven las cosas.\n",
      "Obteniendo frases de Wikipedia para: Premio Nobel\n",
      "Ejemplos de frases obtenidas:\n",
      "- En 1894 compró Bofors, una empresa siderúrgica de hierro y acero que convirtió en un importante fabricante de armamento.\n",
      "- Alfred Nobel y su hermano Robert estuvieron involucrados en negocios petrolíferos en Azerbaiyán y, según el historiador sueco E.\n",
      "Obteniendo frases de Wikipedia para: Frida Kahlo\n",
      "Ejemplos de frases obtenidas:\n",
      "- Su obra gira temáticamente en torno a las vivencias de su vida personal.\n",
      "- Realizó un total de 150 obras, principalmente autorretratos, en los que proyectó sus dificultades por sobrevivir.\n",
      "Obteniendo frases de Wikipedia para: Pablo Picasso\n",
      "Ejemplos de frases obtenidas:\n",
      "- Pablo Ruiz Picasso (Málaga, 25 de octubre de 1881-Mougins, Francia, 8 de abril de 1973) fue un pintor y escultor español, creador, junto con Georges Braque, del cubismo.\n",
      "- Sus trabajos están presentes en museos y colecciones de todo el mundo.\n",
      "Obteniendo frases de Wikipedia para: Leonardo da Vinci\n",
      "Ejemplos de frases obtenidas:\n",
      "- Fue a la vez pintor, anatomista, arquitecto, paleontólogo,[3]​botánico, escritor, escultor, filósofo, ingeniero, inventor, músico, poeta y urbanista.\n",
      "- Sus primeros trabajos de importancia fueron creados en Milán al servicio del duque Ludovico Sforza.\n",
      "Obteniendo frases de Wikipedia para: William Shakespeare\n",
      "Ejemplos de frases obtenidas:\n",
      "- William Shakespeare (Stratford-upon-Avon, Inglaterra, bautizado el 26 de abril jul./6 de mayo greg.\n",
      "- de 1564-Stratford-upon-Avon, Inglaterra, 23 de abril jul./3 de mayo greg.\n",
      "Obteniendo frases de Wikipedia para: Gabriel García Márquez\n",
      "Ejemplos de frases obtenidas:\n",
      "- Gabriel José García Márquez (Aracataca, Magdalena, 6 de marzo de 1927-Ciudad de México, 17 de abril de 2014)[nota 1]​[2]​ fue un escritor, guionista y periodista colombiano.\n",
      "- Reconocido por sus novelas y cuentos, también escribió narrativa de no ficción, discursos, reportajes, críticas cinematográficas y memorias.\n",
      "Obteniendo frases de Wikipedia para: Julio Cortázar\n",
      "Ejemplos de frases obtenidas:\n",
      "- Julio Florencio Cortázar (Ixelles, 26 de agosto de 1914-París, 12 de febrero de 1984) fue un escritor, poeta y profesor argentino.\n",
      "- Junto a Gabriel García Márquez, Mario Vargas Llosa y Carlos Fuentes, fue uno de los exponentes centrales del boom latinoamericano.\n",
      "Obteniendo frases de Wikipedia para: Literatura latinoamericana\n",
      "Ejemplos de frases obtenidas:\n",
      "- La literatura latinoamericana es aquella literatura escrita en español, portugués y francés, por autores de países considerados parte de América Latina.\n",
      "- No incluye las obras anglófonas estadounidenses o canadienses, ni las caribeñas escritas en inglés, neerlandés, criollo u otros idiomas no romances.\n",
      "Obteniendo frases de Wikipedia para: Arte contemporáneo\n",
      "Ejemplos de frases obtenidas:\n",
      "- Desde la teoría postestructuralista se ha hablado de arte 'posmoderno' en oposición al arte 'moderno' en cuestiones como la autoría, la subjetividad del artista o la originalidad.\n",
      "- Esta maniobra se conoce como readymade u objeto-encontrado, y tiene una gran importancia en el desarrollo del arte contemporáneo.\n",
      "Obteniendo frases de Wikipedia para: Marvel\n",
      "Ejemplos de frases obtenidas:\n",
      "- Marvel Worldwide, Inc., conocida como Marvel Comics, es una editorial de historietas estadounidense creada en 1939, inicialmente con el nombre de Timely Publications.\n",
      "- Entre sus personajes emblemáticos del género superheroico se encuentran Spider-Man, Hulk, Wolverine, X-Men, Capitán América, Iron Man,  Thor, Los 4 Fantásticos, Daredevil, Punisher, Los Vengadores, entre otros.\n",
      "Obteniendo frases de Wikipedia para: DC Comics\n",
      "Ejemplos de frases obtenidas:\n",
      "- DC Cómics es una editorial de cómics estadounidense.\n",
      "- Forma parte de DC Entertainment,[1]​ una de las empresas que conforman Warner Bros.\n",
      "Obteniendo frases de Wikipedia para: Netflix\n",
      "Ejemplos de frases obtenidas:\n",
      "- es una empresa de entretenimiento y una plataforma de streaming estadounidense.\n",
      "- Resignado, Hastings decidió crear una cadena de videoclubes sin multas ni compromisos.\n",
      "Obteniendo frases de Wikipedia para: Cine de terror\n",
      "Ejemplos de frases obtenidas:\n",
      "- Estas películas exploran a menudo temas oscuros y pueden ocuparse de temas transgresores.\n",
      "- Las películas de terror han existido por más de un siglo.\n",
      "Obteniendo frases de Wikipedia para: Películas de ciencia ficción\n",
      "Ejemplos de frases obtenidas:\n",
      "- Películas como Star Wars o Close Encounters of the Third Kind allanaron el camino de éxitos en las siguientes décadas como E.T., el extraterrestre (1982).\n",
      "- Sin embargo, existen ejemplos conocidos de películas de ciencia ficción de terror, como Frankenstein y Alien.\n",
      "Obteniendo frases de Wikipedia para: Música electrónica\n",
      "Ejemplos de frases obtenidas:\n",
      "- Algunos ejemplos de dispositivos que producen sonido electro-mecánicamente son el telarmonio,[2]​ el órgano Hammond[3]​ y la guitarra eléctrica.\n",
      "- El primer dispositivo conocido capaz de grabar sonido fue el fonoautógrafo, patentado en 1857 por Édouard-Léon Scott de Martinville.\n",
      "Obteniendo frases de Wikipedia para: Reguetón\n",
      "Ejemplos de frases obtenidas:\n",
      "- Durante esta década el género era llamado underground o melaza y tenía líricas explícitas sobre droga, violencia, amistad, amor o sexo.\n",
      "- Los jóvenes puertorriqueños comenzaron a fusionar hip hop con ritmos de reggae.\n",
      "Obteniendo frases de Wikipedia para: Spotify\n",
      "Ejemplos de frases obtenidas:\n",
      "- Spotify es una empresa sueca de servicios multimedia fundada en el año 2006, cuyo producto es la aplicación homónima empleada para la reproducción de música vía streaming.\n",
      "- Su modelo de negocio es el denominado freemium.\n",
      "Obteniendo frases de Wikipedia para: YouTube\n",
      "Ejemplos de frases obtenidas:\n",
      "- YouTube (abreviado como YT) es una red social y plataforma de video online estadounidense propiedad de Google.\n",
      "- También ofrece YouTube Premium, una opción de suscripción de pago para ver contenidos sin anuncios.\n",
      "Obteniendo frases de Wikipedia para: TikTok\n",
      "Ejemplos de frases obtenidas:\n",
      "- TikTok y Douyin tienen casi la misma interfaz de usuario pero no tienen acceso al contenido del otro.\n",
      "- Cada uno de sus servidores está basado en el mercado donde la aplicación respectiva está disponible.[8]​ Los dos productos son similares, pero las características no son idénticas.\n",
      "Obteniendo frases de Wikipedia para: Fútbol\n",
      "Ejemplos de frases obtenidas:\n",
      "- El terreno de juego es rectangular de césped natural o artificial, con una portería o arco a cada lado del campo.\n",
      "- El objetivo es introducirla dentro de la portería o arco contrario, acción que se denomina marcar un gol.\n",
      "Obteniendo frases de Wikipedia para: Copa Mundial de la FIFA\n",
      "Ejemplos de frases obtenidas:\n",
      "- Solo tres equipos de otras confederaciones han llegado a semifinales: Estados Unidos en 1930, Corea del Sur en 2002 y Marruecos en 2022.\n",
      "- El balón oficial es fabricado por la compañía alemana de equipamiento deportivo Adidas.\n",
      "Obteniendo frases de Wikipedia para: Juegos Olimpicos\n",
      "Ejemplos de frases obtenidas:\n",
      "- organizados en la antigua Grecia con sede en la ciudad de Olimpia, realizados entre los años 776 a.\n",
      "- y el 393 de nuestra era.\n",
      "Obteniendo frases de Wikipedia para: Tenis\n",
      "Ejemplos de frases obtenidas:\n",
      "- El tenis es un deporte que se disputa entre dos jugadores (individuales) o entre dos parejas (dobles).\n",
      "- Más tarde se empezaron a utilizar raquetas.\n",
      "Obteniendo frases de Wikipedia para: NBA\n",
      "Ejemplos de frases obtenidas:\n",
      "- Los 11 equipos que abrieron el telón en la temporada inaugural 1946-47 fueron Boston Celtics, Philadelphia Warriors, New York Knicks, Washington Capitols, Providence Steamrollers, Toronto Huskies, Chicago Stags, St.\n",
      "- Por aquel entonces, la NBA asistió también al cambio de franquicias a ciudades más grandes.\n",
      "Obteniendo frases de Wikipedia para: Boca Juniors\n",
      "Ejemplos de frases obtenidas:\n",
      "- El texto varió entre «CABJ», «C.\n",
      "- Boca Juniors domina el historial, y supera a River Plate por 6 partidos.[165]​\n",
      "Actualizado al 21 de septiembre de 2024.\n",
      "Obteniendo frases de Wikipedia para: River Plate\n",
      "Ejemplos de frases obtenidas:\n",
      "- También es el segundo club más ganador de copas nacionales con 16 títulos.\n",
      "- Ostenta el récord de ser el club más ganador del fútbol argentino desde su profesionalización en 1931 sumando 69 títulos oficiales.\n",
      "Obteniendo frases de Wikipedia para: Messi vs Ronaldo\n",
      "Ejemplos de frases obtenidas:\n",
      "- Sus actuaciones, y en especial su competencia, está también ligada a la de sus respectivos clubes.\n",
      "- Cristiano representaba al Real Madrid C.\n",
      "Obteniendo frases de Wikipedia para: Fórmula 1\n",
      "Ejemplos de frases obtenidas:\n",
      "- La entidad que la dirige es la Federación Internacional del Automóvil (FIA).\n",
      "- La mayoría de los circuitos de carreras donde se celebran los Grandes Premios son autódromos, aunque también se utilizan circuitos callejeros y anteriormente se utilizaron circuitos ruteros.\n",
      "Obteniendo frases de Wikipedia para: Michael Jordan\n",
      "Ejemplos de frases obtenidas:\n",
      "- Michael Jeffrey Jordan (Brooklyn, Nueva York, 17 de febrero de 1963) es un exjugador de baloncesto estadounidense.\n",
      "- Con 1,98 metros de altura, jugaba en la posición de escolta.\n",
      "Obteniendo frases de Wikipedia para: Feminismo\n",
      "Ejemplos de frases obtenidas:\n",
      "- El feminismo centra su análisis en el rol del patriarcado en estructurar las relaciones desiguales de poder entre varones y mujeres.\n",
      "- El feminismo realiza una crítica de la visión androcéntrica de la sociedad, a la que busca transformar para conseguir sus objetivos de una sociedad más justa e igualitaria.\n",
      "Obteniendo frases de Wikipedia para: Día Internacional de la Mujer\n",
      "Ejemplos de frases obtenidas:\n",
      "- Desde entonces se ha extendido a muchos países.\n",
      "- Así, en la comedia Lisístrata, de Aristófanes (siglo V a.\n",
      "Obteniendo frases de Wikipedia para: Diversidad cultural\n",
      "Ejemplos de frases obtenidas:\n",
      "- La diversidad cultural es la existencia de diferentes culturas dentro de una sociedad, país o nación.[1]​[2]​ Implica la inclusión de diferentes perspectivas culturales en una organización o sociedad.\n",
      "- Se puede confundir con el concepto de interculturalidad, que apunta a describir la interacción entre dos o más culturas de un modo horizontal.\n",
      "Obteniendo frases de Wikipedia para: Migración\n",
      "Ejemplos de frases obtenidas:\n",
      "- De acuerdo con lo anterior se pueden considerar dos tipos de migraciones: migraciones humanas y migraciones animales.\n",
      "- Las migraciones de seres humanos se estudian tanto por la demografía como por la geografía de la población.\n",
      "Obteniendo frases de Wikipedia para: Pobreza\n",
      "Ejemplos de frases obtenidas:\n",
      "- La pobreza puede afectar a una persona, a un grupo de personas o a toda una región geográfica.\n",
      "- También se suele considerar como pobreza a las situaciones en que la falta de medios económicos impide acceder a tales recursos.\n",
      "Obteniendo frases de Wikipedia para: Educación pública\n",
      "Ejemplos de frases obtenidas:\n",
      "- El objetivo de la educación pública es la accesibilidad de toda la población a la educación y generar niveles de instrucción deseables para la obtención de una ventaja competitiva.\n",
      "- Estos están encargados de organizar y controlar los servicios educativos de cada país.\n",
      "Obteniendo frases de Wikipedia para: Salud mental\n",
      "Ejemplos de frases obtenidas:\n",
      "- En todo caso, el acento estaría puesto en la salud de la persona antes que en sus deficiencias y enfermedades.\n",
      "- Es importante destacar que las enfermedades mentales no están relacionadas con disminución de la función intelectual de las personas.\n",
      "Obteniendo frases de Wikipedia para: Medio ambiente\n",
      "Ejemplos de frases obtenidas:\n",
      "- Como contraposición al entorno natural está el ambiente construido.\n",
      "- El término medio ambiente se usa a menudo como sinónimo de hábitat, por ejemplo, cuando se dice que el ambiente natural de las jirafas es la sabana.\n",
      "Obteniendo frases de Wikipedia para: Derechos humanos\n",
      "Ejemplos de frases obtenidas:\n",
      "- La doctrina de los derechos humanos ha ejercido una profunda influencia en el derecho internacional, así como en el funcionamiento de las instituciones mundiales y regionales.\n",
      "- A pesar de su aceptación generalizada, el concepto de derechos humanos sigue suscitando debate y escepticismo, sobre todo en lo que respecta a su contenido, naturaleza y justificación.\n",
      "Obteniendo frases de Wikipedia para: Trabajo remoto\n",
      "Ejemplos de frases obtenidas:\n",
      "- Es el trabajo realizado a distancia utilizando las TIC para producir bienes y servicios por cuenta propia o ajena y vender productos y servicios al mundo.\n",
      "- Las TIC necesarias para estas tareas son principalmente ordenador, Internet, celular, teléfono y cámara digital, entre otras.\n",
      "Obteniendo frases de Wikipedia para: Filosofía\n",
      "Ejemplos de frases obtenidas:\n",
      "- La filosofía (del griego φιλοσοφία ‘amor a la sabiduría’, derivado de φιλεῖν, fileîn, ‘amar’, y σοφία, sofía, ‘sabiduría’;[1]​ trans.\n",
      "- Toda definición que se dé a esta palabra variará con la filosofía que se adopte.\n",
      "Obteniendo frases de Wikipedia para: Ética\n",
      "Ejemplos de frases obtenidas:\n",
      "- A lo largo de la historia ha habido diversas maneras de entender la ética y distintas propuestas morales orientadoras de la vida humana.\n",
      "- Preguntas específicas\n",
      "La ética se utiliza en algunos aspectos de la determinación de la política pública, también puede ser utilizada por personas que enfrentan decisiones difíciles.\n",
      "Obteniendo frases de Wikipedia para: Psicología\n",
      "Ejemplos de frases obtenidas:\n",
      "- La psicología emplea métodos empíricos cuantitativos y cualitativos de investigación para analizar el comportamiento.\n",
      "- También se pueden encontrar, especialmente en el ámbito clínico o de consultoría, otro tipo de métodos cualitativos y mixtos.\n",
      "Obteniendo frases de Wikipedia para: Sigmund Freud\n",
      "Ejemplos de frases obtenidas:\n",
      "- Estudió en París, con el neurólogo francés Jean-Martin Charcot, las aplicaciones de la hipnosis en el tratamiento de la histeria.\n",
      "- De vuelta a la ciudad de Viena y en colaboración con Josef Breuer desarrolló el método catártico.\n",
      "Obteniendo frases de Wikipedia para: Carl Jung\n",
      "Ejemplos de frases obtenidas:\n",
      "- Jung fue un pionero de la psicología profunda y uno de los estudiosos de esta disciplina más ampliamente leídos en el siglo xx.\n",
      "- Su abordaje teórico y clínico enfatizó la conexión funcional entre la estructura de la psique y la de sus productos, es decir, sus manifestaciones culturales.\n",
      "Obteniendo frases de Wikipedia para: Existencialismo\n",
      "Ejemplos de frases obtenidas:\n",
      "- No se trata de una escuela filosófica homogénea o unificada ni tampoco una sistematizada y sus seguidores se caracterizan principalmente por sus reacciones contra la filosofía tradicional.\n",
      "- Se consideran tres tipos de «escuelas» filosóficas existencialistas: \n",
      "\n",
      "Existencialismo cristiano: Blaise Pascal y Fiódor Dostoyevski como «precursores» y Søren Kierkegaard, León Chestov y Gabriel Marcel ya como «existencialistas».\n",
      "Obteniendo frases de Wikipedia para: Sociología\n",
      "Ejemplos de frases obtenidas:\n",
      "- Mientras algunos sociólogos realizan investigaciones que pueden aplicarse directamente a la política social y el bienestar, otros se centran en refinar la comprensión de los procesos sociales.\n",
      "- Cooper,[8]​[9]​ Ida Wells-Barnett,[10]​Charlotte Perkins Gilman,[6]​ Beatrice Webb[11]​ y Marianne Weber.[12]​[13]​\n",
      "Algunos de los sociólogos más destacados del siglo XX han sido Talcott Parsons, Erving Goffman, Robert K.\n",
      "Obteniendo frases de Wikipedia para: Economía\n",
      "Ejemplos de frases obtenidas:\n",
      "- La ciencia social encargada de su estudio científico es la ciencia económica y quienes la estudian son los economistas.\n",
      "- Algunas actividades económicas son la agricultura, la ganadería, la industria, el comercio, y las comunicaciones.\n",
      "Obteniendo frases de Wikipedia para: Política\n",
      "Ejemplos de frases obtenidas:\n",
      "- En los estados nacionales modernos, la gente a menudo forma partidos políticos para representar sus ideas.\n",
      "- Los miembros de un partido acuerdan adoptar la misma posición en muchos temas y aceptan apoyar proyectos de ley y sus líderes.\n",
      "Obteniendo frases de Wikipedia para: Democracia\n",
      "Ejemplos de frases obtenidas:\n",
      "- En sentido amplio, democracia es una forma de convivencia social en la que los miembros son libres e iguales y las relaciones sociales se establecen conforme a mecanismos contractuales.\n",
      "- No hay que confundir a la república con la democracia, pues aluden a principios distintos.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "# API Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "def obtener_frases_wikipedia(titulo, max_frases=100):\n",
    "    try:\n",
    "        pagina = wikipedia.page(titulo)\n",
    "        texto = pagina.content\n",
    "        oraciones = re.split(r'(?<=[.!?])\\s+', texto)\n",
    "        frases = [o.strip() for o in oraciones if 5 < len(o.split()) < 30]\n",
    "        return frases[:max_frases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error al buscar '{titulo}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Ejemplo: obtener 50 frases de un artículo\n",
    "frases = obtener_frases_wikipedia(\"Revolución francesa\", max_frases=50)\n",
    "for f in frases[:5]:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "temas = [\n",
    "    # Países y lugares\n",
    "    'Argentina', 'España', 'México', 'Colombia', 'Chile',\n",
    "    'Perú', 'Uruguay', 'Brasil', 'América Latina', 'Europa',\n",
    "\n",
    "    # Cultura argentina\n",
    "    'Lionel Messi', 'Diego Maradona', 'Lali Esposito', 'Charly Garcia', 'Dillom',\n",
    "    'Tiempos Violentos', 'Relatos Salvajes', 'Universidad de Buenos Aires', 'Rock nacional', 'Cine argentino',\n",
    "\n",
    "    # Historia y política\n",
    "    'Revolucion de Mayo', 'Independencia de Argentina', 'Simón Bolívar', 'Segunda Guerra Mundial', 'Guerra Fría',\n",
    "    'Revolución Francesa', 'Guerra Civil Española', 'Napoleón Bonaparte', 'Nelson Mandela', 'Dictadura militar en Argentina',\n",
    "\n",
    "    # Ciencia y tecnología\n",
    "    'Inteligencia artificial', 'ChatGPT', 'Redes neuronales', 'Robótica', 'Energía solar',\n",
    "    'Vacunas', 'COVID-19', 'Cambio climático', 'Computadora cuántica', 'NASA',\n",
    "\n",
    "    # Cultura general\n",
    "    'El Principito', 'Premio Nobel', 'Frida Kahlo', 'Pablo Picasso', 'Leonardo da Vinci',\n",
    "    'William Shakespeare', 'Gabriel García Márquez', 'Julio Cortázar', 'Literatura latinoamericana', 'Arte contemporáneo',\n",
    "\n",
    "    # Entretenimiento y medios\n",
    "    'Marvel', 'DC Comics', 'Netflix', 'Cine de terror', 'Películas de ciencia ficción',\n",
    "    'Música electrónica', 'Reguetón', 'Spotify', 'YouTube', 'TikTok',\n",
    "\n",
    "    # Deportes\n",
    "    'Fútbol', 'Copa Mundial de la FIFA', 'Juegos Olimpicos', 'Tenis', 'NBA',\n",
    "    'Boca Juniors', 'River Plate', 'Messi vs Ronaldo', 'Fórmula 1', 'Michael Jordan',\n",
    "\n",
    "    # Sociedad y actualidad\n",
    "    'Feminismo', 'Día Internacional de la Mujer', 'Diversidad cultural', 'Migración', 'Pobreza',\n",
    "    'Educación pública', 'Salud mental', 'Medio ambiente', 'Derechos humanos', 'Trabajo remoto',\n",
    "\n",
    "    # Filosofía y pensamiento\n",
    "    'Filosofía', 'Ética', 'Psicología', 'Sigmund Freud', 'Carl Jung',\n",
    "    'Existencialismo', 'Sociología', 'Economía', 'Política', 'Democracia'\n",
    "]\n",
    "\n",
    "\n",
    "frases_wikipedia = []\n",
    "for tema in temas:\n",
    "    print(f\"Obteniendo frases de Wikipedia para: {tema}\")\n",
    "    frases = obtener_frases_wikipedia(tema,max_frases=100)\n",
    "    print('Ejemplos de frases obtenidas:')\n",
    "    for f in frases[:2]:\n",
    "        print(f\"- {f}\")\n",
    "    frases_wikipedia.extend(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6648"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frases_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Walmart tiene una sección de electrónica.',\n",
       " 'Tal fue el caso de la ESMA, el más grande que funcionó en el país.',\n",
       " 'La Corte Suprema de Justicia (CSJ) tomó una decisión final.',\n",
       " '¿Cuándo ideó Brad Bird a Los Increíbles?',\n",
       " '¿Quién fue el tirador vencedor? ',\n",
       " '“¿Y? Tengo un rifle de alta potencia en mis manos, ahora mismo, listo para la acción. Sólo necesito poder, ya sabes, verlo, para poder dispararle”.',\n",
       " '==== Plena Edad Media ====\\n\\nEl siglo XI comenzó con el predominio entre los reinos cristianos del reino de Navarra.',\n",
       " '¿Por qué eran las pistolas más ventajosas en los duelos?',\n",
       " 'Es la penalización más severa, pues imposibilita que el corredor pueda puntuar en el Gran Premio.',\n",
       " 'Sería su última canasta con la roja de los Bulls.',\n",
       " 'Trece de ellos fueron considerados «críticos», al borde de la insuficiencia respiratoria u orgánica.',\n",
       " '¿Dónde entrevistaron a Maragall?',\n",
       " 'Entre los mejores golpeadores de derecha ya sea por potencia, precisión, o ambas, se encuentran Pete Sampras, Roger Federer, Ivan Lendl, Juan Martín del Potro y Fernando González.',\n",
       " 'Crevenna y El jinete de la muerte (1980) de Federico Curiel; y que desapareció con la decadencia del western.',\n",
       " 'A ésta le seguirían Pandemonium (1989), La secta (1991) (tercera y cuarta entrega, respectivamente, de la saga iniciada con Demonios de 1985) y Mi novia es un zombie (1994).',\n",
       " '¿Qué cargo llegó a ocupar el progenitor de Gore?',\n",
       " 'Sólo a partir de la utilización de los tubos al vacío, se logró retransmitir el mismo programa pero con horarios diferentes.',\n",
       " 'Mi amigo de Guatemala, llamado David, es un activista de derechos.',\n",
       " '¿Cuándo se realizó la primera prueba del corrimiento al rojo?',\n",
       " 'El COI ha tenido que adaptarse a una variedad de avances económicos, políticos y tecnológicos.',\n",
       " 'El desierto de Sonora se extiende por Estados Unidos y México.',\n",
       " 'En América Latina y el Caribe, menos del 10% de las NDC priorizan el sector educativo como parte de sus estrategias de adaptación al cambio climático.',\n",
       " '¿Adónde ha movido CaixaBank su domicilio fiscal?',\n",
       " 'Muchas de las personas identificadas por seudónimos fueron rastreadas hasta su verdadera identidad por Peter Swales.',\n",
       " '¿Cómo respuesta a qué se dio la movilización estudiantil de 2006?',\n",
       " 'Cómo Betta Edu se convirtió en el chivo expiatorio de un sistema político disfuncional',\n",
       " '¿Cuándo se produjo la guerra de las Malvinas?',\n",
       " 'Presidió el Congreso Popular de 1955.',\n",
       " 'La fórmula de Coca-Cola es un secreto.',\n",
       " 'El principito estaba muy cansado así que se sentaron a descansar, conversando sobre por qué el desierto es tan hermoso.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia\n",
    "\n",
    "print(len(oraciones_raw))\n",
    "\n",
    "import random\n",
    "\n",
    "random.sample(oraciones_raw, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13355\n",
      "703\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class PunctuationCapitalizationRNN(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, num_punct_classes, num_cap_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model                        # ahora el modelo completo\n",
    "        self.projection = nn.Linear(\n",
    "            self.bert.config.hidden_size, hidden_dim\n",
    "        )\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers=2,\n",
    "                           batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.punct_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_punct_classes)\n",
    "        )\n",
    "        self.cap_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_cap_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # BERT retorna last_hidden_state: (B, T, H_bert)\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        projected = self.projection(hidden_states)   # (B, T, hidden_dim)\n",
    "\n",
    "        # mismo packing/padding de antes\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1)\n",
    "            packed = pack_padded_sequence(projected, lengths.cpu(),\n",
    "                                          batch_first=True, enforce_sorted=False)\n",
    "            rnn_out_packed, _ = self.rnn(packed)\n",
    "            rnn_out, _ = pad_packed_sequence(\n",
    "                rnn_out_packed, batch_first=True, total_length=projected.size(1)\n",
    "            )\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(projected)\n",
    "\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        punct_logits = self.punct_classifier(rnn_out)\n",
    "        cap_logits   = self.cap_classifier(rnn_out)\n",
    "        return punct_logits, cap_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de entrenamiento\n",
    "def train(model, dataloader_train, dataloader_test, optimizer, criterion_punct, criterion_cap, device, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_punct = criterion_punct(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "            loss_cap = criterion_cap(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "            loss = loss_punct + loss_cap\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader_train)\n",
    "\n",
    "        \"\"\"\n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, punct_labels, cap_labels in dataloader_test:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                punct_labels = punct_labels.to(device)\n",
    "                cap_labels = cap_labels.to(device)\n",
    "\n",
    "                punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                loss_punct = criterion(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "                loss_cap = criterion(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "                loss = loss_punct + loss_cap\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader_test)\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Funcion de evaluacion\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_punct_correct = 0\n",
    "    total_cap_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids)\n",
    "\n",
    "            # Obtener predicciones (dim: [batch, seq_len])\n",
    "            pred_punct = torch.argmax(punct_logits, dim=-1)\n",
    "            pred_cap = torch.argmax(cap_logits, dim=-1)\n",
    "\n",
    "            # Máscara válida (para ignorar -100)\n",
    "            mask = (punct_labels != -100)\n",
    "\n",
    "            # Cálculo de accuracy\n",
    "            total_punct_correct += (pred_punct[mask] == punct_labels[mask]).sum().item()\n",
    "            total_cap_correct += (pred_cap[mask] == cap_labels[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    punct_acc = total_punct_correct / total_tokens\n",
    "    cap_acc = total_cap_correct / total_tokens\n",
    "\n",
    "    print(f\"📌 Punctuation Accuracy:     {punct_acc:.4f}\")\n",
    "    print(f\"🔡 Capitalization Accuracy: {cap_acc:.4f}\")\n",
    "    return punct_acc, cap_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.0267\n",
      "Epoch 2 | Train Loss: 1.1442\n",
      "Epoch 3 | Train Loss: 0.7156\n",
      "Epoch 4 | Train Loss: 0.4812\n",
      "Epoch 5 | Train Loss: 0.3600\n",
      "Epoch 6 | Train Loss: 0.2851\n",
      "Epoch 7 | Train Loss: 0.2395\n",
      "Epoch 8 | Train Loss: 0.2031\n",
      "Epoch 9 | Train Loss: 0.1693\n",
      "Epoch 10 | Train Loss: 0.1484\n",
      "Epoch 11 | Train Loss: 0.1299\n",
      "Epoch 12 | Train Loss: 0.1159\n",
      "Epoch 13 | Train Loss: 0.1033\n",
      "Epoch 14 | Train Loss: 0.0940\n",
      "Epoch 15 | Train Loss: 0.0861\n",
      "Epoch 16 | Train Loss: 0.0784\n",
      "Epoch 17 | Train Loss: 0.0690\n",
      "Epoch 18 | Train Loss: 0.0637\n",
      "Epoch 19 | Train Loss: 0.0621\n",
      "Epoch 20 | Train Loss: 0.0539\n"
     ]
    }
   ],
   "source": [
    "# Embeddings de BERT\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Congelar la mayoría de los parámetros de BERT salvo los últimos N layers y el pooler\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# le quiero pasar el weight param al criterion para mejorar el desbalanceo de clases en base a un counter de tokens\n",
    "# Contar ocurrencias de cada etiqueta de puntuación y capitalización\n",
    "punct_counter = Counter()\n",
    "cap_counter = Counter()\n",
    "for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "    # Convertir a CPU y a numpy para contar\n",
    "    punct_labels_np = punct_labels.cpu().numpy()\n",
    "    cap_labels_np = cap_labels.cpu().numpy()\n",
    "\n",
    "    # Contar etiquetas válidas (ignorando -100)\n",
    "    valid_punct = punct_labels_np[punct_labels_np != -100]\n",
    "    valid_cap = cap_labels_np[cap_labels_np != -100]\n",
    "\n",
    "    punct_counter.update(valid_punct)\n",
    "    cap_counter.update(valid_cap)\n",
    "\n",
    "# como las clases estan desbalanceadas, calculamos los pesos inversos\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap = sum(cap_counter.values())\n",
    "beta = 0.7\n",
    "\n",
    "total_punct = sum(punct_counter.values())\n",
    "total_cap   = sum(cap_counter.values())\n",
    "\n",
    "punct_weights = {\n",
    "    tag: (total_punct / count)**beta\n",
    "    for tag, count in punct_counter.items()\n",
    "}\n",
    "cap_weights = {\n",
    "    tag: (total_cap / count)**beta\n",
    "    for tag, count in cap_counter.items()\n",
    "}\n",
    "\n",
    "# pasar a tensor (clamp opcional para evitar extremos)\n",
    "punct_weights_tensor = torch.tensor(\n",
    "    [punct_weights.get(i, 1.0) for i in range(len(PUNCT_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "cap_weights_tensor = torch.tensor(\n",
    "    [cap_weights.get(i, 1.0) for i in range(len(CAP_TAGS))],\n",
    "    dtype=torch.float32\n",
    ").to(device).clamp(min=1.0, max=5.0)\n",
    "\n",
    "\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100, weight=punct_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "# Entrenamiento \n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion_punct=criterion_punct, criterion_cap = criterion_cap, device=device, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Punctuation Accuracy:     0.9777\n",
      "🔡 Capitalization Accuracy: 0.8975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9776862797184221, 0.8975295523973967)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_reconstruct(model, sentence, tokenizer, device, max_length=64, verbose=True):\n",
    "    model.eval()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        punct_logits, cap_logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    pred_punct = torch.argmax(punct_logits, dim=-1)[0].cpu().tolist()\n",
    "    pred_cap = torch.argmax(cap_logits, dim=-1)[0].cpu().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    INV_PUNCT_TAGS = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "\n",
    "    final_words = []\n",
    "    current_word = \"\"\n",
    "    current_cap = 0\n",
    "    current_punct = 0\n",
    "    new_word = True\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\n🔍 Predicción token por token:\")\n",
    "        print(f\"{'TOKEN':15s} | {'PUNCT':>5s} | {'SIGNO':>5s} | {'CAP':>3s} | {'FINAL':15s}\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "    for i, (token, punct_label, cap_label) in enumerate(zip(tokens, pred_punct, pred_cap)):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] or attention_mask[0, i].item() == 0:\n",
    "            continue\n",
    "\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += clean_token\n",
    "            if punct_label != 0:\n",
    "                current_punct = punct_label  # usar puntuación del último subtoken relevante\n",
    "        else:\n",
    "            if current_word:\n",
    "                # cerrar palabra anterior\n",
    "                word = current_word\n",
    "                # aplicar capitalización a toda la palabra\n",
    "                if current_cap == 1:\n",
    "                    word = word.capitalize()\n",
    "                elif current_cap == 2:\n",
    "                    word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "                elif current_cap == 3:\n",
    "                    word = word.upper()\n",
    "                # aplicar puntuación del último subtoken\n",
    "                punct = INV_PUNCT_TAGS.get(current_punct, \"Ø\")\n",
    "                if punct == \"¿\":\n",
    "                    word = \"¿\" + word\n",
    "                elif punct != \"Ø\":\n",
    "                    word = word + punct\n",
    "                final_words.append(word)\n",
    "\n",
    "            # empezar nueva palabra\n",
    "            current_word = clean_token\n",
    "            current_cap = cap_label\n",
    "            current_punct = punct_label if punct_label != 0 else 0\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{clean_token:15s} | {punct_label:5d} | {INV_PUNCT_TAGS.get(punct_label, 'Ø'):>5s} | {cap_label:3d} | {clean_token:15s}\")\n",
    "\n",
    "    # Procesar última palabra\n",
    "    if current_word:\n",
    "        word = current_word\n",
    "        if current_cap == 1:\n",
    "            word = word.capitalize()\n",
    "        elif current_cap == 2:\n",
    "            word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "        elif current_cap == 3:\n",
    "            word = word.upper()\n",
    "        punct = INV_PUNCT_TAGS.get(current_punct, \"Ø\")\n",
    "        if punct == \"¿\":\n",
    "            word = \"¿\" + word\n",
    "        elif punct != \"Ø\":\n",
    "            word = word + punct\n",
    "        final_words.append(word)\n",
    "\n",
    "    return \" \".join(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuál es la capital de argentina => ¿Cuál es la capital de Argentina?\n"
     ]
    }
   ],
   "source": [
    "entrada = \"cuál es la capital de argentina\"\n",
    "print(f\"{entrada} => {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.9041\n",
      "Epoch 2 | Train Loss: 2.7525\n",
      "Epoch 3 | Train Loss: 2.8042\n",
      "Epoch 4 | Train Loss: 2.4886\n",
      "Epoch 5 | Train Loss: 2.0364\n",
      "Epoch 6 | Train Loss: 1.7816\n",
      "Epoch 7 | Train Loss: 1.6085\n",
      "Epoch 8 | Train Loss: 1.9414\n",
      "Epoch 9 | Train Loss: 1.6012\n",
      "Epoch 10 | Train Loss: 1.5608\n",
      "Epoch 11 | Train Loss: 1.5680\n",
      "Epoch 12 | Train Loss: 1.6294\n",
      "Epoch 13 | Train Loss: 1.6771\n",
      "Epoch 14 | Train Loss: 1.5966\n",
      "Epoch 15 | Train Loss: 1.5496\n",
      "Epoch 16 | Train Loss: 1.4163\n",
      "Epoch 17 | Train Loss: 1.6198\n",
      "Epoch 18 | Train Loss: 1.6080\n",
      "Epoch 19 | Train Loss: 1.4456\n",
      "Epoch 20 | Train Loss: 1.4271\n",
      "Epoch 21 | Train Loss: 1.4963\n",
      "Epoch 22 | Train Loss: 1.6729\n",
      "Epoch 23 | Train Loss: 1.5201\n",
      "Epoch 24 | Train Loss: 1.5008\n",
      "Epoch 25 | Train Loss: 1.4461\n",
      "Epoch 26 | Train Loss: 1.5343\n",
      "Epoch 27 | Train Loss: 1.4170\n",
      "Epoch 28 | Train Loss: 1.5351\n",
      "Epoch 29 | Train Loss: 1.2816\n",
      "Epoch 30 | Train Loss: 1.3075\n",
      "Epoch 31 | Train Loss: 1.5001\n",
      "Epoch 32 | Train Loss: 1.3044\n",
      "Epoch 33 | Train Loss: 1.3603\n",
      "Epoch 34 | Train Loss: 1.6303\n",
      "Epoch 35 | Train Loss: 1.3930\n",
      "Epoch 36 | Train Loss: 1.4275\n",
      "Epoch 37 | Train Loss: 1.2022\n",
      "Epoch 38 | Train Loss: 1.3402\n",
      "Epoch 39 | Train Loss: 1.2695\n",
      "Epoch 40 | Train Loss: 1.3167\n",
      "Epoch 41 | Train Loss: 1.3456\n",
      "Epoch 42 | Train Loss: 1.0623\n",
      "Epoch 43 | Train Loss: 1.2127\n",
      "Epoch 44 | Train Loss: 1.3160\n",
      "Epoch 45 | Train Loss: 1.4177\n",
      "Epoch 46 | Train Loss: 1.3338\n",
      "Epoch 47 | Train Loss: 1.2174\n",
      "Epoch 48 | Train Loss: 1.2938\n",
      "Epoch 49 | Train Loss: 1.2318\n",
      "Epoch 50 | Train Loss: 1.3380\n",
      "Epoch 51 | Train Loss: 1.1478\n",
      "Epoch 52 | Train Loss: 1.0414\n",
      "Epoch 53 | Train Loss: 1.0351\n",
      "Epoch 54 | Train Loss: 1.0681\n",
      "Epoch 55 | Train Loss: 1.0730\n",
      "Epoch 56 | Train Loss: 1.0859\n",
      "Epoch 57 | Train Loss: 1.1499\n",
      "Epoch 58 | Train Loss: 1.1939\n",
      "Epoch 59 | Train Loss: 1.0210\n",
      "Epoch 60 | Train Loss: 1.0601\n",
      "Epoch 61 | Train Loss: 1.1526\n",
      "Epoch 62 | Train Loss: 0.9718\n",
      "Epoch 63 | Train Loss: 1.2524\n",
      "Epoch 64 | Train Loss: 0.9741\n",
      "Epoch 65 | Train Loss: 0.9509\n",
      "Epoch 66 | Train Loss: 1.2484\n",
      "Epoch 67 | Train Loss: 0.8212\n",
      "Epoch 68 | Train Loss: 0.9991\n",
      "Epoch 69 | Train Loss: 0.8325\n",
      "Epoch 70 | Train Loss: 1.0543\n",
      "Epoch 71 | Train Loss: 0.9744\n",
      "Epoch 72 | Train Loss: 0.8999\n",
      "Epoch 73 | Train Loss: 1.0224\n",
      "Epoch 74 | Train Loss: 0.8276\n",
      "Epoch 75 | Train Loss: 0.7658\n",
      "Epoch 76 | Train Loss: 0.7095\n",
      "Epoch 77 | Train Loss: 0.5454\n",
      "Epoch 78 | Train Loss: 0.6062\n",
      "Epoch 79 | Train Loss: 0.6741\n",
      "Epoch 80 | Train Loss: 0.7357\n",
      "Epoch 81 | Train Loss: 0.8277\n",
      "Epoch 82 | Train Loss: 0.5813\n",
      "Epoch 83 | Train Loss: 0.7103\n",
      "Epoch 84 | Train Loss: 0.7607\n",
      "Epoch 85 | Train Loss: 1.2008\n",
      "Epoch 86 | Train Loss: 1.0331\n",
      "Epoch 87 | Train Loss: 0.8042\n",
      "Epoch 88 | Train Loss: 0.7970\n",
      "Epoch 89 | Train Loss: 0.8939\n",
      "Epoch 90 | Train Loss: 0.7259\n",
      "Epoch 91 | Train Loss: 0.6029\n",
      "Epoch 92 | Train Loss: 0.5131\n",
      "Epoch 93 | Train Loss: 0.6188\n",
      "Epoch 94 | Train Loss: 0.7534\n",
      "Epoch 95 | Train Loss: 0.9649\n",
      "Epoch 96 | Train Loss: 0.6132\n",
      "Epoch 97 | Train Loss: 0.7953\n",
      "Epoch 98 | Train Loss: 0.5279\n",
      "Epoch 99 | Train Loss: 0.5407\n",
      "Epoch 100 | Train Loss: 0.4666\n",
      "Epoch 101 | Train Loss: 0.5466\n",
      "Epoch 102 | Train Loss: 0.5077\n",
      "Epoch 103 | Train Loss: 0.4878\n",
      "Epoch 104 | Train Loss: 0.6902\n",
      "Epoch 105 | Train Loss: 0.5986\n",
      "Epoch 106 | Train Loss: 0.5989\n",
      "Epoch 107 | Train Loss: 0.8205\n",
      "Epoch 108 | Train Loss: 0.6459\n",
      "Epoch 109 | Train Loss: 0.6935\n",
      "Epoch 110 | Train Loss: 0.8354\n",
      "Epoch 111 | Train Loss: 0.3638\n",
      "Epoch 112 | Train Loss: 0.4980\n",
      "Epoch 113 | Train Loss: 0.4571\n",
      "Epoch 114 | Train Loss: 0.6826\n",
      "Epoch 115 | Train Loss: 0.3637\n",
      "Epoch 116 | Train Loss: 0.4391\n",
      "Epoch 117 | Train Loss: 0.4531\n",
      "Epoch 118 | Train Loss: 0.3039\n",
      "Epoch 119 | Train Loss: 0.4022\n",
      "Epoch 120 | Train Loss: 0.5950\n",
      "Epoch 121 | Train Loss: 0.4299\n",
      "Epoch 122 | Train Loss: 0.3995\n",
      "Epoch 123 | Train Loss: 0.4307\n",
      "Epoch 124 | Train Loss: 0.3340\n",
      "Epoch 125 | Train Loss: 0.3994\n",
      "Epoch 126 | Train Loss: 0.3369\n",
      "Epoch 127 | Train Loss: 0.4564\n",
      "Epoch 128 | Train Loss: 0.4133\n",
      "Epoch 129 | Train Loss: 0.2869\n",
      "Epoch 130 | Train Loss: 0.3591\n",
      "Epoch 131 | Train Loss: 0.2112\n",
      "Epoch 132 | Train Loss: 0.5459\n",
      "Epoch 133 | Train Loss: 0.4612\n",
      "Epoch 134 | Train Loss: 0.2449\n",
      "Epoch 135 | Train Loss: 0.1646\n",
      "Epoch 136 | Train Loss: 0.2365\n",
      "Epoch 137 | Train Loss: 0.3438\n",
      "Epoch 138 | Train Loss: 0.2881\n",
      "Epoch 139 | Train Loss: 0.4048\n",
      "Epoch 140 | Train Loss: 0.0992\n",
      "Epoch 141 | Train Loss: 0.1102\n",
      "Epoch 142 | Train Loss: 0.2197\n",
      "Epoch 143 | Train Loss: 0.3388\n",
      "Epoch 144 | Train Loss: 0.1429\n",
      "Epoch 145 | Train Loss: 0.2174\n",
      "Epoch 146 | Train Loss: 0.3072\n",
      "Epoch 147 | Train Loss: 0.1916\n",
      "Epoch 148 | Train Loss: 0.2822\n",
      "Epoch 149 | Train Loss: 0.2708\n",
      "Epoch 150 | Train Loss: 0.1534\n",
      "Epoch 151 | Train Loss: 0.0734\n",
      "Epoch 152 | Train Loss: 0.2549\n",
      "Epoch 153 | Train Loss: 0.0962\n",
      "Epoch 154 | Train Loss: 0.3004\n",
      "Epoch 155 | Train Loss: 0.5569\n",
      "Epoch 156 | Train Loss: 0.8348\n",
      "Epoch 157 | Train Loss: 0.3751\n",
      "Epoch 158 | Train Loss: 0.1608\n",
      "Epoch 159 | Train Loss: 0.1561\n",
      "Epoch 160 | Train Loss: 0.4134\n",
      "Epoch 161 | Train Loss: 0.2224\n",
      "Epoch 162 | Train Loss: 0.6320\n",
      "Epoch 163 | Train Loss: 1.1164\n",
      "Epoch 164 | Train Loss: 0.3918\n",
      "Epoch 165 | Train Loss: 0.1651\n",
      "Epoch 166 | Train Loss: 1.6350\n",
      "Epoch 167 | Train Loss: 0.3820\n",
      "Epoch 168 | Train Loss: 0.1999\n",
      "Epoch 169 | Train Loss: 1.0317\n",
      "Epoch 170 | Train Loss: 0.6835\n",
      "Epoch 171 | Train Loss: 0.1613\n",
      "Epoch 172 | Train Loss: 0.1460\n",
      "Epoch 173 | Train Loss: 0.4859\n",
      "Epoch 174 | Train Loss: 0.3728\n",
      "Epoch 175 | Train Loss: 0.2635\n",
      "Epoch 176 | Train Loss: 0.4728\n",
      "Epoch 177 | Train Loss: 0.2598\n",
      "Epoch 178 | Train Loss: 0.6604\n",
      "Epoch 179 | Train Loss: 0.2222\n",
      "Epoch 180 | Train Loss: 0.3213\n",
      "Epoch 181 | Train Loss: 0.2924\n",
      "Epoch 182 | Train Loss: 0.3553\n",
      "Epoch 183 | Train Loss: 0.2332\n",
      "Epoch 184 | Train Loss: 0.2246\n",
      "Epoch 185 | Train Loss: 0.1460\n",
      "Epoch 186 | Train Loss: 0.2774\n",
      "Epoch 187 | Train Loss: 0.3304\n",
      "Epoch 188 | Train Loss: 0.3077\n",
      "Epoch 189 | Train Loss: 0.2289\n",
      "Epoch 190 | Train Loss: 0.2031\n",
      "Epoch 191 | Train Loss: 0.2062\n",
      "Epoch 192 | Train Loss: 0.1219\n",
      "Epoch 193 | Train Loss: 0.2051\n",
      "Epoch 194 | Train Loss: 0.3155\n",
      "Epoch 195 | Train Loss: 0.2491\n",
      "Epoch 196 | Train Loss: 0.0833\n",
      "Epoch 197 | Train Loss: 0.1502\n",
      "Epoch 198 | Train Loss: 0.1815\n",
      "Epoch 199 | Train Loss: 0.1731\n",
      "Epoch 200 | Train Loss: 0.0969\n",
      "\n",
      "🔍 Predicción token por token:\n",
      "TOKEN           | PUNCT | SIGNO | CAP | FINAL          \n",
      "-------------------------------------------------------\n",
      "buenas          |     0 |     Ø |   1 | buenas         \n",
      "tarde           |     0 |     Ø |   0 | tarde          \n",
      "s               |     1 |     , |   0 | s              \n",
      "qui             |     0 |     Ø |   0 | qui            \n",
      "ero             |     0 |     Ø |   0 | ero            \n",
      "un              |     0 |     Ø |   0 | un             \n",
      "app             |     0 |     Ø |   3 | app            \n",
      "le              |     0 |     Ø |   3 | le             \n",
      "por             |     0 |     Ø |   0 | por            \n",
      "favor           |     2 |     . |   0 | favor          \n",
      "much            |     0 |     Ø |   1 | much           \n",
      "isi             |     0 |     Ø |   1 | isi            \n",
      "mas             |     0 |     Ø |   1 | mas            \n",
      "h               |     0 |     Ø |   3 | h              \n",
      "h               |     0 |     Ø |   3 | h              \n",
      "Predicción: Buenas tardes, quiero un APPLE por favor. Muchisimas HH\n"
     ]
    }
   ],
   "source": [
    "frases = [\"Buenas tardes, quiero un APPLE por favor. Muchisimas HH\"]\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=64,\n",
    "    num_punct_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_cap = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(model, train_loader, train_loader,optimizer, criterion_punct, criterion_cap, device, epochs=200)\n",
    "\n",
    "entrada = \"buenas tardes quiero un apple por favor muchisimas hh\"\n",
    "print(\"Predicción:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"modelo_fine_tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_fine_tuned_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 179,236,873\n",
      "Trainable parameters: 87,424,777\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
