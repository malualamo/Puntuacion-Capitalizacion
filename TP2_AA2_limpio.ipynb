{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "# pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Descomentar en Windows\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# Descomentar en Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del DataLoader (importante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast  # FAST tokenizer recomendado\n",
    "\n",
    "# Cargo tokenizar\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "PUNCT_TAGS = {\"Ø\": 0, \",\": 1, \".\": 2, \"?\": 3, \"¿\": 4}\n",
    "CAP_TAGS = {\"lower\": 0, \"init\": 1, \"mix\": 2, \"upper\": 3}\n",
    "\n",
    "def _get_capitalization_type(word):\n",
    "    if not word or word.islower(): return 0\n",
    "    if word.istitle(): return 1\n",
    "    if word.isupper(): return 3\n",
    "    if any(c.isupper() for c in word[1:]): return 2\n",
    "    return 0\n",
    "\n",
    "def get_cap_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Recibe los labels por palabra y devuelve los labels por token para capitalizacion\n",
    "    Si los subtokens pertenecen a la misma palabra, les pone el mismo label (capitalizacion) \n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for word_idx in token_word_map:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(labels_per_word[word_idx])\n",
    "    return labels\n",
    "\n",
    "def get_punct_labels_for_tokens(labels_per_word, token_word_map):\n",
    "    \"\"\"\n",
    "    Asigna etiquetas de puntuación a los subtokens, siguiendo las reglas:\n",
    "    - ¿ va en el primer subtoken de la palabra.\n",
    "    - ., ?, , van en el último subtoken de la palabra.\n",
    "    - Ø no se asigna a ningún subtoken (todos -100).\n",
    "    \"\"\"\n",
    "    labels = [0] * len(token_word_map)\n",
    "    word_to_token_idxs = {}\n",
    "\n",
    "    # Construimos un diccionario: word_idx -> [lista de posiciones de tokens]\n",
    "    for token_idx, word_idx in enumerate(token_word_map):\n",
    "        if word_idx is not None:\n",
    "            word_to_token_idxs.setdefault(word_idx, []).append(token_idx)\n",
    "\n",
    "    for word_idx, token_idxs in word_to_token_idxs.items():\n",
    "        punct_label = labels_per_word[word_idx]\n",
    "        if punct_label == PUNCT_TAGS[\"¿\"]:\n",
    "            target_idx = token_idxs[0]  # primer subtoken\n",
    "        elif punct_label in {PUNCT_TAGS[\".\"], PUNCT_TAGS[\",\"], PUNCT_TAGS[\"?\"]}:\n",
    "            target_idx = token_idxs[-1]  # último subtoken\n",
    "        else:\n",
    "            continue  # Ø: no se asigna nada\n",
    "\n",
    "        labels[target_idx] = punct_label\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataloader(oraciones_raw, max_length, batch_size, device):\n",
    "    \"\"\"\n",
    "    Crea un DataLoader para entrenar un modelo de restauración de puntuación y capitalización.\n",
    "\n",
    "    A partir de una lista de oraciones correctamente escritas (con puntuación y mayúsculas),\n",
    "    esta función:\n",
    "        - Extrae etiquetas de puntuación y capitalización por palabra.\n",
    "        - \"Corrompe\" el texto al eliminar la puntuación y poner las palabras en minúscula.\n",
    "        - Tokeniza las palabras corruptas usando un tokenizer BERT.\n",
    "        - Alinea las etiquetas con los subtokens del tokenizer.\n",
    "        - Crea tensores para las entradas (input_ids, attention_mask) y etiquetas (puntuación y capitalización).\n",
    "        - Devuelve un DataLoader para entrenamiento en lotes.\n",
    "\n",
    "    Parámetros:\n",
    "        oraciones_raw (List[str]): Lista de oraciones correctamente formateadas.\n",
    "        max_length (int): Longitud máxima de secuencia para truncar/padear.\n",
    "        batch_size (int): Tamaño del batch.\n",
    "        device (str): Dispositivo donde se cargarán los tensores ('cpu' o 'cuda').\n",
    "\n",
    "    Retorna:\n",
    "        DataLoader: DataLoader que entrega batches de (input_ids, attention_mask, punct_labels, cap_labels).\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    attention_masks = []\n",
    "    punct_labels_list = []\n",
    "    cap_labels_list = []\n",
    "\n",
    "    for sent in oraciones_raw:\n",
    "        # Extraer palabras con puntuación\n",
    "        matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sent)) # Detecta puntuaciones y las splitea\n",
    "        words = []\n",
    "        punct_labels = []\n",
    "        cap_labels = []\n",
    "\n",
    "        for i, m in enumerate(matches): # Recorre cada palabra detectada\n",
    "            word_raw = m.group(0) \n",
    "            clean_word = re.sub(r\"[.,?¿]\", \"\", word_raw) # Limpia la palabra \"Hola!\" -> \"Hola\"\n",
    "\n",
    "            # Puntuación\n",
    "            before = sent[m.start() - 1] if m.start() > 0 else \"\" # Signo anterior\n",
    "            after = sent[m.end()] if m.end() < len(sent) else \"\"  # Signo posterior\n",
    "            if before == '¿':\n",
    "                punct = PUNCT_TAGS[\"¿\"]\n",
    "            elif after in PUNCT_TAGS:\n",
    "                punct = PUNCT_TAGS[after]\n",
    "            else:\n",
    "                punct = PUNCT_TAGS[\"Ø\"]\n",
    "\n",
    "            # Capitalización\n",
    "            cap = _get_capitalization_type(word_raw)\n",
    "\n",
    "            clean_word = clean_word.lower() # Limpia la palabra Hola -> hola\n",
    "\n",
    "            words.append(clean_word)\n",
    "            punct_labels.append(punct)\n",
    "            cap_labels.append(cap)\n",
    "\n",
    "        # Tokenización con BERT\n",
    "        encoding = tokenizer(words,\n",
    "                             is_split_into_words=True,\n",
    "                             return_tensors='pt',\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             max_length=max_length,\n",
    "                             return_attention_mask=True)\n",
    "\n",
    "        # Extraer datos que nos sirven del encoding\n",
    "        input_ids = encoding['input_ids'][0]\n",
    "        attention_mask = encoding['attention_mask'][0]\n",
    "        word_ids = encoding.word_ids(batch_index=0)  # Mapea cada subtoken a su palabra\n",
    "\n",
    "        # Alinear etiquetas a subtokens (hasta ahora las teniamos en palabras)\n",
    "        punct_labels_aligned = get_punct_labels_for_tokens(punct_labels, word_ids)\n",
    "        cap_labels_aligned = get_cap_labels_for_tokens(cap_labels, word_ids)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        punct_tensor = torch.tensor(punct_labels_aligned)\n",
    "        cap_tensor = torch.tensor(cap_labels_aligned)\n",
    "\n",
    "        # Aplicar -100 a posiciones de padding\n",
    "        punct_tensor[attention_mask == 0] = -100\n",
    "        cap_tensor[attention_mask == 0] = -100\n",
    "\n",
    "        # Agregar a listas (por oracion)\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_masks.append(attention_mask)\n",
    "        punct_labels_list.append(punct_tensor)\n",
    "        cap_labels_list.append(cap_tensor)\n",
    "\n",
    "    # Stackear tensores (por batch)\n",
    "    input_ids = torch.stack(input_ids_list).to(device)\n",
    "    attention_masks = torch.stack(attention_masks).to(device)\n",
    "    punct_labels = torch.stack(punct_labels_list).to(device)\n",
    "    cap_labels = torch.stack(cap_labels_list).to(device)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, punct_labels, cap_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linea para entender como el get loader determina las puntuaciones\n",
    "\n",
    "import re\n",
    "\n",
    "# Ejemplo de oración\n",
    "sentence = \"¿Te gusta la soda? Que raro, a mi me gusta mas tomar CocaCola\"\n",
    "\n",
    "# Encuentra palabras con posible puntuación al final\n",
    "matches = list(re.finditer(r\"\\b\\w+[^\\s\\w]?\\b\", sentence))\n",
    "\n",
    "# Mostramos los resultados\n",
    "for match in matches:\n",
    "    word_raw = match.group(0)\n",
    "    start = match.start()\n",
    "    end = match.end()\n",
    "\n",
    "    # Caracter anterior y posterior\n",
    "    before = sentence[start - 1] if start > 0 else \"\"\n",
    "    after = sentence[end] if end < len(sentence) else \"\"\n",
    "\n",
    "    print(f\"Palabra detectada : '{word_raw}'\")\n",
    "    print(f\"Antes del match   : '{before}'\")\n",
    "    print(f\"Después del match : '{after}'\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armado del dataset\n",
    "\n",
    "DATA_URLS = {\n",
    "    \"train\": \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/train.json\",\n",
    "    \"dev\":   \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/dev.json\",\n",
    "    \"test\":  \"https://huggingface.co/datasets/PlanTL-GOB-ES/SQAC/resolve/main/test.json\",\n",
    "}\n",
    "\n",
    "raw = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=DATA_URLS,\n",
    "    field=\"data\",\n",
    ")\n",
    "\n",
    "questions = []\n",
    "\n",
    "\n",
    "for i in range(0, len(raw[\"train\"])):\n",
    "  for p in raw[\"train\"][i]['paragraphs']:\n",
    "    p_questions = [qas['question'] for qas in p['qas']]\n",
    "    questions += p_questions\n",
    "\n",
    "N_QUESTIONS = 5000\n",
    "questions = questions[:N_QUESTIONS]\n",
    "print(f\"Se descargaron {len(questions)} preguntas en Español.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rnn = load_dataset(\"google/wmt24pp\", \"en-es_MX\", split=\"train\")\n",
    "oraciones_rnn = dataset_rnn['target'][1:]\n",
    "\n",
    "print(f\"Se descargaron {len(oraciones_rnn)} oraciones en Español (del dataset del notebook 10).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oraciones_sinteticas = []\n",
    "import json\n",
    "with open('./datasets.json', 'r') as file:\n",
    "  data = json.load(file)\n",
    "\n",
    "oraciones_sinteticas = data['otros'] + data['marcas']\n",
    "print(f\"Hay {len(oraciones_sinteticas)} oraciones sintéticas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# API Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "def obtener_frases_wikipedia(titulo, max_frases=100):\n",
    "    try:\n",
    "        pagina = wikipedia.page(titulo)\n",
    "        texto = pagina.content\n",
    "        oraciones = re.split(r'(?<=[.!?])\\s+', texto)\n",
    "        frases = [o.strip() for o in oraciones if 5 < len(o.split()) < 30]\n",
    "        return frases[:max_frases]\n",
    "    except Exception as e:\n",
    "        print(f\"Error al buscar '{titulo}': {e}\")\n",
    "        return []\n",
    "\n",
    "# Ejemplo: obtener 50 frases de un artículo\n",
    "frases = obtener_frases_wikipedia(\"Revolución francesa\", max_frases=50)\n",
    "for f in frases[:5]:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "temas = [\n",
    "    # Países y lugares\n",
    "    'Argentina', 'España', 'México', 'Colombia', 'Chile',\n",
    "    'Perú', 'Uruguay', 'Brasil', 'América Latina', 'Europa',\n",
    "\n",
    "    # Cultura argentina\n",
    "    'Lionel Messi', 'Diego Maradona', 'Lali Esposito', 'Charly Garcia', 'Dillom',\n",
    "    'Tiempos Violentos', 'Relatos Salvajes', 'Universidad de Buenos Aires', 'Rock nacional', 'Cine argentino',\n",
    "\n",
    "    # Historia y política\n",
    "    'Revolucion de Mayo', 'Independencia de Argentina', 'Simón Bolívar', 'Segunda Guerra Mundial', 'Guerra Fría',\n",
    "    'Revolución Francesa', 'Guerra Civil Española', 'Napoleón Bonaparte', 'Nelson Mandela', 'Dictadura militar en Argentina',\n",
    "\n",
    "    # Ciencia y tecnología\n",
    "    'Inteligencia artificial', 'ChatGPT', 'Redes neuronales', 'Robótica', 'Energía solar',\n",
    "    'Vacunas', 'COVID-19', 'Cambio climático', 'Computadora cuántica', 'NASA',\n",
    "\n",
    "    # Cultura general\n",
    "    'El Principito', 'Premio Nobel', 'Frida Kahlo', 'Pablo Picasso', 'Leonardo da Vinci',\n",
    "    'William Shakespeare', 'Gabriel García Márquez', 'Julio Cortázar', 'Literatura latinoamericana', 'Arte contemporáneo',\n",
    "\n",
    "    # Entretenimiento y medios\n",
    "    'Marvel', 'DC Comics', 'Netflix', 'Cine de terror', 'Películas de ciencia ficción',\n",
    "    'Música electrónica', 'Reguetón', 'Spotify', 'YouTube', 'TikTok',\n",
    "\n",
    "    # Deportes\n",
    "    'Fútbol', 'Copa Mundial de la FIFA', 'Juegos Olimpicos', 'Tenis', 'NBA',\n",
    "    'Boca Juniors', 'River Plate', 'Messi vs Ronaldo', 'Fórmula 1', 'Michael Jordan',\n",
    "\n",
    "    # Sociedad y actualidad\n",
    "    'Feminismo', 'Día Internacional de la Mujer', 'Diversidad cultural', 'Migración', 'Pobreza',\n",
    "    'Educación pública', 'Salud mental', 'Medio ambiente', 'Derechos humanos', 'Trabajo remoto',\n",
    "\n",
    "    # Filosofía y pensamiento\n",
    "    'Filosofía', 'Ética', 'Psicología', 'Sigmund Freud', 'Carl Jung',\n",
    "    'Existencialismo', 'Sociología', 'Economía', 'Política', 'Democracia'\n",
    "]\n",
    "\n",
    "\n",
    "frases_wikipedia = []\n",
    "for tema in temas:\n",
    "    print(f\"Obteniendo frases de Wikipedia para: {tema}\")\n",
    "    frases = obtener_frases_wikipedia(tema,max_frases=100)\n",
    "    print('Ejemplos de frases obtenidas:')\n",
    "    for f in frases[:2]:\n",
    "        print(f\"- {f}\")\n",
    "    frases_wikipedia.extend(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frases_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia\n",
    "\n",
    "print(len(oraciones_raw))\n",
    "\n",
    "import random\n",
    "\n",
    "random.sample(oraciones_raw, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class PunctuationCapitalizationRNN(nn.Module):\n",
    "    def __init__(self, bert_embeddings, hidden_dim, num_punct_classes, num_cap_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = bert_embeddings  # Embeddings de BERT\n",
    "\n",
    "        # Capa de proyección: adapta dimensión de salida de BERT al hidden_dim de la RNN\n",
    "        self.projection = nn.Linear(self.embedding.embedding_dim, hidden_dim)\n",
    "\n",
    "        # LSTM más profunda: 2 capas\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=False  # manteniendo unidireccional\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Clasificador de puntuación con capa oculta\n",
    "        self.punct_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_punct_classes)\n",
    "        )\n",
    "\n",
    "        # Clasificador de capitalización con capa oculta\n",
    "        self.cap_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_cap_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Embeddings de BERT (sin fine-tuning)\n",
    "        with torch.no_grad():\n",
    "            embedded = self.embedding(input_ids)  # (batch, seq_len, emb_dim)\n",
    "            embedded = embedded.detach()\n",
    "\n",
    "        # Proyección a hidden_dim\n",
    "        projected = self.projection(embedded)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # LSTM con manejo de padding\n",
    "        if attention_mask is not None:\n",
    "            lengths = attention_mask.sum(dim=1)\n",
    "            packed = pack_padded_sequence(projected, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            rnn_out_packed, _ = self.rnn(packed)\n",
    "            rnn_out, _ = pad_packed_sequence(rnn_out_packed, batch_first=True, total_length=projected.size(1))\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(projected)\n",
    "\n",
    "        # Regularización\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "\n",
    "        # Cabezales separados\n",
    "        punct_logits = self.punct_classifier(rnn_out)\n",
    "        cap_logits = self.cap_classifier(rnn_out)\n",
    "\n",
    "        return punct_logits, cap_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de entrenamiento\n",
    "def train(model, dataloader_train, dataloader_test, optimizer, criterion, device, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader_train:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss_punct = criterion(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "            loss_cap = criterion(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "            loss = loss_punct + loss_cap\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(dataloader_train)\n",
    "\n",
    "        \"\"\"\n",
    "        # Evaluación en test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, punct_labels, cap_labels in dataloader_test:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                punct_labels = punct_labels.to(device)\n",
    "                cap_labels = cap_labels.to(device)\n",
    "\n",
    "                punct_logits, cap_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                loss_punct = criterion(punct_logits.view(-1, punct_logits.shape[-1]), punct_labels.view(-1))\n",
    "                loss_cap = criterion(cap_logits.view(-1, cap_logits.shape[-1]), cap_labels.view(-1))\n",
    "\n",
    "                loss = loss_punct + loss_cap\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(dataloader_test)\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Funcion de evaluacion\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_punct_correct = 0\n",
    "    total_cap_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, punct_labels, cap_labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            punct_labels = punct_labels.to(device)\n",
    "            cap_labels = cap_labels.to(device)\n",
    "\n",
    "            punct_logits, cap_logits = model(input_ids)\n",
    "\n",
    "            # Obtener predicciones (dim: [batch, seq_len])\n",
    "            pred_punct = torch.argmax(punct_logits, dim=-1)\n",
    "            pred_cap = torch.argmax(cap_logits, dim=-1)\n",
    "\n",
    "            # Máscara válida (para ignorar -100)\n",
    "            mask = (punct_labels != -100)\n",
    "\n",
    "            # Cálculo de accuracy\n",
    "            total_punct_correct += (pred_punct[mask] == punct_labels[mask]).sum().item()\n",
    "            total_cap_correct += (pred_cap[mask] == cap_labels[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    punct_acc = total_punct_correct / total_tokens\n",
    "    cap_acc = total_cap_correct / total_tokens\n",
    "\n",
    "    print(f\"📌 Punctuation Accuracy:     {punct_acc:.4f}\")\n",
    "    print(f\"🔡 Capitalization Accuracy: {cap_acc:.4f}\")\n",
    "    return punct_acc, cap_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings de BERT\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Extraemos la capa de embeddings de BERT\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings  # Es una instancia de nn.Embedding\n",
    "\n",
    "# (Opcional) Congelamos los embeddings para que no se modifiquen durante el entrenamiento\n",
    "for param in bert_embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Crear el modelo\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_embeddings=bert_embeddings,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "# Configurar optimizador y función de pérdida\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "\n",
    "# Entrenamiento \n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion=criterion, device=device, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_reconstruct(model, sentence, tokenizer, device, max_length=64):\n",
    "    model.eval()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        punct_logits, cap_logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    pred_punct = torch.argmax(punct_logits, dim=-1)[0].cpu().tolist()\n",
    "    pred_cap = torch.argmax(cap_logits, dim=-1)[0].cpu().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    INV_PUNCT_TAGS = {v: k for k, v in PUNCT_TAGS.items()}\n",
    "\n",
    "    final_words = []\n",
    "    current_word = \"\"\n",
    "    current_cap = 0\n",
    "    current_punct = 0\n",
    "    new_word = True\n",
    "\n",
    "    print(\"\\n🔍 Predicción token por token:\")\n",
    "    print(f\"{'TOKEN':15s} | {'PUNCT':>5s} | {'SIGNO':>5s} | {'CAP':>3s} | {'FINAL':15s}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for i, (token, punct_label, cap_label) in enumerate(zip(tokens, pred_punct, pred_cap)):\n",
    "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"] or attention_mask[0, i].item() == 0:\n",
    "            continue\n",
    "\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += clean_token\n",
    "            if punct_label != 0:\n",
    "                current_punct = punct_label  # usar puntuación del último subtoken relevante\n",
    "        else:\n",
    "            if current_word:\n",
    "                # cerrar palabra anterior\n",
    "                word = current_word\n",
    "                # aplicar capitalización a toda la palabra\n",
    "                if current_cap == 1:\n",
    "                    word = word.capitalize()\n",
    "                elif current_cap == 2:\n",
    "                    word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "                elif current_cap == 3:\n",
    "                    word = word.upper()\n",
    "                # aplicar puntuación del último subtoken\n",
    "                punct = INV_PUNCT_TAGS.get(current_punct, \"Ø\")\n",
    "                if punct == \"¿\":\n",
    "                    word = \"¿\" + word\n",
    "                elif punct != \"Ø\":\n",
    "                    word = word + punct\n",
    "                final_words.append(word)\n",
    "\n",
    "            # empezar nueva palabra\n",
    "            current_word = clean_token\n",
    "            current_cap = cap_label\n",
    "            current_punct = punct_label if punct_label != 0 else 0\n",
    "\n",
    "\n",
    "        print(f\"{clean_token:15s} | {punct_label:5d} | {INV_PUNCT_TAGS.get(punct_label, 'Ø'):>5s} | {cap_label:3d} | {clean_token:15s}\")\n",
    "\n",
    "    # Procesar última palabra\n",
    "    if current_word:\n",
    "        word = current_word\n",
    "        if current_cap == 1:\n",
    "            word = word.capitalize()\n",
    "        elif current_cap == 2:\n",
    "            word = ''.join(c.upper() if random.random() > 0.5 else c.lower() for c in word)\n",
    "        elif current_cap == 3:\n",
    "            word = word.upper()\n",
    "        punct = INV_PUNCT_TAGS.get(current_punct, \"Ø\")\n",
    "        if punct == \"¿\":\n",
    "            word = \"¿\" + word\n",
    "        elif punct != \"Ø\":\n",
    "            word = word + punct\n",
    "        final_words.append(word)\n",
    "\n",
    "    return \" \".join(final_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = \"me invito a comer jorge vos queres venir o le digo que no\"\n",
    "print(\"Predicción:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\"Buenas tardes, quiero un APPLE por favor. Muchisimas HH\"]\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_embeddings=bert_embeddings,\n",
    "    hidden_dim=64,\n",
    "    num_punct_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(model, train_loader, train_loader,optimizer, criterion, device, epochs=200)\n",
    "\n",
    "entrada = \"buenas tardes quiero un apple por favor muchisimas hh\"\n",
    "print(\"Predicción:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"modelo.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
