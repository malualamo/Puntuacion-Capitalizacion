{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AA2\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.datautils import *\n",
    "from utils.MLutils import *\n",
    "from utils.resources import *\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel\n",
    "from data.variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando: cuda\n"
     ]
    }
   ],
   "source": [
    "linux = True\n",
    "device = None\n",
    "\n",
    "if linux:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"usando:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fuente 1: Conjunto de preguntas en espa;ol\n",
    "- Fuente 2: Dataset provisto para Notebook 10\n",
    "- Fuente 3: Dataset sintetico generado con Gemini\n",
    "- Fuente 4: Articulos de Wikipedia\n",
    "- Fuente 5: Subtitulos de peliculas\n",
    "- Fuente 6: Mixture of preguntas y afirmaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n",
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n",
      "Hay 1413 oraciones sintéticas.\n",
      "Se cargaron 6648 frases de Wikipedia.\n",
      "Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "Frases extraídas en total: 947\n",
      "✅ Se extrajeron 1000 frases de Relatos Salvajes.\n"
     ]
    }
   ],
   "source": [
    "questions, question_for_mixture = get_questions()\n",
    "oraciones_rnn = get_notebook_dataset()\n",
    "oraciones_sinteticas = get_gemini_dataset()\n",
    "frases_wikipedia = get_wikipedia_dataset()\n",
    "esperando_la_carroza, frases_relatos_salvajes = get_pelis_dataset()\n",
    "mixtures = get_mixture_dataset(oraciones_sinteticas, question_for_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos las fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 20244\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintéticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Cantidad de oraciones de mixture: 4239\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ese año, organizó los territorios ocupados en Italia en lo que se conoció como la República Cisalpina.',\n",
       " 'De entre ellos cabe destacar a John Locke y Voltaire, cuyas ideas fueron muy importantes para el desarrollo de la noción moderna de derechos.',\n",
       " '¿Cuál fue el género de la lista de best seller de The New York Times en el que se clasificó El mundo sin nosotros?',\n",
       " 'En carrera, se le agregan los segundos al tiempo final que haya obtenido en su última vuelta, con la posible pérdida de posiciones.',\n",
       " '¿Qué causó el aumento del déficit fiscal en Uruguay? El Pumpkin Spice Latte de Starbucks es famoso.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes + mixtures\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintéticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "print('Cantidad de oraciones de mixture:',len(mixtures))\n",
    "\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos en conjuntos de `train` y `test` con el tokenizer de `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19231\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.RNNBidirectionalAttention import PunctuationCapitalizationRNNBidirectionalAttention\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PunctuationCapitalizationRNNBidirectionalAttention(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_start_classes=len(PUNCT_START_TAGS),\n",
    "    num_punct_end_classes=len(PUNCT_END_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "start_w, end_w, cap_w = compute_class_weights(\n",
    "    dataloader_train,\n",
    "    num_classes_list=[\n",
    "        len(PUNCT_START_TAGS),\n",
    "        len(PUNCT_END_TAGS),\n",
    "        len(CAP_TAGS)\n",
    "    ],\n",
    "    device=device,\n",
    "    beta=0.7\n",
    ")\n",
    "\n",
    "criterion_start = nn.CrossEntropyLoss(ignore_index=-100, weight=start_w)\n",
    "criterion_end   = nn.CrossEntropyLoss(ignore_index=-100, weight=end_w)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_w)\n",
    "\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.lstm1.parameters()) \\\n",
    "  + list(model.lstm2.parameters()) \\\n",
    "  + list(model.punct_start_classifier.parameters()) \\\n",
    "  + list(model.punct_end_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.2058\n",
      "Epoch 2 | Train Loss: 1.6434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcriterion_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcriterion_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcriterion_cap\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_cap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AA2\\Puntualizacion-Capitalizacion\\utils\\MLutils.py:27\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader_train, optimizer, criterion_start, criterion_end, criterion_cap, device, epochs)\u001b[39m\n\u001b[32m     23\u001b[39m cap_labels = cap_labels.to(device)\n\u001b[32m     25\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m punct_start_logits, punct_end_logits, cap_logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m loss_start = criterion_start(punct_start_logits.view(-\u001b[32m1\u001b[39m, punct_start_logits.shape[-\u001b[32m1\u001b[39m]),\n\u001b[32m     30\u001b[39m                              punct_start_labels.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     31\u001b[39m loss_end = criterion_end(punct_end_logits.view(-\u001b[32m1\u001b[39m, punct_end_logits.shape[-\u001b[32m1\u001b[39m]),\n\u001b[32m     32\u001b[39m                          punct_end_labels.view(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AA2\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AA2\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AA2\\Puntualizacion-Capitalizacion\\train\\RNNBidirectionalAttention.py:63\u001b[39m, in \u001b[36mPunctuationCapitalizationRNNBidirectionalAttention.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     60\u001b[39m x = \u001b[38;5;28mself\u001b[39m.projection(bert_out.last_hidden_state)       \u001b[38;5;66;03m# [B, T, hidden_dim]\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     lengths = \u001b[43mattention_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     packed = pack_padded_sequence(x, lengths, batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     65\u001b[39m     packed1, _ = \u001b[38;5;28mself\u001b[39m.lstm1(packed)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "      model, \n",
    "      dataloader_train=dataloader_train, \n",
    "      optimizer=optimizer, \n",
    "      criterion_start=criterion_start,\n",
    "      criterion_end=criterion_end, \n",
    "      criterion_cap = criterion_cap, \n",
    "      device=device, \n",
    "      epochs=20\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_start_classes=len(PUNCT_START_TAGS),\n",
    "    num_punct_end_classes=len(PUNCT_END_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_rnn_tres_cabezas.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = \"no te lo puedo creer messi se va a retirar\"\n",
    "\n",
    "print(f\"Entrada: {entrada}\")\n",
    "print(f\"Salida: {predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de control de Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\"¿QUE?\"]\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_loader = get_dataloader(frases, max_length=25, batch_size=1, device=device,tokenizer=tokenizer)\n",
    "\n",
    "model = PunctuationCapitalizationRNN(\n",
    "    bert_model=bert_model,\n",
    "    hidden_dim=64,\n",
    "    num_punct_start_classes=5,\n",
    "    num_punct_end_classes=5,\n",
    "    num_cap_classes=4\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Alto LR\n",
    "\n",
    "criterion_punct_start = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_punct_end   = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion_cap         = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    dataloader_train=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion_start=criterion_punct_start,\n",
    "    criterion_end=criterion_punct_end,\n",
    "    criterion_cap=criterion_cap,\n",
    "    device=device,\n",
    "    epochs=200\n",
    ")\n",
    "\n",
    "entrada = \"que\"\n",
    "print(\"Predicción:\", predict_and_reconstruct(model, entrada, tokenizer, device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_rnn_tres_cabezas.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion CSV TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datautils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a un archivo TXT con párrafos (una instancia por párrafo)\n",
    "ruta_txt = \"predict/prueba.txt\"\n",
    "\n",
    "# Ejecutar predicciones y guardar CSV\n",
    "df_predicciones = predicciones_TP(ruta_txt, model, tokenizer, device, max_length=128, verbose=True)\n",
    "\n",
    "# Mostrar las primeras filas del dataframe con las predicciones\n",
    "print(df_predicciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.5 ('torch-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "57e0b81741af75de75a5a8af10aaa09c8be751a9e5f6a5da905dc13dee22058a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
