{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabro/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.datautils import *\n",
    "from utils.MLutils import *\n",
    "from utils.resources import *\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel\n",
    "import unicodedata\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "# pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando: cuda\n"
     ]
    }
   ],
   "source": [
    "linux = True\n",
    "device = None\n",
    "\n",
    "if linux:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 1: Conjunto de preguntas en espa;ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n"
     ]
    }
   ],
   "source": [
    "questions, question_for_mixture = get_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 2: Dataset provisto para Notebook 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n"
     ]
    }
   ],
   "source": [
    "oraciones_rnn = get_notebook_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 3: Dataset sintetico generado con Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1413 oraciones sintéticas.\n"
     ]
    }
   ],
   "source": [
    "oraciones_sinteticas = get_gemini_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 4: Articulos de Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Argentina, oficialmente República Argentina,[a]\\u200b es un país soberano de América del Sur, ubicado en el extremo sur y sudeste de ese subcontinente.', 'Adopta la forma de gobierno republicana, democrática, representativa y federal.', 'Poseen Carta Magna, bandera y fuerzas de seguridad propias, el dominio de los recursos naturales circunscriptos en su territorio y delegan los poderes exclusivos al Gobierno Federal.', 'Hasta mediados del siglo XX, fue una de las economías más prósperas del mundo.', 'No obstante, es la segunda economía más importante de Sudamérica —detrás de Brasil— y la 24.º más grande del mundo por PIB nominal.']\n"
     ]
    }
   ],
   "source": [
    "frases_wikipedia = get_wikipedia_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 5: Subtitulos de peliculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "✅ Frases extraídas y guardadas. Total: 947\n",
      "['Mamá Cora.', '¡Mamá Cor... Mamá Cora!', '¡Acabala, carajo! ¡Cerrá el pico de una vez! Estás hablando de mi madre, ¿no? Y vos, ¿por qué no le buscás un lugarcito? Al fin y al cabo sos tan hijo de ella como yo. Además, tu mujer tiene mucha más paciencia que la mía.', 'Es un sueño.', 'Mejor, ¿quién te necesita?', 'No...', 'Abrígate. Mirá que cuando vuelvas va a refrescar.', 'Angelito, no llores más, mi amor te vas a hacer mal.', 'Por favor. No empecés otra vez con esa cantinela...', '¿No sabés que duerme? Atendélo vos.']\n",
      "✅ Se extrajeron 1000 frases de Relatos Salvajes.\n"
     ]
    }
   ],
   "source": [
    "esperando_la_carroza, frases_relatos_salvajes = get_pelis_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 6 (beta): Mixture de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¿De qué color es la circunferencia de la bandera nipona? ¿De qué corona es archivo oficial el Archivo General de Simancas?',\n",
       " 'Mi profesor de matemáticas, el señor López, es muy claro. ¿Qué mando de Wii es opcional para la mayoría de eventos del videojuego?',\n",
       " '¿Por dónde viajó Obama en este periplo internacional? ¿Cuándo le encargaron la placa a Xavier?',\n",
       " 'El Walkman fue un producto icónico de Sony. ¿Dónde ha mejorado sensiblemente la seguridad ciudadana?',\n",
       " '¿Qué significan las siglas MRE? ¿Dónde debe jugar un futbolista?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cant_oraciones = len(oraciones_sinteticas)\n",
    "question_for_mixture = [re.sub(r'[\\\\\\(\\)!¡“]', '', unicodedata.normalize(\"NFC\", q).strip()) for q in question_for_mixture]\n",
    "oraciones_sinteticas = [re.sub(r'[\\\\\\(\\)!¡“]', '', unicodedata.normalize(\"NFC\", a).strip()) for a in oraciones_sinteticas]\n",
    "\n",
    "tanda_1 = question_for_mixture[:cant_oraciones]\n",
    "question_affirmation = [f\"{q} {a}\" for q, a in zip(tanda_1, oraciones_sinteticas)]\n",
    "\n",
    "tanda_2 = question_for_mixture[cant_oraciones:2*cant_oraciones]\n",
    "affirmation_question = [f\"{a} {q}\" for q, a in zip(tanda_2, oraciones_sinteticas)]\n",
    "\n",
    "tanda_3 = question_for_mixture[2*cant_oraciones:3*cant_oraciones]\n",
    "tanda_3_shuffled = random.sample(tanda_3, len(tanda_3))\n",
    "question_question = [f\"{q} {p}\" for q, p in zip(tanda_3, tanda_3_shuffled)]\n",
    "\n",
    "mixtures = question_affirmation + affirmation_question + question_question\n",
    "\n",
    "random.sample(mixtures, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos las fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 20244\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintéticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Cantidad de oraciones Compuestas: 4239\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['¿Que tal?',\n",
       " '“Oye, Cohren, ¿Harris aún tiene a Bola de Nieve?”, preguntó Nemic mientras se ponían cómodos.',\n",
       " 'C., concluyendo con la conquista de los españoles.',\n",
       " '¿Cuál es la población de la zona si la comparamos con la que había a mediados del siglo pasado?',\n",
       " 'A diferencia de la Primera Guerra Mundial, la rendición (tanto la japonesa como la alemana) se produjo por derrota incondicional, sin pasar por ningún tipo de negociación.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes + mixtures\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintéticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "print('Cantidad de oraciones Compuestas:',len(mixtures))\n",
    "\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos en conjuntos de `train` y `test` con el tokenizer de `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19231\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 180,944,905\n",
      "Trainable parameters: 17,857,801\n"
     ]
    }
   ],
   "source": [
    "from train.RNNBidirectional import PunctuationCapitalizationRNNBidirectional\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model = PunctuationCapitalizationRNNBidirectional(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(\"model_bidirec.pt\", map_location=device)\n",
    "# si guardaste state_dict puro\n",
    "if isinstance(ckpt, dict) and \"model_state_dict\" not in ckpt:\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "# si guardaste un dict con más cosas (epoch, optim, etc.)\n",
    "elif \"model_state_dict\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 180,683,785\n",
      "Trainable parameters: 17,596,681\n"
     ]
    }
   ],
   "source": [
    "from train.RNNBidirectionalAttention import PunctuationCapitalizationRNNBidirectionalAttention \n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model = PunctuationCapitalizationRNNBidirectionalAttention(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PackedSequence' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     12\u001b[39m trainable_params = [\n\u001b[32m     13\u001b[39m     p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m bert_model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad\n\u001b[32m     14\u001b[39m ] + \u001b[38;5;28mlist\u001b[39m(model.projection.parameters()) \\\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m   + \u001b[38;5;28mlist\u001b[39m(model.punct_classifier.parameters()) \\\n\u001b[32m     19\u001b[39m   + \u001b[38;5;28mlist\u001b[39m(model.cap_classifier.parameters())\n\u001b[32m     21\u001b[39m optimizer = torch.optim.AdamW(trainable_params, lr=\u001b[32m2e-5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_punct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion_punct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_cap\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_cap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/utils/MLutils.py:22\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader_train, dataloader_test, optimizer, criterion_punct, criterion_cap, device, epochs)\u001b[39m\n\u001b[32m     18\u001b[39m cap_labels = cap_labels.to(device)\n\u001b[32m     20\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m punct_logits, cap_logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m loss_punct = criterion_punct(punct_logits.view(-\u001b[32m1\u001b[39m, punct_logits.shape[-\u001b[32m1\u001b[39m]), punct_labels.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     25\u001b[39m loss_cap = criterion_cap(cap_logits.view(-\u001b[32m1\u001b[39m, cap_logits.shape[-\u001b[32m1\u001b[39m]), cap_labels.view(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/train/RNNBidirectionalAttention.py:49\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m.punct_classifier = nn.Sequential(\n\u001b[32m     37\u001b[39m         nn.Linear(\u001b[32m2\u001b[39m * hidden_dim, hidden_dim),\n\u001b[32m     38\u001b[39m         nn.ReLU(),\n\u001b[32m     39\u001b[39m         nn.Dropout(dropout),\n\u001b[32m     40\u001b[39m         nn.Linear(hidden_dim, num_punct_classes),\n\u001b[32m     41\u001b[39m     )\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.cap_classifier = nn.Sequential(\n\u001b[32m     43\u001b[39m         nn.Linear(\u001b[32m2\u001b[39m * hidden_dim, hidden_dim),\n\u001b[32m     44\u001b[39m         nn.ReLU(),\n\u001b[32m     45\u001b[39m         nn.Dropout(dropout),\n\u001b[32m     46\u001b[39m         nn.Linear(hidden_dim, num_cap_classes),\n\u001b[32m     47\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     50\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.bert(input_ids, attention_mask=attention_mask)\n\u001b[32m     51\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.projection(outputs.last_hidden_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:1218\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1212\u001b[39m     (attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m torch.is_floating_point(attn_mask))\n\u001b[32m   1213\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m (key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1214\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m torch.is_floating_point(key_padding_mask)\n\u001b[32m   1215\u001b[39m ):\n\u001b[32m   1216\u001b[39m     why_not_fast_path = \u001b[33m\"\u001b[39m\u001b[33mfloating-point masks are not supported for fast path.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1218\u001b[39m is_batched = \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m() == \u001b[32m3\u001b[39m\n\u001b[32m   1220\u001b[39m key_padding_mask = F._canonical_mask(\n\u001b[32m   1221\u001b[39m     mask=key_padding_mask,\n\u001b[32m   1222\u001b[39m     mask_name=\u001b[33m\"\u001b[39m\u001b[33mkey_padding_mask\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1225\u001b[39m     target_type=query.dtype,\n\u001b[32m   1226\u001b[39m )\n\u001b[32m   1228\u001b[39m attn_mask = F._canonical_mask(\n\u001b[32m   1229\u001b[39m     mask=attn_mask,\n\u001b[32m   1230\u001b[39m     mask_name=\u001b[33m\"\u001b[39m\u001b[33mattn_mask\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     check_other=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1235\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'PackedSequence' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "punct_weights_tensor, cap_weights_tensor = compute_class_weights(\n",
    "    dataloader_train,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS),\n",
    "    device=device,\n",
    "    beta=0.7\n",
    ")\n",
    "\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100, weight=punct_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "\n",
    "\"\"\"\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\"\"\"\n",
    "\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.lstm1.parameters()) \\\n",
    "  + list(model.lstm2.parameters()) \\\n",
    "  + list(model.attention.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "\n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion_punct=criterion_punct, criterion_cap = criterion_cap, device=device, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiero aprender a programar en Python. Conocés algún curso.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"quiero aprender a programar en python conocés algún curso\"\n",
    "print(f\"{predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_bidirec_more_data.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
