{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabro/Documents/uba/aprendizaje_automatico/Puntualizacion-Capitalizacion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.datautils import *\n",
    "from utils.MLutils import *\n",
    "from utils.resources import *\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel\n",
    "import unicodedata\n",
    "\n",
    "# linea que arregla algunos errores de loadeo de datasets\n",
    "# pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando: cuda\n"
     ]
    }
   ],
   "source": [
    "linux = True\n",
    "device = None\n",
    "\n",
    "if linux:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejericio b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 1: Conjunto de preguntas en espa;ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n"
     ]
    }
   ],
   "source": [
    "questions, question_for_mixture = get_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 2: Dataset provisto para Notebook 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n"
     ]
    }
   ],
   "source": [
    "oraciones_rnn = get_notebook_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 3: Dataset sintetico generado con Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 1413 oraciones sintéticas.\n"
     ]
    }
   ],
   "source": [
    "oraciones_sinteticas = get_gemini_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 4: Articulos de Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Argentina, oficialmente República Argentina,[a]\\u200b es un país soberano de América del Sur, ubicado en el extremo sur y sudeste de ese subcontinente.', 'Adopta la forma de gobierno republicana, democrática, representativa y federal.', 'Poseen Carta Magna, bandera y fuerzas de seguridad propias, el dominio de los recursos naturales circunscriptos en su territorio y delegan los poderes exclusivos al Gobierno Federal.', 'Hasta mediados del siglo XX, fue una de las economías más prósperas del mundo.', 'No obstante, es la segunda economía más importante de Sudamérica —detrás de Brasil— y la 24.º más grande del mundo por PIB nominal.']\n"
     ]
    }
   ],
   "source": [
    "frases_wikipedia = get_wikipedia_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 5: Subtitulos de peliculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "✅ Frases extraídas y guardadas. Total: 947\n",
      "['¿Dónde se me ve nerviosa? Jorge ocúpate si se ensució.', '¿Verdad? Forense.- Una vez que el juez dé el permiso, podrán llevarla a...', '_ Elvira, no quisiera poner el dedo en la llaga, pero al fin y al cabo Sergio y vos están viviendo en esta casa que fue de Mamá C ora, y estos son sus muebles.', 'es que los muertos me impresionan.', 'Yo te lo plancho, Susana.', '¿Qué no le va a poder dar?', 'Por ahí.', '¿Para qué querés saber con quién se acostó Elvira?', 'Yo sirvo, chiquita.', 'Lo que le vendría bien es que te la llevaras a vivir un tiempo a tu casa.']\n",
      "✅ Se extrajeron 1000 frases de Relatos Salvajes.\n"
     ]
    }
   ],
   "source": [
    "esperando_la_carroza, frases_relatos_salvajes = get_pelis_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuente 6 (beta): Mixture de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Coca-Cola produce más de 500 marcas. ¿Cuál es el color de fondo de la enseña de Haro?',\n",
       " '¿Qué magnitud alcanzó el seísmo ocurrido hace una semana? ¿Quién hizo acto de presencia por el barrio de Sant Ramon?',\n",
       " '¿Cuál es la empresa editora del manga de Naruto? ¿Quién fundó el Archivo General de Simancas?',\n",
       " '¿Quién les está haciendo la vida imposible? La sede de Johnson & Johnson está en América.',\n",
       " 'Qué pena que el concierto se haya cancelado en el último momento ¿Cuándo llegó Wollstonecraft a París?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cant_oraciones = len(oraciones_sinteticas)\n",
    "question_for_mixture = [re.sub(r'[\\\\\\(\\)!¡“]', '', unicodedata.normalize(\"NFC\", q).strip()) for q in question_for_mixture]\n",
    "oraciones_sinteticas = [re.sub(r'[\\\\\\(\\)!¡“]', '', unicodedata.normalize(\"NFC\", a).strip()) for a in oraciones_sinteticas]\n",
    "\n",
    "tanda_1 = question_for_mixture[:cant_oraciones]\n",
    "question_affirmation = [f\"{q} {a}\" for q, a in zip(tanda_1, oraciones_sinteticas)]\n",
    "\n",
    "tanda_2 = question_for_mixture[cant_oraciones:2*cant_oraciones]\n",
    "affirmation_question = [f\"{a} {q}\" for q, a in zip(tanda_2, oraciones_sinteticas)]\n",
    "\n",
    "tanda_3 = question_for_mixture[2*cant_oraciones:3*cant_oraciones]\n",
    "tanda_3_shuffled = random.sample(tanda_3, len(tanda_3))\n",
    "question_question = [f\"{q} {p}\" for q, p in zip(tanda_3, tanda_3_shuffled)]\n",
    "\n",
    "mixtures = question_affirmation + affirmation_question + question_question\n",
    "\n",
    "random.sample(mixtures, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos las fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 20244\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintéticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Cantidad de oraciones Compuestas: 4239\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['¿De qué ciudad viene el Ivesur-PTV? ¿Cuál era la actitud del Círculo Beliáyev respecto a las costumbres compositivas de Occidente?',\n",
       " '== Autores más relevantes ==\\n\\n\\n== Subsidiarias de DC Comics (propiedad de Warner Bros.',\n",
       " 'Argentina, en años sin campeonatos mundiales por la guerra en Europa, tuvo absoluta supremacía en el fútbol continental.',\n",
       " 'La reunión de la OEA no llegó a un consenso sobre la situación.',\n",
       " 'El concepto de democracia participativa propone la creación de formas democráticas directas para atenuar el carácter puramente representativo (audiencias públicas, recursos administrativos, ombudsman).']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes + mixtures\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintéticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "print('Cantidad de oraciones Compuestas:',len(mixtures))\n",
    "\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos en conjuntos de `train` y `test` con el tokenizer de `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19231\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 180,944,905\n",
      "Trainable parameters: 17,857,801\n"
     ]
    }
   ],
   "source": [
    "from train.RNNBidirectional import PunctuationCapitalizationRNNBidirectional\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model = PunctuationCapitalizationRNNBidirectional(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(\"model_bidirec.pt\", map_location=device)\n",
    "# si guardaste state_dict puro\n",
    "if isinstance(ckpt, dict) and \"model_state_dict\" not in ckpt:\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "# si guardaste un dict con más cosas (epoch, optim, etc.)\n",
    "elif \"model_state_dict\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Punctuation Prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           Ø       0.99      0.98      0.98     19951\n",
      "           ,       0.52      0.85      0.64       499\n",
      "           .       0.83      0.76      0.79       652\n",
      "           ?       0.84      0.70      0.76       565\n",
      "           ¿       0.93      0.85      0.89       579\n",
      "\n",
      "    accuracy                           0.96     22246\n",
      "   macro avg       0.82      0.83      0.81     22246\n",
      "weighted avg       0.97      0.96      0.96     22246\n",
      "\n",
      "\n",
      "Classification Report for Capitalization Prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       lower       0.98      0.99      0.99     15161\n",
      "        init       0.96      0.95      0.95      4734\n",
      "         mix       0.90      0.66      0.76        65\n",
      "       upper       0.87      0.95      0.91       260\n",
      "\n",
      "    accuracy                           0.98     20220\n",
      "   macro avg       0.93      0.89      0.90     20220\n",
      "weighted avg       0.98      0.98      0.98     20220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(dataloader, model, label_index, target_names, task_name):\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels_punct, labels_cap = batch\n",
    "        labels = labels_punct if label_index == 0 else labels_cap\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_punct, logits_cap = model(input_ids, attention_mask)\n",
    "\n",
    "        logits = logits_punct if label_index == 0 else logits_cap\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    flattened_y_true = [label for seq in y_true for label in seq]\n",
    "    flattened_y_pred = [pred for seq in y_pred for pred in seq]\n",
    "\n",
    "    filtered_y_true = [\n",
    "        label for label, pred in zip(flattened_y_true, flattened_y_pred) if label != -100 and pred != -100\n",
    "    ]\n",
    "    filtered_y_pred = [\n",
    "        pred for label, pred in zip(flattened_y_true, flattened_y_pred) if label != -100 and pred != -100\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nClassification Report for {task_name}:\")\n",
    "    print(classification_report(filtered_y_true, filtered_y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "evaluate_model(dataloader_test, model, label_index=0, target_names=PUNCT_TAGS, task_name=\"Punctuation Prediction\")\n",
    "evaluate_model(dataloader_test, model, label_index=1, target_names=CAP_TAGS, task_name=\"Capitalization Prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 181,995,529\n",
      "Trainable parameters: 18,908,425\n"
     ]
    }
   ],
   "source": [
    "from train.RNNBidirectionalAttention import PunctuationCapitalizationRNNBidirectionalAttention \n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "bert_embeddings = bert_model.embeddings.word_embeddings\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model = PunctuationCapitalizationRNNBidirectionalAttention(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.9828\n",
      "Epoch 2 | Train Loss: 1.2857\n",
      "Epoch 3 | Train Loss: 0.8370\n",
      "Epoch 4 | Train Loss: 0.6504\n",
      "Epoch 5 | Train Loss: 0.5590\n",
      "Epoch 6 | Train Loss: 0.5034\n",
      "Epoch 7 | Train Loss: 0.4591\n",
      "Epoch 8 | Train Loss: 0.4270\n",
      "Epoch 9 | Train Loss: 0.4025\n",
      "Epoch 10 | Train Loss: 0.3850\n",
      "Epoch 11 | Train Loss: 0.3628\n",
      "Epoch 12 | Train Loss: 0.3470\n",
      "Epoch 13 | Train Loss: 0.3322\n",
      "Epoch 14 | Train Loss: 0.3163\n",
      "Epoch 15 | Train Loss: 0.3061\n",
      "Epoch 16 | Train Loss: 0.2918\n",
      "Epoch 17 | Train Loss: 0.2832\n",
      "Epoch 18 | Train Loss: 0.2715\n",
      "Epoch 19 | Train Loss: 0.2644\n",
      "Epoch 20 | Train Loss: 0.2529\n"
     ]
    }
   ],
   "source": [
    "punct_weights_tensor, cap_weights_tensor = compute_class_weights(\n",
    "    dataloader_train,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS),\n",
    "    device=device,\n",
    "    beta=0.7\n",
    ")\n",
    "\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100, weight=punct_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "\n",
    "\"\"\"\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.rnn.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\"\"\"\n",
    "\n",
    "trainable_params = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.lstm1.parameters()) \\\n",
    "  + list(model.lstm2.parameters()) \\\n",
    "  + list(model.attention.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-5)\n",
    "\n",
    "train(model, dataloader_train=dataloader_train, dataloader_test=dataloader_test,optimizer=optimizer, criterion_punct=criterion_punct, criterion_cap = criterion_cap, device=device, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es terrible, lo que está pasando, en Chaco, te enteraste.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"es terrible lo que está pasando en chaco te enteraste\"\n",
    "print(f\"{predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_bidirec_attention.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
