{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils.datautils import *\n",
    "from utils.MLutils import *\n",
    "from utils.resources import *\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics import classification_report\n",
    "from data.variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando: cuda\n"
     ]
    }
   ],
   "source": [
    "linux = True\n",
    "device = None\n",
    "\n",
    "if linux:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(\"usando:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fuente 1: Conjunto de preguntas en espa;ol\n",
    "- Fuente 2: Dataset provisto para Notebook 10\n",
    "- Fuente 3: Dataset sintetico generado con Gemini\n",
    "- Fuente 4: Articulos de Wikipedia\n",
    "- Fuente 5: Subtitulos de peliculas\n",
    "- Fuente 6: Mixture of preguntas y afirmaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se descargaron 5000 preguntas en Español.\n",
      "Se descargaron 997 oraciones en Español (del dataset del notebook 10).\n",
      "Hay 1413 oraciones sintéticas.\n",
      "Se cargaron 6648 frases de Wikipedia.\n",
      "✅ Se extrajeron 947 frases completas y se guardaron en 'dialogos_esperando_la_carroza.json'\n",
      "Frases extraídas en total: 947\n",
      "✅ Se extrajeron 1000 frases de Relatos Salvajes.\n"
     ]
    }
   ],
   "source": [
    "questions, question_for_mixture = get_questions()\n",
    "oraciones_rnn = get_notebook_dataset()\n",
    "oraciones_sinteticas = get_gemini_dataset()\n",
    "frases_wikipedia = get_wikipedia_dataset()\n",
    "esperando_la_carroza, frases_relatos_salvajes = get_pelis_dataset()\n",
    "mixtures = get_mixture_dataset(oraciones_sinteticas, question_for_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos las fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de oraciones: 20244\n",
      "Cantidad de oraciones de preguntas: 5000\n",
      "Cantidad de oraciones en espa;ol de hugging face: 997\n",
      "Cantidad de oraciones sintéticas: 1413\n",
      "Cantidad de oraciones de Wikipedia: 6648\n",
      "Cantidad de oraciones de Esperando la carroza: 947\n",
      "Cantidad de oraciones de Relatos Salvajes: 1000\n",
      "Cantidad de oraciones Compuestas: 4239\n",
      "Algunas oraciones aleatorias:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['¿Cuál fue el resultado del encuentro?',\n",
       " 'Con mi suegra.',\n",
       " 'Mi operación requería mucho reposo, así que vi las temporadas 1 a 3 de “Anatomía según Grey”. Me ayudó que McDreamy fuera neurocirujano. Yo prestaba atención a cómo colocaban al paciente básicamente por intentar adivinar lo que habían hecho conmigo. Desde mi cirugía en 2018, vi más representaciones de neurocirugía en Hospital Playlist que en cualquier otro programa médico que haya visto desde Grey. Es agradable tener arte y entretenimiento que nos ayude a darle sentido a nuestra vida.',\n",
       " 'El estrecho de Bering fue un puente de tierra en la Edad de Hielo. ¿A cuántos narcos pudieron detener el octubre pasado?',\n",
       " '¿Cómo se suele realizar la separación de la leche en el cuajado? El nuevo Ford Puma es un coche híbrido.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_raw = questions + oraciones_rnn + oraciones_sinteticas + frases_wikipedia + esperando_la_carroza  + frases_relatos_salvajes + mixtures\n",
    "\n",
    "print('Cantidad total de oraciones:',len(oraciones_raw))\n",
    "print('Cantidad de oraciones de preguntas:',len(questions))\n",
    "print('Cantidad de oraciones en espa;ol de hugging face:',len(oraciones_rnn))\n",
    "print('Cantidad de oraciones sintéticas:',len(oraciones_sinteticas))\n",
    "print('Cantidad de oraciones de Wikipedia:',len(frases_wikipedia))\n",
    "print('Cantidad de oraciones de Esperando la carroza:',len(esperando_la_carroza))\n",
    "print('Cantidad de oraciones de Relatos Salvajes:',len(frases_relatos_salvajes))\n",
    "print('Cantidad de oraciones Compuestas:',len(mixtures))\n",
    "\n",
    "print(\"Algunas oraciones aleatorias:\")\n",
    "random.sample(oraciones_raw, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos en conjuntos de `train` y `test` con el tokenizer de `BERT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19231\n",
      "1013\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "train_sents, test_sents = train_test_split(oraciones_raw, test_size=0.05, random_state=42)\n",
    "\n",
    "dataloader_train = get_dataloader(oraciones_raw=oraciones_raw, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "dataloader_test = get_dataloader(oraciones_raw=test_sents, max_length=64, batch_size=64, device=device, tokenizer=tokenizer)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 180,944,905\n",
      "Trainable parameters: 17,857,801\n"
     ]
    }
   ],
   "source": [
    "from train.RNNBidirectional import PunctuationCapitalizationRNNBidirectional\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model = PunctuationCapitalizationRNNBidirectional(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(\"model_bidirec.pt\", map_location=device)\n",
    "# si guardaste state_dict puro\n",
    "if isinstance(ckpt, dict) and \"model_state_dict\" not in ckpt:\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "# si guardaste un dict con más cosas (epoch, optim, etc.)\n",
    "elif \"model_state_dict\" in ckpt:\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for Punctuation Prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           Ø       0.99      0.98      0.98     19951\n",
      "           ,       0.52      0.85      0.64       499\n",
      "           .       0.83      0.76      0.79       652\n",
      "           ?       0.84      0.70      0.76       565\n",
      "           ¿       0.93      0.85      0.89       579\n",
      "\n",
      "    accuracy                           0.96     22246\n",
      "   macro avg       0.82      0.83      0.81     22246\n",
      "weighted avg       0.97      0.96      0.96     22246\n",
      "\n",
      "\n",
      "Classification Report for Capitalization Prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       lower       0.98      0.99      0.99     15161\n",
      "        init       0.96      0.95      0.95      4734\n",
      "         mix       0.90      0.66      0.76        65\n",
      "       upper       0.87      0.95      0.91       260\n",
      "\n",
      "    accuracy                           0.98     20220\n",
      "   macro avg       0.93      0.89      0.90     20220\n",
      "weighted avg       0.98      0.98      0.98     20220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con atencion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 181,995,529\n",
      "Trainable parameters: 18,908,425\n"
     ]
    }
   ],
   "source": [
    "from train.RNNBidirectionalAttention import PunctuationCapitalizationRNNBidirectionalAttention \n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "N = 2\n",
    "for layer in bert_model.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in bert_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model = PunctuationCapitalizationRNNBidirectionalAttention(\n",
    "    bert_model = bert_model,\n",
    "    hidden_dim=256,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.9828\n",
      "Epoch 2 | Train Loss: 1.2857\n",
      "Epoch 3 | Train Loss: 0.8370\n",
      "Epoch 4 | Train Loss: 0.6504\n",
      "Epoch 5 | Train Loss: 0.5590\n",
      "Epoch 6 | Train Loss: 0.5034\n",
      "Epoch 7 | Train Loss: 0.4591\n",
      "Epoch 8 | Train Loss: 0.4270\n",
      "Epoch 9 | Train Loss: 0.4025\n",
      "Epoch 10 | Train Loss: 0.3850\n",
      "Epoch 11 | Train Loss: 0.3628\n",
      "Epoch 12 | Train Loss: 0.3470\n",
      "Epoch 13 | Train Loss: 0.3322\n",
      "Epoch 14 | Train Loss: 0.3163\n",
      "Epoch 15 | Train Loss: 0.3061\n",
      "Epoch 16 | Train Loss: 0.2918\n",
      "Epoch 17 | Train Loss: 0.2832\n",
      "Epoch 18 | Train Loss: 0.2715\n",
      "Epoch 19 | Train Loss: 0.2644\n",
      "Epoch 20 | Train Loss: 0.2529\n"
     ]
    }
   ],
   "source": [
    "punct_weights_tensor, cap_weights_tensor = compute_class_weights(\n",
    "    dataloader_train,\n",
    "    num_punct_classes=len(PUNCT_TAGS),\n",
    "    num_cap_classes=len(CAP_TAGS),\n",
    "    device=device,\n",
    "    beta=0.7\n",
    ")\n",
    "\n",
    "criterion_punct = nn.CrossEntropyLoss(ignore_index=-100, weight=punct_weights_tensor)\n",
    "criterion_cap   = nn.CrossEntropyLoss(ignore_index=-100, weight=cap_weights_tensor)\n",
    "\n",
    "\n",
    "trainable_params_attn = [\n",
    "    p for p in bert_model.parameters() if p.requires_grad\n",
    "] + list(model.projection.parameters()) \\\n",
    "  + list(model.lstm1.parameters()) \\\n",
    "  + list(model.lstm2.parameters()) \\\n",
    "  + list(model.attention.parameters()) \\\n",
    "  + list(model.punct_classifier.parameters()) \\\n",
    "  + list(model.cap_classifier.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params_attn, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model, \n",
    "    dataloader_train=dataloader_train, \n",
    "    optimizer=optimizer, \n",
    "    criterion_punct=criterion_punct, \n",
    "    criterion_cap = criterion_cap, \n",
    "    device=device, \n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataloader_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es terrible, lo que está pasando, en Chaco, te enteraste.\n"
     ]
    }
   ],
   "source": [
    "entrada = \"es terrible lo que está pasando en chaco te enteraste\"\n",
    "print(f\"{predict_and_reconstruct(model, entrada, tokenizer, device, verbose=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_TP(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    csv_path=\"data/testeo.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_bidirec_attention.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
